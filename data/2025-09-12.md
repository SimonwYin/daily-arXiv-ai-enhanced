<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 53]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.DC](#cs.DC) [Total: 7]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.NI](#cs.NI) [Total: 5]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Uncertainty Estimation using Variance-Gated Distributions](https://arxiv.org/abs/2509.08846)
*H. Martin Gillis,Isaac Xu,Thomas Trappenberg*

Main category: cs.LG

TL;DR: 作者基于类别概率信噪比与集成方差门控，提出了一种新的不确定性估计与分解方法，并讨论了委员会机器多样性崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯或其近似方法将预测不确定性加法分解为知识不确定性（epistemic）和数据不确定性（aleatoric），但最近对加法分解的合理性提出质疑，作者希望提供更直观且可解释的替代方案。

Method: 利用模型预测中类别概率分布的信噪比来衡量不确定性，并通过集成计算置信因子，设计方差门控度量对单次预测进行缩放，从而实现不确定性分解与量化。

Result: 提出的框架提供了一种基于信噪比的度量，可以用于判断预测可信度并用于分析集成模型多样性是否出现崩溃；具体实验细节与定量结果未在摘要中给出。

Conclusion: 论文提出了一种基于类别概率分布信噪比的直观不确定性估计与分解框架，并引入了一个由集成置信度导出的方差门控度量，用于缩放预测；同时讨论了委员会机器多样性崩溃问题。

Abstract: Evaluation of per-sample uncertainty quantification from neural networks is
essential for decision-making involving high-risk applications. A common
approach is to use the predictive distribution from Bayesian or approximation
models and decompose the corresponding predictive uncertainty into epistemic
(model-related) and aleatoric (data-related) components. However, additive
decomposition has recently been questioned. In this work, we propose an
intuitive framework for uncertainty estimation and decomposition based on the
signal-to-noise ratio of class probability distributions across different model
predictions. We introduce a variance-gated measure that scales predictions by a
confidence factor derived from ensembles. We use this measure to discuss the
existence of a collapse in the diversity of committee machines.

</details>


### [2] [Instance-Optimal Matrix Multiplicative Weight Update and Its Quantum Applications](https://arxiv.org/abs/2509.08911)
*Weiyuan Gong,Tongyang Li,Xinzhao Wang,Zhiyu Zhang*

Main category: cs.LG

TL;DR: 提出一个基于新势函数与“单边”Jensen迹不等式的矩阵LEA算法，在保证与MMWU相同复杂度下实现实例最优后悔界，并在若干量子学习任务上取得更好性能。


<details>
  <summary>Details</summary>
Motivation: 在矩阵版本的专家意见学习中，希望获得依赖于比较器复杂性的更精细（instance-optimal）后悔界，而不是仅仅依赖维度的通用界限，且保持与MMWU相同的计算开销。

Method: 构建了一般的势函数（potential-based）框架，把MMWU看作指数势函数的特例；引入基于拉普拉斯变换技术的新型“单边”Jensen矩阵迹不等式，从而允许在矩阵LEA中使用非指数势函数；最终采用源自向量LEA的最优势函数（与虚误差函数有关）导出具体算法。

Result: 给出实例最优的后悔界O(\sqrt{T\cdot S(X||d^{-1}I_d)})，证明算法计算复杂度与MMWU相当；提供矩阵LEA的内存下界；在量子学习任务上（带退极化噪声的量子态学习、随机量子态、Gibbs态）和若干非线性量子性质预测（purity、量子虚冷却、Rényi-2相关）上均优于现有方法。

Conclusion: 本文提出了一种在矩阵专家意见（matrix LEA）问题上优于传统MMWU的算法，能将后悔界从O(\sqrt{T\log d})改进到依赖比较器信息量的实例最优界O(\sqrt{T\cdot S(X||d^{-1}I_d)}})，且计算复杂度与MMWU相同，改善是“免费”的。

Abstract: The Matrix Multiplicative Weight Update (MMWU) is a seminal online learning
algorithm with numerous applications. Applied to the matrix version of the
Learning from Expert Advice (LEA) problem on the $d$-dimensional spectraplex,
it is well known that MMWU achieves the minimax-optimal regret bound of
$O(\sqrt{T\log d})$, where $T$ is the time horizon. In this paper, we present
an improved algorithm achieving the instance-optimal regret bound of
$O(\sqrt{T\cdot S(X||d^{-1}I_d)})$, where $X$ is the comparator in the regret,
$I_d$ is the identity matrix, and $S(\cdot||\cdot)$ denotes the quantum
relative entropy. Furthermore, our algorithm has the same computational
complexity as MMWU, indicating that the improvement in the regret bound is
``free''.
  Technically, we first develop a general potential-based framework for matrix
LEA, with MMWU being its special case induced by the standard exponential
potential. Then, the crux of our analysis is a new ``one-sided'' Jensen's trace
inequality built on a Laplace transform technique, which allows the application
of general potential functions beyond exponential to matrix LEA. Our algorithm
is finally induced by an optimal potential function from the vector LEA
problem, based on the imaginary error function.
  Complementing the above, we provide a memory lower bound for matrix LEA, and
explore the applications of our algorithm in quantum learning theory. We show
that it outperforms the state of the art for learning quantum states corrupted
by depolarization noise, random quantum states, and Gibbs states. In addition,
applying our algorithm to linearized convex losses enables predicting nonlinear
quantum properties, such as purity, quantum virtual cooling, and R\'{e}nyi-$2$
correlation.

</details>


### [3] [Corruption-Tolerant Asynchronous Q-Learning with Near-Optimal Rates](https://arxiv.org/abs/2509.08933)
*Sreejeet Maity,Aritra Mitra*

Main category: cs.LG

TL;DR: 提出并分析了在奖励被任意篡改情形下的异步鲁棒Q学习算法，给出有限时间上界与下界，并设计了无需先验信息的自适应版本，分析依赖改进的鞅不等式。


<details>
  <summary>Details</summary>
Motivation: 现实中奖励信号可能因极端噪声、传感器故障或恶意攻击被任意篡改，传统Q学习对这类对抗性扰动敏感，因此需要理论上有鲁棒性保证的算法。

Method: 构建带鲁棒性的Q学习变体，通过在更新或估计步骤中引入对异常/对抗样本的容错处理，基于异步采样模型与时间相关数据分析算法的有限时间误差界；进一步设计无须事先知道真实奖励分布统计信息的自适应变体，分析中使用细化的Azuma-Hoeffding不等式（用于近似鞅序列）。

Result: 证明鲁棒Q学习在对抗性篡改下仍能以与无对抗情况相同的有限时间收敛速率达到最优策略，误差上界含一项与被篡改样本比例成正比；给出匹配的信息论下界；并提供一个无需先验奖励分布知识的算法及其收敛分析，借助细化的Azuma-Hoeffding不等式。

Conclusion: 本文提出一种对抗性奖励扰动下的异步Q学习算法，证明其在有限时间内的收敛率与无扰动情形匹配，仅在界中额外出现与被篡改样本比例成正比的项，并给出相应的信息论下界表明该项不可避免。

Abstract: We consider the problem of learning the optimal policy in a discounted,
infinite-horizon reinforcement learning (RL) setting where the reward signal is
subject to adversarial corruption. Such corruption, which may arise from
extreme noise, sensor faults, or malicious attacks, can severely degrade the
performance of classical algorithms such as Q-learning. To address this
challenge, we propose a new provably robust variant of the Q-learning algorithm
that operates effectively even when a fraction of the observed rewards are
arbitrarily perturbed by an adversary. Under the asynchronous sampling model
with time-correlated data, we establish that despite adversarial corruption,
the finite-time convergence rate of our algorithm matches that of existing
results for the non-adversarial case, up to an additive term proportional to
the fraction of corrupted samples. Moreover, we derive an information-theoretic
lower bound revealing that the additive corruption term in our upper bounds is
unavoidable.
  Next, we propose a variant of our algorithm that requires no prior knowledge
of the statistics of the true reward distributions. The analysis of this
setting is particularly challenging and is enabled by carefully exploiting a
refined Azuma-Hoeffding inequality for almost-martingales, a technical tool
that might be of independent interest. Collectively, our contributions provide
the first finite-time robustness guarantees for asynchronous Q-learning,
bridging a significant gap in robust RL.

</details>


### [4] [Group Distributionally Robust Machine Learning under Group Level Distributional Uncertainty](https://arxiv.org/abs/2509.08942)
*Xenia Konti,Yi Shen,Zifan Wang,Karl Henrik Johansson,Michael J. Pencina,Nicoleta J. Economou-Zavlanos,Michael M. Zavlanos*

Main category: cs.LG

TL;DR: 本文提出利用Wasserstein-DRO来提升最差组性能并处理组内分布不确定性，提出相应的梯度下降-上升算法并验证了理论与实证有效性。


<details>
  <summary>Details</summary>
Motivation: 在多源异构数据场景下，训练数据通常不足以准确估计各子群分布，导致模型学习到虚假的相关性并在小众或弱表示组上表现差，需考虑组内分布不确定性以提升最差组性能。

Method: 构建每组的Wasserstein不确定集合并将最差组性能目标嵌入DRO，设计了梯度下降-上升算法求解该最优化问题，并给出收敛性分析。

Result: 理论上给出算法收敛性证明；实验上在真实世界数据集上显示该方法能改善最差组性能并提高对分布漂移和噪声的鲁棒性。

Conclusion: 该论文提出了基于Wasserstein的分布鲁棒优化（DRO）框架，以同时提升最差组性能并考虑组内分布不确定性，实验证明在真实数据上有效。

Abstract: The performance of machine learning (ML) models critically depends on the
quality and representativeness of the training data. In applications with
multiple heterogeneous data generating sources, standard ML methods often learn
spurious correlations that perform well on average but degrade performance for
atypical or underrepresented groups. Prior work addresses this issue by
optimizing the worst-group performance. However, these approaches typically
assume that the underlying data distributions for each group can be accurately
estimated using the training data, a condition that is frequently violated in
noisy, non-stationary, and evolving environments. In this work, we propose a
novel framework that relies on Wasserstein-based distributionally robust
optimization (DRO) to account for the distributional uncertainty within each
group, while simultaneously preserving the objective of improving the
worst-group performance. We develop a gradient descent-ascent algorithm to
solve the proposed DRO problem and provide convergence results. Finally, we
validate the effectiveness of our method on real-world data.

</details>


### [5] [FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis](https://arxiv.org/abs/2509.08961)
*Md. Sajeebul Islam Sk.,Md Jobayer,Md Mehedi Hasan Shawon,Md. Golam Raibul Alam*

Main category: cs.LG

TL;DR: FoundationalECGNet fuses wavelet denoising, CBAM, GAT, and transformers for hierarchical ECG classification, reporting near-99% F1-scores and offering interpretable risk estimates.


<details>
  <summary>Details</summary>
Motivation: Address challenges in ECG analysis such as noise, class imbalance, and dataset heterogeneity to build a scalable, interpretable automated diagnostic system.

Method: Dual-stage denoising (Morlet + Daubechies wavelets), feature extraction via CBAM, spatial modeling with GAT, temporal modeling with Time Series Transformer; hierarchical classification (Normal vs Abnormal, then 5-way classification); evaluation across multiple datasets.

Result: Reported 99% F1 for Normal vs Abnormal; 99% F1 for Conduction Disorders and Hypertrophy; 98.9% F1 for Arrhythmias; state-of-the-art multi-class performance and risk-level outputs.

Conclusion: The paper proposes FoundationalECGNet, a multi-stage ECG classification framework that combines wavelet denoising, attention mechanisms, graph attention networks, and time-series transformers to achieve high accuracy and interpretability in detecting cardiac conditions; the authors claim near-perfect F1-scores across tasks and provide risk estimates.

Abstract: Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,
underscoring the importance of accurate and scalable diagnostic systems.
Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,
yet challenges such as noise, class imbalance, and dataset heterogeneity limit
current methods. To address these issues, we propose FoundationalECGNet, a
foundational framework for automated ECG classification. The model integrates a
dual-stage denoising by Morlet and Daubechies wavelets transformation,
Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),
and Time Series Transformers (TST) to jointly capture spatial and temporal
dependencies in multi-channel ECG signals. FoundationalECGNet first
distinguishes between Normal and Abnormal ECG signals, and then classifies the
Abnormal signals into one of five cardiac conditions: Arrhythmias, Conduction
Disorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Across
multiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormal
classification and shows state-of-the-art performance in multi-class disease
detection, including a 99% F1-score for Conduction Disorders and Hypertrophy,
as well as a 98.9% F1-score for Arrhythmias. Additionally, the model provides
risk level estimations to facilitate clinical decision-making. In conclusion,
FoundationalECGNet represents a scalable, interpretable, and generalizable
solution for automated ECG analysis, with the potential to improve diagnostic
precision and patient outcomes in healthcare settings. We'll share the code
after acceptance.

</details>


### [6] [Value bounds and Convergence Analysis for Averages of LRP attributions](https://arxiv.org/abs/2509.08963)
*Alexander Binder,Nastaran Takmil-Homayouni,Urun Dogan*

Main category: cs.LG

TL;DR: TL;DR: 将LRP归因表示为修改梯度矩阵乘积，通过奇异值与分量界证明了归因均值收敛性并给出乘性常数；关键发现是LRP-beta的常数与权重范数无关，提升其在数据增强和SmoothGrad下的数值稳定性。


<details>
  <summary>Details</summary>
Motivation: 动机: 理解归因方法数值稳定性与分布性质，特别是在数据增强和SmoothGrad类方法下多次采样平均的行为，以及比较不同LRP变体（例如LRP-beta与LRP-epsilon）与梯度法对权重范数敏感性的差异。

Method: 方法: 将LRP-type方法表示为一系列“修改过的梯度矩阵”的乘积，借助矩阵乘法和奇异值分析技术推导上界；对每一分量导出分量级界，并把这些界用于证明经验均值到期望的收敛速率，通过乘性常数量化该收敛性。

Result: 结果: 推导出奇异值上界与分量级边界，并证明了可控的乘性常数；表明在含多次非几何数据增强或SmoothGrad情形下，LRP-beta的归因均值收敛常数不依赖网络权重范数，而基于梯度的方法与LRP-epsilon则依赖权重范数。

Conclusion: 论文总结: 将LRP类归因方法表示为修改后的梯度矩阵乘积，类比链式法则中的雅可比矩阵连乘，推导奇异值上界和分量级边界，进而得到控制归因图经验均值收敛到期望的乘性常数；发现LRP-beta的常数不依赖权重范数，区别于基于梯度的方法和LRP-epsilon。

Abstract: We analyze numerical properties of Layer-wise relevance propagation
(LRP)-type attribution methods by representing them as a product of modified
gradient matrices. This representation creates an analogy to matrix
multiplications of Jacobi-matrices which arise from the chain rule of
differentiation. In order to shed light on the distribution of attribution
values, we derive upper bounds for singular values. Furthermore we derive
component-wise bounds for attribution map values. As a main result, we apply
these component-wise bounds to obtain multiplicative constants. These constants
govern the convergence of empirical means of attributions to expectations of
attribution maps. This finding has important implications for scenarios where
multiple non-geometric data augmentations are applied to individual test
samples, as well as for Smoothgrad-type attribution methods. In particular, our
analysis reveals that the constants for LRP-beta remain independent of weight
norms, a significant distinction from both gradient-based methods and
LRP-epsilon.

</details>


### [7] [Green Federated Learning via Carbon-Aware Client and Time Slot Scheduling](https://arxiv.org/abs/2509.08980)
*Daniel Richards Arputharaj,Charlotte Rodriguez,Angelo Rodio,Giovanni Neglia*

Main category: cs.LG

TL;DR: 本文在联邦学习中提出一种结合松弛时间、α-公平碳分配和全局微调的碳感知调度方法，能够在不同碳预算下显著降低碳排放并提升模型性能，紧碳约束下效果最佳。


<details>
  <summary>Details</summary>
Motivation: 训练大规模模型带来大量碳排放，联邦学习能利用地理和时间上的碳强度差异来降低排放，研究如何通过调度与选择在保证模型性能的同时减少碳足迹。

Method: 首先构建并分析了利用松弛时间的碳感知调度策略，量化其节能潜力；随后研究了调度带来的性能权衡（统计异质性、选择偏差、模型更新的时序相关性）；最后设计并实现一个集成松弛时间、α-公平碳分配和全局微调的碳感知调度器，并在真实碳强度数据上进行实验评估。

Result: 在真实碳强度数据上的实验表明，该碳感知调度器在多种碳预算下均优于不考虑松弛时间的基线，在紧碳约束下模型准确率提升尤为明显。

Conclusion: 提出并验证了在联邦学习中通过碳感知的客户端选择和训练调度（包括允许延期训练的“松弛时间”、α-公平碳分配和全局微调阶段）来减少碳排放，同时在不同碳预算下提升模型准确率，尤其在严格碳约束下效果显著。

Abstract: Training large-scale machine learning models incurs substantial carbon
emissions. Federated Learning (FL), by distributing computation across
geographically dispersed clients, offers a natural framework to leverage
regional and temporal variations in Carbon Intensity (CI). This paper
investigates how to reduce emissions in FL through carbon-aware client
selection and training scheduling. We first quantify the emission savings of a
carbon-aware scheduling policy that leverages slack time -- permitting a modest
extension of the training duration so that clients can defer local training
rounds to lower-carbon periods. We then examine the performance trade-offs of
such scheduling which stem from statistical heterogeneity among clients,
selection bias in participation, and temporal correlation in model updates. To
leverage these trade-offs, we construct a carbon-aware scheduler that
integrates slack time, $\alpha$-fair carbon allocation, and a global
fine-tuning phase. Experiments on real-world CI data show that our scheduler
outperforms slack-agnostic baselines, achieving higher model accuracy across a
wide range of carbon budgets, with especially strong gains under tight carbon
constraints.

</details>


### [8] [Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers](https://arxiv.org/abs/2509.08988)
*Brendan Young,Brendan Alvey,Andreas Werbrouck,Will Murphy,James Keller,Mattias J. Young,Matthew Maschmann*

Main category: cs.LG

TL;DR: 结合PyePAL+高斯过程、UMAP可视化与模糊语言摘要，本文实现了高效的多目标自适应优化与可解释化的聚合物旋涂工艺设计。


<details>
  <summary>Details</summary>
Motivation: 旋涂聚合物薄膜的力学性能由多重工艺变量决定，实验代价高且存在多目标冲突，因此需要高效的自适应多目标优化并提供可解释性以帮助专家理解与决策。

Method: 使用高斯过程回归构建多目标响应面（硬度和弹性），通过PyePAL主动选择采样点以高效逼近Pareto前沿；用UMAP将高维设计空间嵌入二维以便可视化Pareto探索路径；用模糊语言摘要把参数—性能关系转化为人类可读的模糊语言规则。

Result: 实验显示该框架能更高效地定位到有前景的工艺参数组合（更快逼近Pareto前沿），并通过二维可视化和模糊语言规则帮助专家分析设计空间与发现可迁移的经验性规律。

Conclusion: 本文提出将主动学习的Pareto前沿优化（PyePAL）与可视化（UMAP）和可解释AI（模糊语言摘要）相结合，用于自适应地通过旋涂工艺参数优化聚合物薄膜的力学特性，并能提供可理解的设计规则。

Abstract: Spin coating polymer thin films to achieve specific mechanical properties is
inherently a multi-objective optimization problem. We present a framework that
integrates an active Pareto front learning algorithm (PyePAL) with
visualization and explainable AI techniques to optimize processing parameters.
PyePAL uses Gaussian process models to predict objective values (hardness and
elasticity) from the design variables (spin speed, dilution, and polymer
mixture), guiding the adaptive selection of samples toward promising regions of
the design space. To enable interpretable insights into the high-dimensional
design space, we utilize UMAP (Uniform Manifold Approximation and Projection)
for two-dimensional visualization of the Pareto front exploration.
Additionally, we incorporate fuzzy linguistic summaries, which translate the
learned relationships between process parameters and performance objectives
into linguistic statements, thus enhancing the explainability and understanding
of the optimization results. Experimental results demonstrate that our method
efficiently identifies promising polymer designs, while the visual and
linguistic explanations facilitate expert-driven analysis and knowledge
discovery.

</details>


### [9] [ProDiGy: Proximity- and Dissimilarity-Based Byzantine-Robust Federated Learning](https://arxiv.org/abs/2509.09534)
*Sena Ergisi,Luis Maßny,Rawad Bitar*

Main category: cs.LG

TL;DR: 提出ProDiGy：基于梯度接近性与差异性的双重评分实现拜占庭鲁棒联邦学习，在非IID数据下比现有方法更稳健并能有效识别协同攻击。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中，数据异质性导致各客户端梯度本身有较大差异，现有基于相似性的防御方法在非IID场景下容易误判或失效。需要一种既能容忍自然多样性又能识别异常统一行为（如协同攻击）的防御机制。

Method: 提出了联合双重评分系统：一是衡量梯度之间的接近性（proximity），二是衡量梯度的差异性（dissimilarity）。基于这两个得分，对客户端梯度进行加权或筛选，从而削弱攻击者影响。实验中将ProDiGy与多种现有防御方法比较，覆盖IID和非IID场景及多种攻击类型。

Result: 广泛实验表明，ProDiGy在多种攻击情形和数据分布下均优于现有方法，尤其在非IID场景中仍能保持较高准确率和防御能力。检测到的一个关键点是：诚实客户端在非IID下应该表现出自然相似性，而攻击者往往显示异常一致性，ProDiGy利用这一点来识别攻击。

Conclusion: ProDiGy是一种新颖的拜占庭鲁棒联邦学习算法，通过对客户端梯度进行双重评分（基于接近度和差异性）来辨别和抑制恶意更新。在非IID数据条件下，ProDiGy相较于现有防御方法能更好地维持模型准确性和鲁棒性。

Abstract: Federated Learning (FL) emerged as a widely studied paradigm for distributed
learning. Despite its many advantages, FL remains vulnerable to adversarial
attacks, especially under data heterogeneity. We propose a new Byzantine-robust
FL algorithm called ProDiGy. The key novelty lies in evaluating the client
gradients using a joint dual scoring system based on the gradients' proximity
and dissimilarity. We demonstrate through extensive numerical experiments that
ProDiGy outperforms existing defenses in various scenarios. In particular, when
the clients' data do not follow an IID distribution, while other defense
mechanisms fail, ProDiGy maintains strong defense capabilities and model
accuracy. These findings highlight the effectiveness of a dual perspective
approach that promotes natural similarity among honest clients while detecting
suspicious uniformity as a potential indicator of an attack.

</details>


### [10] [Fast attention mechanisms: a tale of parallelism](https://arxiv.org/abs/2509.09001)
*Jingwen Liu,Hantao Yu,Clayton Sanford,Alexandr Andoni,Daniel Hsu*

Main category: cs.LG

TL;DR: ANNA 提供次二次复杂度的注意力近似，同时保持与标准注意力相当的表达能力，并能在常数深度下模拟低秩注意力，从而为高效 Transformer 的理论分析提供统一视角。


<details>
  <summary>Details</summary>
Motivation: 标准 Transformer 的自注意力为二次时间复杂度，限制了在长序列或大规模场景下的可扩展性；需要一种既高效又能保留强表达能力的注意力近似机制。

Method: 提出 Approximate Nearest Neighbor Attention (ANNA)，一种次二次时间复杂度的注意力机制；在 MPC 理论框架下证明其保留表达能力，并分析其在 Match2 与 k-hop 等推理任务上的深度与近似最优性；另外证明常数深度的 ANNA-transformers 可以模拟常数深度低秩 transformer。

Result: 给出 ANNA 的构造与复杂度分析，理论证明 ANNA-transformers 在表达能力和解决关键推理任务（如 Match2、k-hop）上的有效性与近优深度界；并证明常数深度 ANNA 能模拟常数深度低秩 transformer，从理论上统一多种高效注意力方法。

Conclusion: ANNA-transformers 在时间复杂度上优于标准自注意力，同时保留了模拟 MPC 算法的表达能力，并能在常数深度下模拟低秩高效注意力，从而统一了多种高效注意力近似方法的理论分析途径。

Abstract: Transformers have the representational capacity to simulate Massively
Parallel Computation (MPC) algorithms, but they suffer from quadratic time
complexity, which severely limits their scalability. We introduce an efficient
attention mechanism called Approximate Nearest Neighbor Attention (ANNA) with
sub-quadratic time complexity. We prove that ANNA-transformers (1) retain the
expressive power previously established for standard attention in terms of
matching the capabilities of MPC algorithms, and (2) can solve key reasoning
tasks such as Match2 and $k$-hop with near-optimal depth. Using the MPC
framework, we further prove that constant-depth ANNA-transformers can simulate
constant-depth low-rank transformers, thereby providing a unified way to reason
about a broad class of efficient attention approximations.

</details>


### [11] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: open-sci-ref发布了一组0.13B–1.7B参数、训练至最多1T token的开源Transformer基线（含中间检查点与日志），并发现NemoTron-CC HQ语料在各项基准上表现最好，便于研究者比较与复现。


<details>
  <summary>Details</summary>
Motivation: 提供可复现的开源训练基线与中间检查点，帮助研究者在相同算力轴上比较、对齐不同训练方法和数据集的效果，提升研究可比性与研究效率。

Method: 通过在多个开放参考语料（包括NemoTron-CC HQ、DCLM-baseline、FineWeb-Edu等）上训练不同参数规模的Transformer模型，并保留中间检查点、训练日志与下游评估，进行标准化评估和规模律比较。

Result: 实验表明，在所评估的数据集中，NemoTron-CC HQ始终优于其他数据集，其次是DCLM-baseline与FineWeb-Edu；此外提供的基线、检查点和日志简化了复现并支持训练动态分析。

Conclusion: 本文提出了 open-sci-ref，一套在多模型规模（0.13B 到 1.7B）和多语料规模（高达1万亿token）上训练的密集Transformer研究基线，用于在8个开放参考数据集上建立对比基准。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [12] [Deep Context-Conditioned Anomaly Detection for Tabular Data](https://arxiv.org/abs/2509.09030)
*Spencer King,Zhilu Zhang,Ruofan Yu,Baris Coskun,Wei Ding,Qian Cui*

Main category: cs.LG

TL;DR: 提出自动识别上下文特征并基于条件自编码器建模的表格数据异常检测方法，在多组基准上显著优于SOTA，强调上下文建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 动机源自现实世界表格数据常包含异质上下文（例如不同用户或设备），使得在全局分布中稀有但在特定上下文下为正常的事件被误判为异常。为避免单一全局分布忽略上下文差异，需建模条件分布。

Method: 方法包括自动挑选上下文特征并使用条件深度自编码器对数据的条件分布建模。具体做法是先识别影响数据分布的上下文变量，然后在每个上下文条件下训练自编码器（或对自编码器输入进行条件化），以估计条件重构误差作为异常评分。

Result: 在多个表格基准数据集上的大量实验显示，该方法优于现有最先进方法，证明了在异常检测中考虑上下文的重要性。

Conclusion: 该文提出了面向表格数据的基于上下文的异常检测框架，通过自动识别上下文特征并基于条件分布建模，能够更好地区分在不同上下文下的正常与异常样本。

Abstract: Anomaly detection is critical in domains such as cybersecurity and finance,
especially when working with large-scale tabular data. Yet, unsupervised
anomaly detection -- where no labeled anomalies are available -- remains a
significant challenge. Although various deep learning methods have been
proposed to model a dataset's joint distribution, real-world tabular data often
contain heterogeneous contexts (e.g., different users), making globally rare
events normal under certain contexts. Consequently, relying on a single global
distribution can overlook these contextual nuances, degrading detection
performance. In this paper, we present a context-conditional anomaly detection
framework tailored for tabular datasets. Our approach automatically identifies
context features and models the conditional data distribution using a simple
deep autoencoder. Extensive experiments on multiple tabular benchmark datasets
demonstrate that our method outperforms state-of-the-art approaches,
underscoring the importance of context in accurately distinguishing anomalous
from normal instances.

</details>


### [13] [MoWE : A Mixture of Weather Experts](https://arxiv.org/abs/2509.09052)
*Dibyajyoti Chakraborty,Romit Maulik,Peter Harrington,Dallas Foster,Mohammad Amin Nabian,Sanjay Choudhry*

Main category: cs.LG

TL;DR: 提出一种基于ViT的Mixture of Experts门控融合方法，在每格点按时效动态加权多个模型输出，低计算代价下实现比任何单模型更好的RMSE，2天尺度上最高降10%。


<details>
  <summary>Details</summary>
Motivation: 近年来数据驱动天气预报尽管达到SOTA，但进展停滞。作者通过不重做新的预报器，而是优化组合现有高质量模型，以提高整体性能并节约计算资源。

Method: 使用ViT（Vision Transformer）构建门控网络，输入包含多个专家模型的输出与预报时效信息，学习在每个网格点对各专家的权重分配，训练资源低于单一专家模型。

Result: 在2天预测时效上，MoWE在RMSE上比最佳AI天气模型降低了最多10%，显著优于单一专家与简单平均融合。

Conclusion: 该文提出将Mixture of Experts（MoE）用于融合已有数据驱动天气预报模型，通过在每个网格点基于lead time动态加权实现合成预报，从而提高精度。

Abstract: Data-driven weather models have recently achieved state-of-the-art
performance, yet progress has plateaued in recent years. This paper introduces
a Mixture of Experts (MoWE) approach as a novel paradigm to overcome these
limitations, not by creating a new forecaster, but by optimally combining the
outputs of existing models. The MoWE model is trained with significantly lower
computational resources than the individual experts. Our model employs a Vision
Transformer-based gating network that dynamically learns to weight the
contributions of multiple "expert" models at each grid point, conditioned on
forecast lead time. This approach creates a synthesized deterministic forecast
that is more accurate than any individual component in terms of Root Mean
Squared Error (RMSE). Our results demonstrate the effectiveness of this method,
achieving up to a 10% lower RMSE than the best-performing AI weather model on a
2-day forecast horizon, significantly outperforming individual experts as well
as a simple average across experts. This work presents a computationally
efficient and scalable strategy to push the state of the art in data-driven
weather prediction by making the most out of leading high-quality forecast
models.

</details>


### [14] [A Scoping Review of Machine Learning Applications in Power System Protection and Disturbance Management](https://arxiv.org/abs/2509.09053)
*Julian Oelhaf,Georg Kordowich,Mehran Pashaei,Christian Bergler,Andreas Maier,Johann Jäger,Siming Bayer*

Main category: cs.LG

TL;DR: 综述指出机器学习在电力保护中具备潜力但尚未成熟，需更多真实数据、稳健性验证和部署可行性研究，并提出分类法与标准化指南以推动研究可比性和可复现性。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源和分布式能源的大规模并网，传统保护方案面临挑战，促使研究者探索基于机器学习的更灵活、适应性强的保护与故障处理方法。

Method: 采用PRISMA-ScR流程，对100余篇文献进行筛选和分析，构建了以保护任务为中心的机器学习分类法，评估了数据集、方法学和评估指标的一致性与质量。

Result: 发现文献高度碎片化，存在术语不一致、数据集质量参差、方法透明度低和评价指标不统一的问题；提出标准化报告、数据集文档化和一致评估协议等改进建议。

Conclusion: 该综述认为当前机器学习在电力系统保护领域虽有高准确率的仿真结果，但在真实工况验证、稳健性测试和可部署性方面存在明显不足，限制了其实际应用潜力。

Abstract: The integration of renewable and distributed energy resources reshapes modern
power systems, challenging conventional protection schemes. This scoping review
synthesizes recent literature on machine learning (ML) applications in power
system protection and disturbance management, following the PRISMA for Scoping
Reviews framework. Based on over 100 publications, three key objectives are
addressed: (i) assessing the scope of ML research in protection tasks; (ii)
evaluating ML performance across diverse operational scenarios; and (iii)
identifying methods suitable for evolving grid conditions. ML models often
demonstrate high accuracy on simulated datasets; however, their performance
under real-world conditions remains insufficiently validated. The existing
literature is fragmented, with inconsistencies in methodological rigor, dataset
quality, and evaluation metrics. This lack of standardization hampers the
comparability of results and limits the generalizability of findings. To
address these challenges, this review introduces a ML-oriented taxonomy for
protection tasks, resolves key terminological inconsistencies, and advocates
for standardized reporting practices. It further provides guidelines for
comprehensive dataset documentation, methodological transparency, and
consistent evaluation protocols, aiming to improve reproducibility and enhance
the practical relevance of research outcomes. Critical gaps remain, including
the scarcity of real-world validation, insufficient robustness testing, and
limited consideration of deployment feasibility. Future research should
prioritize public benchmark datasets, realistic validation methods, and
advanced ML architectures. These steps are essential to move ML-based
protection from theoretical promise to practical deployment in increasingly
dynamic and decentralized power systems.

</details>


### [15] [STRIDE: Scalable and Interpretable XAI via Subset-Free Functional Decomposition](https://arxiv.org/abs/2509.09070)
*Chaeyun Ko*

Main category: cs.LG

TL;DR: STRIDE在RKHS中用递归核中心化实现无子集枚举的正交功能分解，提升可扩展性并提供比标量归因更丰富的解释，实验证明在表格数据上有显著速度与保真度优势。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法受限于对子集枚举的指数开销以及把影响简化为标量值导致的表达力损失，作者旨在通过函数分解在RKHS中同时缓解这两点。

Method: 在RKHS里将解释表示为对各特征子集的函数分解，使用递归的核中心化程序构造解析投影以得到每个子集的功能分量f_S(x_S)，从而无需枚举所有特征子集；方法为模型无关，并提供局部与全局视角。

Result: 在10个公开表格数据集上，STRIDE相对于基线（如TreeSHAP）在速度上中位数加速约3.0倍，最快9.7倍，最慢0.6倍；同时在R^2上达到0.81到0.999，且大多数数据集上保持显著的秩一致性，并能进行诸如“分量手术”的交互影响度量。

Conclusion: STRIDE提出了一种在RKHS中基于正交功能分解且避免子集枚举的可扩展XAI框架，通过递归核中心化的解析投影计算分量函数，兼顾局部与全局解释，并在理论与实证上展示了正交性、L^2收敛性及较高的拟合保真度和计算加速。

Abstract: Most explainable AI (XAI) frameworks face two practical limitations: the
exponential cost of reasoning over feature subsets and the reduced
expressiveness of summarizing effects as single scalar values. We present
STRIDE, a scalable framework that aims to mitigate both issues by framing
explanation as a subset-enumeration-free, orthogonal functional decomposition
in a Reproducing Kernel Hilbert Space (RKHS). Rather than focusing only on
scalar attributions, STRIDE computes functional components f_S(x_S) via an
analytical projection scheme based on a recursive kernel-centering procedure,
avoiding explicit subset enumeration. In the tabular setups we study, the
approach is model-agnostic, provides both local and global views, and is
supported by theoretical results on orthogonality and L^2 convergence under
stated assumptions. On public tabular benchmarks in our environment, we
observed speedups ranging from 0.6 times (slower than TreeSHAP on a small
dataset) to 9.7 times (California), with a median approximate 3.0 times across
10 datasets, while maintaining high fidelity (R^2 between 0.81 and 0.999) and
substantial rank agreement on most datasets. Overall, STRIDE complements scalar
attribution methods by offering a structured functional perspective, enabling
novel diagnostics like 'component surgery' to quantitatively measure the impact
of specific interactions within our experimental scope.

</details>


### [16] ["A 6 or a 9?": Ensemble Learning Through the Multiplicity of Performant Models and Explanations](https://arxiv.org/abs/2509.09073)
*Gianlucca Zuin,Adriano Veloso*

Main category: cs.LG

TL;DR: 利用基于性能与解释的分组从高性能模型集合中挑选多样化代表，构建更鲁棒的集成以改善泛化，尤其在Rashomon效应显著时效果明显。


<details>
  <summary>Details</summary>
Motivation: 在存在多种同样表现良好但决策机制不同的模型时，传统单一模型或随机集成可能无法覆盖解空间的多样性，导致在分布转移或未见数据上泛化不佳。因此有必要有策略地选择多样化且仍高性能的模型构成集成。

Method: 作者首先识别一组在验证集上表现相近的高性能模型（Rashomon集），然后根据模型预测与解释（例如特征重要性或局部解释）对这些模型进行聚类/分组，从每组中选取代表模型构建集成，强调覆盖不同解的区域以增加鲁棒性。

Result: 在公开与企业内部真实协作数据集上验证，Rashomon Ensemble在Rashomon比率高的情况下可带来最高约0.20+的AUROC提升，并在多个实际业务场景展示鲁棒性与实用性。

Conclusion: 该论文提出的Rashomon Ensemble通过在高性能模型集合中基于性能与解释的分组选择多样化模型，从而提高泛化能力，尤其在Rashomon比率高的场景能显著提升AUROC。

Abstract: Creating models from past observations and ensuring their effectiveness on
new data is the essence of machine learning. However, selecting models that
generalize well remains a challenging task. Related to this topic, the Rashomon
Effect refers to cases where multiple models perform similarly well for a given
learning problem. This often occurs in real-world scenarios, like the
manufacturing process or medical diagnosis, where diverse patterns in data lead
to multiple high-performing solutions. We propose the Rashomon Ensemble, a
method that strategically selects models from these diverse high-performing
solutions to improve generalization. By grouping models based on both their
performance and explanations, we construct ensembles that maximize diversity
while maintaining predictive accuracy. This selection ensures that each model
covers a distinct region of the solution space, making the ensemble more robust
to distribution shifts and variations in unseen data. We validate our approach
on both open and proprietary collaborative real-world datasets, demonstrating
up to 0.20+ AUROC improvements in scenarios where the Rashomon ratio is large.
Additionally, we demonstrate tangible benefits for businesses in various
real-world applications, highlighting the robustness, practicality, and
effectiveness of our approach.

</details>


### [17] [An entropy formula for the Deep Linear Network](https://arxiv.org/abs/2509.09088)
*Govind Menon,Tianmin Yu*

Main category: cs.LG

TL;DR: 本文用群作用与黎曼子浸映研究DLN的黎曼几何，构造了平衡流形切空间的正交基，进而定义并计算了玻尔兹曼熵，并说明可观测量空间的几何可由子浸映得到。


<details>
  <summary>Details</summary>
Motivation: 将学习过程的动力学与热力学概念（如熵）联系起来，提供几何视角理解过参数化与可观测量之间的关系。

Method: 通过群作用分析过参数化结构，使用黎曼子浸映从参数空间到可观测量空间传递度量，并利用Jacobi矩阵理论显式构造平衡流形切空间的正交基。

Result: 证明了可观测量空间上引用的黎曼几何可由平衡流形通过黎曼子浸映得到；给出平衡流形上叶片（群轨道）导致的玻尔兹曼熵表达式；提供了构造切空间正交基的具体方法。

Conclusion: 论文构建了深线性网络(DLN)参数空间的黎曼几何框架，利用群作用与黎曼子浸映将过参数化和可观测量空间联系起来，并在平衡流形上定义并计算了玻尔兹曼熵。

Abstract: We study the Riemannian geometry of the Deep Linear Network (DLN) as a
foundation for a thermodynamic description of the learning process. The main
tools are the use of group actions to analyze overparametrization and the use
of Riemannian submersion from the space of parameters to the space of
observables. The foliation of the balanced manifold in the parameter space by
group orbits is used to define and compute a Boltzmann entropy. We also show
that the Riemannian geometry on the space of observables defined in [2] is
obtained by Riemannian submersion of the balanced manifold. The main technical
step is an explicit construction of an orthonormal basis for the tangent space
of the balanced manifold using the theory of Jacobi matrices.

</details>


### [18] [Sensitivity-LoRA: Low-Load Sensitivity-Based Fine-Tuning for Large Language Models](https://arxiv.org/abs/2509.09119)
*Hao Zhang,Bo Huang,Zhenjia Li,Xi Xiao,Hui Yi Leong,Zumeng Zhang,Xinwei Long,Tianyang Wang,Hao Xu*

Main category: cs.LG

TL;DR: 提出一种基于海森敏感性的动态秩分配LoRA方法，能高效地为不同权重矩阵分配低秩，从而提升微调效果与稳定性，同时保持低计算成本。


<details>
  <summary>Details</summary>
Motivation: 动机在于传统LoRA对所有增量矩阵采用统一秩r，无法兼顾不同层和参数块的重要性；现有的秩分配方法复杂或代价高，因而需要一种高效、稳定且实用的动态秩分配方案。

Method: 方法包括计算权重更新的全局与局部敏感性，利用海森信息评估各层的重要性，并据此分配不同的低秩r给每个增量矩阵；设计了低开销的近似海森计算策略以减少计算成本，结合标准LoRA训练流程实现实际微调。

Result: 实验表明Sensitivity-LoRA在多个任务与基准上相较于统一r的LoRA以及其他秩分配方法，能在相同参数预算下取得更好的任务性能、训练效率与稳定性；并显著降低额外计算开销。

Conclusion: 该论文提出Sensitivity-LoRA，通过基于损失函数二阶导数（海森矩阵）评估权重敏感性，动态分配LoRA秩以提高参数高效微调效果；在多种任务和基准上展示了更高的性能、效率与稳定性。

Abstract: Large Language Models (LLMs) have transformed both everyday life and
scientific research. However, adapting LLMs from general-purpose models to
specialized tasks remains challenging, particularly in resource-constrained
environments. Low-Rank Adaptation (LoRA), a prominent method within
Parameter-Efficient Fine-Tuning (PEFT), has emerged as a promising approach to
LLMs by approximating model weight updates using low-rank decomposition.
However, LoRA is limited by its uniform rank ( r ) allocation to each
incremental matrix, and existing rank allocation techniques aimed at addressing
this issue remain computationally inefficient, complex, and unstable, hindering
practical applications. To address these limitations, we propose
Sensitivity-LoRA, an efficient fine-tuning method that dynamically allocates
ranks to weight matrices based on both their global and local sensitivities. It
leverages the second-order derivatives (Hessian Matrix) of the loss function to
effectively capture weight sensitivity, enabling optimal rank allocation with
minimal computational overhead. Our experimental results have demonstrated
robust effectiveness, efficiency and stability of Sensitivity-LoRA across
diverse tasks and benchmarks.

</details>


### [19] [Learning What Matters: Causal Time Series Modeling for Arctic Sea Ice Prediction](https://arxiv.org/abs/2509.09128)
*Emam Hossain,Md Osman Gani*

Main category: cs.LG

TL;DR: 将MVGC和PCMCI+用于因果特征选择并与混合神经网络结合，用于北极海冰范围时序预测，结果显示因果输入提升了准确性、可解释性与效率，方法可推广到其他高维动态问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习多基于相关性，易受虚假关联影响，导致模型在分布漂移与外推场景下性能下降且缺乏可解释性，作者希望通过引入因果推断提升模型的可靠性与可解释性。

Method: 使用多变量Granger因果性(MVGC)与PCMCI+对1979-2021年北极海冰范围及相关海气变量进行因果特征选择，构建仅包含因果驱动输入的混合神经网络（文中未详述网络具体结构，但提及混合架构），并在不同预报时滞下评估预测性能与可解释性。

Result: 实验显示，使用因果选取的输入变量能提升不同预报时滞下的预测精度、减少冗余特征并提高计算效率；论文宣称该方法对高维动态系统具有可扩展性，并在理论与实践上推进了因果感知预测建模。

Conclusion: 该论文提出了将因果发现方法（MVGC与PCMCI+）与混合神经网络相结合的因果感知深度学习框架，以改善海冰时序预测的鲁棒性、可解释性与泛化能力。

Abstract: Conventional machine learning and deep learning models typically rely on
correlation-based learning, which often fails to distinguish genuine causal
relationships from spurious associations, limiting their robustness,
interpretability, and ability to generalize. To overcome these limitations, we
introduce a causality-aware deep learning framework that integrates
Multivariate Granger Causality (MVGC) and PCMCI+ for causal feature selection
within a hybrid neural architecture. Leveraging 43 years (1979-2021) of Arctic
Sea Ice Extent (SIE) data and associated ocean-atmospheric variables at daily
and monthly resolutions, the proposed method identifies causally influential
predictors, prioritizes direct causes of SIE dynamics, reduces unnecessary
features, and enhances computational efficiency. Experimental results show that
incorporating causal inputs leads to improved prediction accuracy and
interpretability across varying lead times. While demonstrated on Arctic SIE
forecasting, the framework is broadly applicable to other dynamic,
high-dimensional domains, offering a scalable approach that advances both the
theoretical foundations and practical performance of causality-informed
predictive modeling.

</details>


### [20] [Continuous-Time Value Iteration for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2509.09135)
*Xuefeng Wang,Lei Zhang,Henglin Pu,Ahmed H. Qureshi,Husheng Li*

Main category: cs.LG

TL;DR: 将PINNs与VGI结合用于连续时间多智能体RL，通过对值梯度的迭代精炼提升HJB价值近似，从而得到更稳定、更优的策略，且在MPE和MuJoCo连续化实验中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 传统RL在高频或不规则时间交互下表现差，CTRL能用HJB刻画连续时间价值但在多智能体中受维数灾难和中心化值函数近似困难影响，导致策略不稳定。

Method: 用PINNs来逼近HJB方程定义的连续时间值函数，并通过VGI对值梯度进行迭代更新，以保证值与其微分结构一致；在多智能体设置下采用集中化值函数近似并在连续时间多智能体基准（MPE、MuJoCo连续化）上训练与评估。

Result: 在连续时间多智能体基准上，CT-MARL优于现有连续时间RL基线，并能扩展到更复杂的多智能体动力学环境。

Conclusion: 本文提出将物理信息神经网络(PINNs)与连续时间多智能体强化学习(CT-MARL)相结合，通过引入值梯度迭代(VGI)模块，在轨迹上迭代精炼值函数梯度，提升梯度保真度与价值函数近似，进而增强策略学习稳定性与性能。

Abstract: Existing reinforcement learning (RL) methods struggle with complex dynamical
systems that demand interactions at high frequencies or irregular time
intervals. Continuous-time RL (CTRL) has emerged as a promising alternative by
replacing discrete-time Bellman recursion with differential value functions
defined as viscosity solutions of the Hamilton--Jacobi--Bellman (HJB) equation.
While CTRL has shown promise, its applications have been largely limited to the
single-agent domain. This limitation stems from two key challenges: (i)
conventional solution methods for HJB equations suffer from the curse of
dimensionality (CoD), making them intractable in high-dimensional systems; and
(ii) even with HJB-based learning approaches, accurately approximating
centralized value functions in multi-agent settings remains difficult, which in
turn destabilizes policy training. In this paper, we propose a CT-MARL
framework that uses physics-informed neural networks (PINNs) to approximate
HJB-based value functions at scale. To ensure the value is consistent with its
differential structure, we align value learning with value-gradient learning by
introducing a Value Gradient Iteration (VGI) module that iteratively refines
value gradients along trajectories. This improves gradient fidelity, in turn
yielding more accurate values and stronger policy learning. We evaluate our
method using continuous-time variants of standard benchmarks, including
multi-agent particle environment (MPE) and multi-agent MuJoCo. Our results
demonstrate that our approach consistently outperforms existing continuous-time
RL baselines and scales to complex multi-agent dynamics.

</details>


### [21] [Peering Partner Recommendation for ISPs using Machine Learning](https://arxiv.org/abs/2509.09146)
*Md Ibrahim Ibne Alam,Ankur Senapati,Anindo Mahmood,Murat Yuksel,Koushik Kar*

Main category: cs.LG

TL;DR: 用PeeringDB/CAIDA等公开数据训练机器学习模型（尤其是XGBoost），可高效、鲁棒地预测ISP是否应建立对等关系，达到约98%准确率，可用于自动化对等伙伴选择。


<details>
  <summary>Details</summary>
Motivation: 对等连接优于转发（transit）但建立对等关系过程耗时且复杂，自动化选择对等伙伴可提升运营效率并优化互联网互联结构，因此探索能利用公开数据预测对等关系的ML方法。

Method: 收集公开数据源（如PeeringDB、CAIDA等）关于ISP的属性与连接关系，构建标签化数据集（是否对等），并训练三类ML模型：树基（XGBoost等）、神经网络、Transformer。通过交叉验证和鲁棒性测试（时间、空间迁移，以及缺失数据）评估模型性能。

Result: XGBoost在测试中达到约98%准确率，推断性能优于神经网络和Transformer，且对时间、地理空间变化与数据缺失表现出较好鲁棒性，表明公开数据足以训练高性能分类器来预测ISP对等关系。

Conclusion: 本文提出并验证了基于公开数据和机器学习（主要是XGBoost）自动预测ISP对等关系的方法，结论是树模型尤其是XGBoost在准确率（98%）和鲁棒性（时间、空间、缺失数据）方面优于神经网络与变换器模型，可用于自动化对等伙伴选择以提升全球互联网效率。

Abstract: Internet service providers (ISPs) need to connect with other ISPs to provide
global connectivity services to their users. To ensure global connectivity,
ISPs can either use transit service(s) or establish direct peering
relationships between themselves via Internet exchange points (IXPs). Peering
offers more room for ISP-specific optimizations and is preferred, but it often
involves a lengthy and complex process. Automating peering partner selection
can enhance efficiency in the global Internet ecosystem. We explore the use of
publicly available data on ISPs to develop a machine learning (ML) model that
can predict whether an ISP pair should peer or not. At first, we explore public
databases, e.g., PeeringDB, CAIDA, etc., to gather data on ISPs. Then, we
evaluate the performance of three broad types of ML models for predicting
peering relationships: tree-based, neural network-based, and transformer-based.
Among these, we observe that tree-based models achieve the highest accuracy and
efficiency in our experiments. The XGBoost model trained with publicly
available data showed promising performance, with a 98% accuracy rate in
predicting peering partners. In addition, the model demonstrated great
resilience to variations in time, space, and missing data. We envision that
ISPs can adopt our method to fully automate the peering partner selection
process, thus transitioning to a more efficient and optimized Internet
ecosystem.

</details>


### [22] [HISPASpoof: A New Dataset For Spanish Speech Forensics](https://arxiv.org/abs/2509.09155)
*Maria Risques,Kratika Bhagtani,Amit Kumar Singh Yadav,Edward J. Delp*

Main category: cs.LG

TL;DR: HISPASpoof是首个大规模西班牙语真/合成语音数据集（6种口音，6个zero-shot TTS），实验证明可提升检测和归属性能，弥补了西班牙语在语音取证领域的空白。


<details>
  <summary>Details</summary>
Motivation: 现有语音检测器主要集中在英语和中文，而西班牙语使用者众多但数据严重不足。零样本TTS和VC技术能够生成高度逼真的合成语音，造成潜在滥用风险，因此需要专门的西班牙语基准数据集来提升检测和归属能力。

Method: 从六个方言的公开语料中收集真实语音，使用六个零样本（zero-shot）TTS系统生成合成语音，构建包含真实与合成样本的数据集；在该数据集上评估五种代表性检测方法，并测试检测器从英语到西班牙语的泛化能力；还评估了合成语音的归属任务（识别生成方法）。

Result: 实验表明：用英语训练的检测器无法良好泛化到西班牙语；在HISPASpoof上训练能显著提升检测性能；并提供了合成语音归属评估结果，展示了该数据集对推进西班牙语语音取证的价值。

Conclusion: 该论文提出了HISPASpoof，这是首个大规模西班牙语合成语音检测与归属数据集，填补了西班牙语在语音取证中的空白。

Abstract: Zero-shot Voice Cloning (VC) and Text-to-Speech (TTS) methods have advanced
rapidly, enabling the generation of highly realistic synthetic speech and
raising serious concerns about their misuse. While numerous detectors have been
developed for English and Chinese, Spanish-spoken by over 600 million people
worldwide-remains underrepresented in speech forensics. To address this gap, we
introduce HISPASpoof, the first large-scale Spanish dataset designed for
synthetic speech detection and attribution. It includes real speech from public
corpora across six accents and synthetic speech generated with six zero-shot
TTS systems. We evaluate five representative methods, showing that detectors
trained on English fail to generalize to Spanish, while training on HISPASpoof
substantially improves detection. We also evaluate synthetic speech attribution
performance on HISPASpoof, i.e., identifying the generation method of synthetic
speech. HISPASpoof thus provides a critical benchmark for advancing reliable
and inclusive speech forensics in Spanish.

</details>


### [23] [Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication](https://arxiv.org/abs/2509.09168)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis*

Main category: cs.LG

TL;DR: 训练-free的自适应Token合并+贝叶斯优化构建Pareto前沿，能在边缘语义通信中按需权衡准确率与计算/传输开销，适应信道变化并显著降低FLOPs。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer在语义通信中表现优秀但计算代价高，限制了在资源受限的6G边缘设备上的部署。需要一种无需训练即可减少计算和传输的方案，同时在噪声无线信道下保持鲁棒性。

Method: 将每层的合并比例建模为多目标优化问题（在准确率和计算开销之间权衡），并使用基于高斯过程的贝叶斯优化来搜索Pareto前沿，从而在运行时根据需求和信道条件灵活选择配置。

Result: 实验显示该方法在各种SNR条件下，在保持竞争性准确率的同时显著降低浮点运算量（FLOPs），优于其他基线方法，并且自适应策略能根据信道质量调整合并强度，有效权衡延迟与语义保真度。

Conclusion: 该论文提出了一个训练-free的自适应Token合并框架，用于预训练视觉Transformer，在不重新训练模型的情况下同时缩短推理时间和减少传输资源。

Abstract: Large-scale transformer models have emerged as a powerful tool for semantic
communication systems, enabling edge devices to extract rich representations
for robust inference across noisy wireless channels. However, their substantial
computational demands remain a major barrier to practical deployment in
resource-constrained 6G networks. In this paper, we present a training-free
framework for adaptive token merging in pretrained vision transformers to
jointly reduce inference time and transmission resource usage. We formulate the
selection of per-layer merging proportions as a multi-objective optimization
problem to balance accuracy and computational cost. We employ Gaussian
process-based Bayesian optimization to construct a Pareto frontier of optimal
configurations, enabling flexible runtime adaptation to dynamic application
requirements and channel conditions. Extensive experiments demonstrate that our
method consistently outperforms other baselines and achieves significant
reductions in floating-point operations while maintaining competitive accuracy
across a wide range of signal-to-noise ratio (SNR) conditions. Additional
results highlight the effectiveness of adaptive policies that adjust merging
aggressiveness in response to channel quality, providing a practical mechanism
to trade off latency and semantic fidelity on demand. These findings establish
a scalable and efficient approach for deploying transformer-based semantic
communication in future edge intelligence systems.

</details>


### [24] [Quantum Machine Learning, Quantitative Trading, Reinforcement Learning, Deep Learning](https://arxiv.org/abs/2509.09176)
*Jun-Hao Chen,Yu-Chien Huang,Yun-Cheng Tsai,Samuel Yen-Chi Chen*

Main category: cs.LG

TL;DR: 作者以QLSTM+QA3C構建USD/TWD長倉交易代理，使用2000-2025資料回測，取得11.87%收益且最大回撤0.92%，顯示量子啟發混合模型在FX交易中具競爭力，但受限於古典量子模擬與策略簡化。


<details>
  <summary>Details</summary>
Motivation: 探索量子啟發架構能否提升金融時序預測與強化學習策略的表現，特別是在外匯小幅獲利且需嚴格風險控制的場景。

Method: 設計以QLSTM為特徵提取器（序列長度=4）輸出短期趨勢，輸入QA3C多工worker（8個）並行訓練以產生策略；狀態包含QLSTM特徵與技術指標，報酬函數偏向趨勢追隨並加入風險控制項；在2000-01-01至2025-04-30資料上以80%訓練/20%測試切分進行回測。

Result: 長倉代理在約5年期間實現11.87%總回報、最大回撤0.92%，表現優於若干貨幣ETF；指出QLSTM在小利潤、低回撤交易中有效。

Conclusion: 本文展示量子啟發神經網絡與深度強化學習在外匯交易上的可行性，結合QLSTM作短期趨勢預測與QA3C作交易決策，實驗結果在USD/TWD長倉策略上取得優於多數貨幣ETF的收益與較低回撤，證明混合模型在小利潤高風控場景具競爭力。

Abstract: The convergence of quantum-inspired neural networks and deep reinforcement
learning offers a promising avenue for financial trading. We implemented a
trading agent for USD/TWD by integrating Quantum Long Short-Term Memory (QLSTM)
for short-term trend prediction with Quantum Asynchronous Advantage
Actor-Critic (QA3C), a quantum-enhanced variant of the classical A3C. Trained
on data from 2000-01-01 to 2025-04-30 (80\% training, 20\% testing), the
long-only agent achieves 11.87\% return over around 5 years with 0.92\% max
drawdown, outperforming several currency ETFs. We detail state design (QLSTM
features and indicators), reward function for trend-following/risk control, and
multi-core training. Results show hybrid models yield competitive FX trading
performance. Implications include QLSTM's effectiveness for small-profit trades
with tight risk and future enhancements. Key hyperparameters: QLSTM sequence
length$=$4, QA3C workers$=$8. Limitations: classical quantum simulation and
simplified strategy. \footnote{The views expressed in this article are those of
the authors and do not represent the views of Wells Fargo. This article is for
informational purposes only. Nothing contained in this article should be
construed as investment advice. Wells Fargo makes no express or implied
warranties and expressly disclaims all legal, tax, and accounting implications
related to this article.

</details>


### [25] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: 针对序列强化学习中固定截断对短/长生成不公的问题，FSPO在log-IS比率上做KL校正且按√L缩放的带状截断，理论与实证均显示能实现长度公平并提升性能。


<details>
  <summary>Details</summary>
Motivation: 观察到将PPO/GRPO风格的固定截断范围应用到序列级别会系统性地对短/长回应重新加权，扭曲目标，需在重要性采样权重空间实现长度公平性。

Method: 提出FSPO：在序列级别强化学习中，对序列对数重要性比率进行带KL修正的带状截断，截断带宽随序列长度按√L缩放，源于对高斯近似的动机，从而在权重空间实现长度公平。

Result: 理论上引入LRE并证明小LRE下的方向余弦保证；实证上FSPO在多个评估数据集上提高稳定性、平坦化不同长度区间的截断率并优于所有基线。

Conclusion: FSPO通过在重要性采样权重空间中直接进行长度公平截断，解决了将PPO/GRPO类截断直接应用于序列导致的短长回答重加权偏差问题，理论上通过引入Length Reweighting Error (LRE)刻画长度不公平并证明小LRE可保证截断更新方向与真实更新有良好夹角，方法上采用带KL修正漂移项且随序列长度按√L缩放的对数IS比率截断；实验上在不同数据集上使截断率在长度区间上更均匀、训练更稳定并优于基线。

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [26] [Breaking the Statistical Similarity Trap in Extreme Convection Detection](https://arxiv.org/abs/2509.09195)
*Md Tanveer Hossain Munim*

Main category: cs.LG

TL;DR: 本文揭示并定量化了深度学习天气建模的“统计相似性陷阱”，提出DART——一种面向极端对流检测的双解码器回归框架，显著提升对低于220K亮温事件的检测能力并具备快速训练与操作校准能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估标准偏好模糊预测，忽视稀有高影响事件，导致对极端天气的低效检测与不可信的模型评估。

Method: 提出DART框架：双解码器专门处理背景与极端部分，使用基于物理的过采样策略和针对性损失函数，对粗尺度大气预报进行细尺度卫星亮温回归并优化220K以下的极端对流检测。

Result: 实验证明传统基线在相关性上可达97.9%但CSI为0，DART在CSI、偏差和操作灵活性上显著优于基线（例如CSI=0.273，bias=2.52），并在2023年8月吉大港洪灾案例中得到实证验证。

Conclusion: DART有效地解决了模型评估中的“统计相似性陷阱”，通过双解码器、背景/极端分解、物理驱动过采样及任务相关损失实现对极端对流事件的高精度检测和可操作性校准。

Abstract: Current evaluation metrics for deep learning weather models create a
"Statistical Similarity Trap", rewarding blurry predictions while missing rare,
high-impact events. We provide quantitative evidence of this trap, showing
sophisticated baselines achieve 97.9% correlation yet 0.00 CSI for dangerous
convection detection. We introduce DART (Dual Architecture for Regression
Tasks), a framework addressing the challenge of transforming coarse atmospheric
forecasts into high-resolution satellite brightness temperature fields
optimized for extreme convection detection (below 220 K). DART employs
dual-decoder architecture with explicit background/extreme decomposition,
physically motivated oversampling, and task-specific loss functions. We present
four key findings: (1) empirical validation of the Statistical Similarity Trap
across multiple sophisticated baselines; (2) the "IVT Paradox", removing
Integrated Water Vapor Transport, widely regarded as essential for atmospheric
river analysis, improves extreme convection detection by 270%; (3)
architectural necessity demonstrated through operational flexibility (DART
achieves CSI = 0.273 with bias = 2.52 vs. 6.72 for baselines at equivalent
CSI), and (4) real-world validation with the August 2023 Chittagong flooding
disaster as a case study. To our knowledge, this is the first work to
systematically address this hybrid conversion-segmentation-downscaling task,
with no direct prior benchmarks identified in existing literature. Our
validation against diverse statistical and deep learning baselines sufficiently
demonstrates DART's specialized design. The framework enables precise
operational calibration through beta-tuning, trains in under 10 minutes on
standard hardware, and integrates seamlessly with existing meteorological
workflows, demonstrating a pathway toward trustworthy AI for extreme weather
preparedness.

</details>


### [27] [Incentivizing Safer Actions in Policy Optimization for Constrained Reinforcement Learning](https://arxiv.org/abs/2509.09208)
*Somnath Hazra,Pallab Dasgupta,Soumyajit Dey*

Main category: cs.LG

TL;DR: 提出IP3O：在PPO中引入逐步增强的惩罚以稳定受约束RL训练，并提供实验与理论支持。


<details>
  <summary>Details</summary>
Motivation: 连续控制场景中，策略优化在接近约束边界时不稳定，导致训练性能不佳；因此需要一种能在接近边界前主动保持约束满足的机制。

Method: 在标准奖励之外引入自适应激励（惩罚）项，并在PPO的优化过程中逐渐加重该惩罚；算法实现为Incrementally Penalized Proximal Policy Optimization（IP3O）。

Result: 在基准环境上，IP3O优于若干最先进的安全RL算法；并给出最坏情形下最优性误差界的理论保证。

Conclusion: 该论文提出了IP3O，通过逐步增加的惩罚机制在PPO基础上维持训练稳定性，从而在连续控制的受约束RL中改善收益-约束权衡。

Abstract: Constrained Reinforcement Learning (RL) aims to maximize the return while
adhering to predefined constraint limits, which represent domain-specific
safety requirements. In continuous control settings, where learning agents
govern system actions, balancing the trade-off between reward maximization and
constraint satisfaction remains a significant challenge. Policy optimization
methods often exhibit instability near constraint boundaries, resulting in
suboptimal training performance. To address this issue, we introduce a novel
approach that integrates an adaptive incentive mechanism in addition to the
reward structure to stay within the constraint bound before approaching the
constraint boundary. Building on this insight, we propose Incrementally
Penalized Proximal Policy Optimization (IP3O), a practical algorithm that
enforces a progressively increasing penalty to stabilize training dynamics.
Through empirical evaluation on benchmark environments, we demonstrate the
efficacy of IP3O compared to the performance of state-of-the-art Safe RL
algorithms. Furthermore, we provide theoretical guarantees by deriving a bound
on the worst-case error of the optimality achieved by our algorithm.

</details>


### [28] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 本文通过文献综述+LASSO特征选择，结合多种分类器验证农业旅游增长指标，结果显示LASSO+Logistic Regression在提出的数据分割下达到98–99%准确率，优于RF、DT和XGBoost。


<details>
  <summary>Details</summary>
Motivation: 农业旅游是快速发展的旅游子领域，有潜力促进农村经济多元化并保护本土文化与传统农业实践。为实现可持续增长，需要系统识别与验证影响农业旅游发展的关键指标。

Method: 两阶段方法：第一阶段通过全面文献综述识别潜在指标；第二阶段使用机器学习特征选择（主要为LASSO）结合分类器（LR、DT、RF、XGBoost）在70-30%及80-20%训练-测试分割下评估各指标的预测能力，比较各模型分类准确率。

Result: 在LASSO特征选择下，LR在70-30%分割下达98%准确率，RF 95%；在80-20%分割下LR达99%，DT和XGBoost达97%。总体表明采用LASSO与LR的组合在本研究数据与设置中效果最佳。

Conclusion: 该研究通过文献回顾与机器学习特征选择相结合，识别并验证了促进农业旅游（agro-tourism）增长的重要指标，结论为基于LASSO选择的特征，Logistic Regression在分类任务上表现最佳（在给定实验设置下准确率达98–99%），Random Forest、Decision Tree与XGBoost次之。

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [29] [Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement](https://arxiv.org/abs/2509.09219)
*Jakob Nyberg,Pontus Johnson*

Main category: cs.LG

TL;DR: 提出Vejde框架：用二分图与图神经网络把结构化MDP映射为可归纳的策略表示，支持变规模问题；实验表明在RDDL多个领域上对未见实例具有良好泛化，性能接近每实例训练的MLP。


<details>
  <summary>Details</summary>
Motivation: 传统基于表征的策略在面对结构化、高可变规模的环境时难以泛化；希望构建能利用对象和关系信息的归纳策略，能在未见实例上迁移并保持性能。

Method: 将MDP状态表示为事实数据库，把每个状态转换为二分图（实体与事实节点），通过消息传递的图神经网络映射到潜在状态；利用事实化的状态与动作表示，训练可归纳的策略网络。训练方式包括监督学习与强化学习；评估在RDDL定义的八个领域每个十个实例，按训练/测试实例划分，比较Vejde、每实例MLP基线与在线规划器Prost。

Result: Vejde在未见测试实例上的平均得分与在训练实例上相比无显著下降；在多数领域中归纳策略的平均表现接近每实例训练的MLP代理，且通常优于或不逊色于在线规划器Prost。

Conclusion: Vejde提出了一种将数据抽象、图神经网络与强化学习结合的框架，能够对具有丰富结构状态（实体与关系）的决策问题学习归纳策略函数，并在不同规模和结构的问题实例上实现良好泛化。

Abstract: We present and evaluate Vejde; a framework which combines data abstraction,
graph neural networks and reinforcement learning to produce inductive policy
functions for decision problems with richly structured states, such as object
classes and relations. MDP states are represented as data bases of facts about
entities, and Vejde converts each state to a bipartite graph, which is mapped
to latent states through neural message passing. The factored representation of
both states and actions allows Vejde agents to handle problems of varying size
and structure. We tested Vejde agents on eight problem domains defined in RDDL,
with ten problem instances each, where policies were trained using both
supervised and reinforcement learning. To test policy generalization, we
separate problem instances in two sets, one for training and the other solely
for testing. Test results on unseen instances for the Vejde agents were
compared to MLP agents trained on each problem instance, as well as the online
planning algorithm Prost. Our results show that Vejde policies in average
generalize to the test instances without a significant loss in score.
Additionally, the inductive agents received scores on unseen test instances
that on average were close to the instance-specific MLP agents.

</details>


### [30] [Constructing a Question-Answering Simulator through the Distillation of LLMs](https://arxiv.org/abs/2509.09226)
*Haipeng Liu,Ting Long,Jing Fu*

Main category: cs.LG

TL;DR: LDSim通过蒸馏LLM的知识与推理到轻量序列模型，兼顾性能与效率，提升答题模拟与知识追踪效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-free方法速度快但效果较弱，LLM-based方法效果好但推理慢且占用大；目标是在保持高效性的前提下，借助LLM知识提升模拟器性能。

Method: 设计一种LLM蒸馏框架：利用LLM生成关于QA历史的增强特征/推理提示，构建蒸馏目标（如软标签、注意力/中间表示或推理步骤），并用这些目标训练一个传统序列模型（如RNN/Transformer或更轻量的KT模型），实现知识和推理能力的迁移。

Result: 在多个模拟任务和知识追踪任务上，LDSim显著优于传统LLM-free方法，并在性能上接近或达到部分LLM-based方法，同时推理速度更快、GPU内存占用更低。并公开代码。

Conclusion: 本论文提出LDSim，通过将LLM的领域知识和推理能力蒸馏到轻量模拟器中，在保持推理速度和内存效率的同时提升答题模拟与知识追踪性能。

Abstract: The question-answering (QA) simulator is a model that mimics real student
learning behaviors and predicts their correctness of their responses to
questions. QA simulators enable educational recommender systems (ERS) to
collect large amounts of training data without interacting with real students,
thereby preventing harmful recommendations made by an undertrained ERS from
undermining actual student learning. Given the QA history, there are two
categories of solutions to predict the correctness, conducting the simulation:
(1) LLM-free methods, which apply a traditional sequential model to transfer
the QA history into a vector representation first, and make predictions based
on the representation; (2) LLM-based methods, which leverage the domain
knowledge and reasoning capability of LLM to enhence the prediction. LLM-free
methods offer fast inference but generally yield suboptimal performance. In
contrast, most LLM-based methods achieve better results, but at the cost of
slower inference speed and higher GPU memory consumption. In this paper, we
propose a method named LLM Distillation based Simulator (LDSim), which distills
domain knowledge and reasoning capability from an LLM to better assist
prediction, thereby improving simulation performance. Extensive experiments
demonstrate that our LDSim achieves strong results on both the simulation task
and the knowledge tracing (KT) task. Our code is publicly available at
https://anonymous.4open.science/r/LDSim-05A9.

</details>


### [31] [Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis](https://arxiv.org/abs/2509.09251)
*Hanyang Wang,Yuxuan Yang,Hongjun Wang,Lihui Wang*

Main category: cs.LG

TL;DR: 提出MMT-FD：结合时频增强、元学习与少量对比学习，能从未标注数据提取表征并用1%标注样本实现接近99%的故障诊断准确率，适用于不同旋转机械的泛化诊断。


<details>
  <summary>Details</summary>
Motivation: 动机是工业场景中获取充足标注故障样本困难且昂贵，同时不同设备特性差异大导致模型难以泛化，因此需要一种能从未标注数据中提取鲁棒表征并在少量标注下快速适应的诊断方法。

Method: 方法包括时间-频率域编码器与元学习泛化模型的结合：首先对未标注振动数据进行时频随机增强，编码器学习状态表征；然后将增强数据输入元学习网络进行分类与泛化训练，最后用少量标注样本进行微调。整个训练过程中引入小规模对比学习迭代以提升效率。

Result: 在滚动轴承数据集和转子试验台上验证，宣称在仅1%标注样本情况下达到99%故障诊断准确率，且具有较强的跨设备泛化能力。

Conclusion: 该论文提出了一种用于旋转机械少样本无监督故障诊断的多注意力元变换器（MMT-FD），旨在解决工业中标注样本稀缺和模型泛化差的问题。

Abstract: The intelligent fault diagnosis of rotating mechanical equipment usually
requires a large amount of labeled sample data. However, in practical
industrial applications, acquiring enough data is both challenging and
expensive in terms of time and cost. Moreover, different types of rotating
mechanical equipment with different unique mechanical properties, require
separate training of diagnostic models for each case. To address the challenges
of limited fault samples and the lack of generalizability in prediction models
for practical engineering applications, we propose a Multi-Attention Meta
Transformer method for few-shot unsupervised rotating machinery fault diagnosis
(MMT-FD). This framework extracts potential fault representations from
unlabeled data and demonstrates strong generalization capabilities, making it
suitable for diagnosing faults across various types of mechanical equipment.
The MMT-FD framework integrates a time-frequency domain encoder and a
meta-learning generalization model. The time-frequency domain encoder predicts
status representations generated through random augmentations in the
time-frequency domain. These enhanced data are then fed into a meta-learning
network for classification and generalization training, followed by fine-tuning
using a limited amount of labeled data. The model is iteratively optimized
using a small number of contrastive learning iterations, resulting in high
efficiency. To validate the framework, we conducted experiments on a bearing
fault dataset and rotor test bench data. The results demonstrate that the
MMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeled
sample data, exhibiting robust generalization capabilities.

</details>


### [32] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 本文识别并修正了策略梯度与熵耦合带来的学习动力学问题，提出EMPG依据步骤不确定性重标度更新并引入未来清晰度奖励，有效提升长时序LLM代理的学习效率与性能。


<details>
  <summary>Details</summary>
Motivation: 长时序任务中结果型稀疏奖励导致难以为中间步骤分配信用；且策略梯度大小与策略熵耦合，造成自信正确动作更新过小、自信错误更新过大，阻碍学习稳定性与效率。

Method: 提出Entropy-Modulated Policy Gradients（EMPG）：根据每一步的不确定性和最终任务成败，放大对自信且正确动作的更新、惩罚自信错误、减弱不确定步骤的更新以稳定探索；并加入未来清晰度奖励（future clarity bonus）鼓励可预测的解路径。通过与强基线的比较验证方法有效。

Result: 在WebShop、ALFWorld和Deep Search三项挑战性任务上，EMPG相较于强的策略梯度基线取得了显著的性能提升和更稳定的训练行为。

Conclusion: EMPG通过根据步骤不确定性和最终结果调节策略梯度的幅度，有效解决了LLM基代理在长时序任务中因稀疏奖励与熵耦合导致的学习效率问题，显著提升了在WebShop、ALFWorld和Deep Search等任务上的表现。

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [33] [Data Driven Discovery of Emergent Dynamics in Reaction Diffusion Systems from Sparse and Noisy Observations](https://arxiv.org/abs/2509.09278)
*Saumitra Dwivedi,Ricardo da Silva Torres,Ibrahim A. Hameed,Gunnar Tufte,Anniken Susanne T. Karlsen*

Main category: cs.LG

TL;DR: 提出DRSALife，用数据驱动学习Soft ALife规则以识别反应-扩散系统涌现动力学；在合成任务上达成约74%预测精度，并能较好抵抗噪声与稀疏数据，同时实现PDE参数辨识。


<details>
  <summary>Details</summary>
Motivation: 在许多涉及涌现动力学的领域中（神经科学、生态学、流行病学等），常常缺乏对底层物理模型的先验知识，传统基于PDE的方法难以直接建立或识别系统。将Soft ALife（Agent-based/CA）作为通用、可解释的离散规则体系，用数据驱动方法自动学习规则，可实现无物理先验的系统识别与预测。

Method: 提出了DRSALife数据驱动规则学习框架，将观测时空数据映射到Agent-based和元胞自动机规则集合。方法包括规则表示、搜索/优化策略（可能结合进化算法或梯度方法）、噪声与稀疏数据处理策略，以及基于学习规则再构建PDE的参数辨识流程。通过对合成反应-扩散数据进行训练和评估，使用准确率等指标量化预测性能，并在不同噪声和采样稀疏性条件下测试鲁棒性。

Result: 学习得到的规则在预测涌现行为方面总体准确率约74%，并在添加高斯噪声和时间稀疏观测下保持较好性能。此外，该框架还能成功辨识对应反应-扩散PDE的结构与参数，验证了从离散规则到连续方程的可逆映射能力。

Conclusion: 本文提出并验证了DRSALife框架，用于从观测数据中学习Soft ALife规则集，以无物理先验条件下建模反应-扩散系统的涌现动力学。实验表明该方法在多种示例（Rule 30、生命游戏、Vicsek群聚）上能较好重现动力学，并能在噪声和时间稀疏性下保持鲁棒性，同时还可识别出对应PDE的结构与参数。

Abstract: Data-driven discovery of emergent dynamics is gaining popularity,
particularly in the context of reaction-diffusion systems. These systems are
widely studied across various fields, including neuroscience, ecology,
epidemiology, and several other subject areas that deal with emergent dynamics.
A current challenge in the discovery process relates to system identification
when there is no prior knowledge of the underlying physics. We attempt to
address this challenge by learning Soft Artificial Life (Soft ALife) models,
such as Agent-based and Cellular Automata (CA) models, from observed data for
reaction-diffusion systems. In this paper, we present findings on the
applicability of a conceptual framework, the Data-driven Rulesets for Soft
Artificial Life (DRSALife) model, to learn Soft ALife rulesets that accurately
represent emergent dynamics in a reaction-diffusion system from observed data.
This model has demonstrated promising results for Elementary CA Rule 30, Game
of Life, and Vicsek Flocking problems in recent work. To our knowledge, this is
one of the few studies that explore machine-based Soft ALife ruleset learning
and system identification for reaction-diffusion dynamics without any prior
knowledge of the underlying physics. Moreover, we provide comprehensive
findings from experiments investigating the potential effects of using noisy
and sparse observed datasets on learning emergent dynamics. Additionally, we
successfully identify the structure and parameters of the underlying partial
differential equations (PDEs) representing these dynamics. Experimental results
demonstrate that the learned models are able to predict the emergent dynamics
with good accuracy (74%) and exhibit quite robust performance when subjected to
Gaussian noise and temporal sparsity.

</details>


### [34] [MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts](https://arxiv.org/abs/2509.09337)
*Junda Ye,Zhongbao Zhang,Li Sun,Siqiang Luo*

Main category: cs.LG

TL;DR: 提出MoSE：用匿名游走提取子图并动态路由到子图专家，增强GNN的高阶结构表示能力，理论与实验均支持其优越性与可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN依赖局部一阶消息传递，难以捕捉复杂高阶子图模式；既有将随机游走核融入GNN的工作通常只针对图级任务且内核固定，灵活性与适用性受限。

Method: 从图中提取匿名游走子图（anonymous walks），对子图进行动态路由（routing）到不同专家网络（experts），每个专家专注于特定结构语义，最终融合输出用于下游任务；并进行了SWL可表达性分析和实证评估。

Result: 理论上证明MoSE优于SWL测试，实验证明在多个基准与任务（包括节点/图级）上超越竞争方法，并通过可视化展示了不同专家捕捉的结构模式，提升了可解释性。

Conclusion: MoSE通过子图专家混合机制提高了GNN对复杂高阶结构模式的表达能力，在理论上超越了子图WL测试，并在多任务上表现优于基线，同时提供可解释性。

Abstract: While graph neural networks (GNNs) have achieved great success in learning
from graph-structured data, their reliance on local, pairwise message passing
restricts their ability to capture complex, high-order subgraph patterns.
leading to insufficient structural expressiveness. Recent efforts have
attempted to enhance structural expressiveness by integrating random walk
kernels into GNNs. However, these methods are inherently designed for
graph-level tasks, which limits their applicability to other downstream tasks
such as node classification. Moreover, their fixed kernel configurations hinder
the model's flexibility in capturing diverse subgraph structures. To address
these limitations, this paper proposes a novel Mixture of Subgraph Experts
(MoSE) framework for flexible and expressive subgraph-based representation
learning across diverse graph tasks. Specifically, MoSE extracts informative
subgraphs via anonymous walks and dynamically routes them to specialized
experts based on structural semantics, enabling the model to capture diverse
subgraph patterns with improved flexibility and interpretability. We further
provide a theoretical analysis of MoSE's expressivity within the Subgraph
Weisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.
Extensive experiments, together with visualizations of learned subgraph
experts, demonstrate that MoSE not only outperforms competitive baselines but
also provides interpretable insights into structural patterns learned by the
model.

</details>


### [35] [Robust Non-Linear Correlations via Polynomial Regression](https://arxiv.org/abs/2509.09380)
*Luca Giuliani,Michele Lombardi*

Main category: cs.LG

TL;DR: 用多项式核限制HGR计算，获得更稳健、确定性的估计与可用次梯度，适合约束机器学习中的正则化应用。


<details>
  <summary>Details</summary>
Motivation: 现有可微HGR估计器受不可计算性导致的偏差-方差权衡影响，可能在实际应用中不够稳健，因此需要一种更可控、更可靠的计算方法。

Method: 构造用户可配置的多项式核来近似或限制HGR优化问题，从而得到更稳定、确定性的数值解；并在受约束学习任务中计算该量及其次梯度进行实验验证。

Result: 提出的方法在鲁棒性和确定性上优于已有方法，计算速度更快且效果几乎相当；实验表明该方法能产生有意义的次梯度，可用于作为损失正则化。

Conclusion: 作者提出了基于多项式核的HGR相关系数可计算方法，强调比先前可微估计器更稳健、确定性更强，并能在受约束的机器学习中作为正则项提供有用的次梯度。

Abstract: The Hirschfeld-Gebelein-R\'enyi (HGR) correlation coefficient is an extension
of Pearson's correlation that is not limited to linear correlations, with
potential applications in algorithmic fairness, scientific analysis, and causal
discovery. Recently, novel algorithms to estimate HGR in a differentiable
manner have been proposed to facilitate its use as a loss regularizer in
constrained machine learning applications. However, the inherent
uncomputability of HGR requires a bias-variance trade-off, which can possibly
compromise the robustness of the proposed methods, hence raising technical
concerns if applied in real-world scenarios. We introduce a novel computational
approach for HGR that relies on user-configurable polynomial kernels, offering
greater robustness compared to previous methods and featuring a faster yet
almost equally effective restriction. Our approach provides significant
advantages in terms of robustness and determinism, making it a more reliable
option for real-world applications. Moreover, we present a brief experimental
analysis to validate the applicability of our approach within a constrained
machine learning framework, showing that its computation yields an insightful
subgradient that can serve as a loss regularizer.

</details>


### [36] [MetaLLMix : An XAI Aided LLM-Meta-learning Based Approach for Hyper-parameters Optimization](https://arxiv.org/abs/2509.09387)
*Mohammed Tiouti,Mohamed Bal-Ghaoui*

Main category: cs.LG

TL;DR: MetaLLMiX 用历史实验+SHAP+元学习+本地轻量 LLM 的零样本方案，实现了可解释、迅速且节省成本的超参数与模型选择，性能与传统 HPO 可比并大幅降低时间与 API 成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于 LLM 的自动机器学习方法依赖重复试错和昂贵的 API，缺乏可解释性与泛化性；希望在无需额外计算的情况下，实现高效、可解释且可本地部署的超参数/模型选择。

Method: 构建历史实验数据库并用 SHAP 解释各超参数对性能的影响，利用这些可解释特征作为输入，结合元学习模型来预测最优超参数与预训练模型；采用轻量级开源 LLM 本地部署作为结果评判器（LLM-as-judge）以控制输出格式与完整性。

Result: 在 8 个医学影像数据集上与传统 HPO 方法比较，MetaLLMiX 在 5/8 任务取得最优结果，推理响应时间降低 99.6–99.9%，训练时间在 6 个数据集上加速 2.4–15.7 倍，精度与最好基线相差仅 1–5%。

Conclusion: MetaLLMiX 提出了一种结合元学习、可解释 AI（SHAP）与高效 LLM 推理的零样本超参数与模型选择框架，能够基于历史实验结果直接推荐超参数和预训练模型，无需额外训练试验。

Abstract: Effective model and hyperparameter selection remains a major challenge in
deep learning, often requiring extensive expertise and computation. While
AutoML and large language models (LLMs) promise automation, current LLM-based
approaches rely on trial and error and expensive APIs, which provide limited
interpretability and generalizability. We propose MetaLLMiX, a zero-shot
hyperparameter optimization framework combining meta-learning, explainable AI,
and efficient LLM reasoning. By leveraging historical experiment outcomes with
SHAP explanations, MetaLLMiX recommends optimal hyperparameters and pretrained
models without additional trials. We further employ an LLM-as-judge evaluation
to control output format, accuracy, and completeness. Experiments on eight
medical imaging datasets using nine open-source lightweight LLMs show that
MetaLLMiX achieves competitive or superior performance to traditional HPO
methods while drastically reducing computational cost. Our local deployment
outperforms prior API-based approaches, achieving optimal results on 5 of 8
tasks, response time reductions of 99.6-99.9%, and the fastest training times
on 6 datasets (2.4-15.7x faster), maintaining accuracy within 1-5% of
best-performing baselines.

</details>


### [37] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: LLM 的自生成反事实解释常在有效性与最小性间权衡，整体不可依赖，可能误导高风险应用。


<details>
  <summary>Details</summary>
Motivation: 评估 LLM 自生成反事实解释（SCE）作为可解释性工具的可靠性，尤其在协同人类决策的高风险场景中的适用性。

Method: 设计自动化评估框架，衡量 SCE 的有效性（是否达到目标预测）与最小性（编辑距离等度量），在多种 LLM、数据集和设置上进行实证比较。

Result: 发现 LLM 生成的反事实通常有效但不最小；在要求最小性时又倾向于过小修改导致无效。此现象在不同模型和数据集上稳定存在，表明 SCE 可能提供误导性解释。

Conclusion: SCEs 对 LLM 的可解释性作用有限，常见有效性与最小性之间存在权衡；在要求最小性时又常导致无效微调，可能误导用户。

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [38] [Kriging prior Regression: A Case for Kriging-Based Spatial Features with TabPFN in Soil Mapping](https://arxiv.org/abs/2509.09408)
*Jonas Schmidinger,Viacheslav Barkov,Sebastian Vogel,Martin Atzmueller,Gerard B M Heuvelink*

Main category: cs.LG

TL;DR: 提出KpR：用普通克里金生成空间滞后特征，结合TabPFN能在小样本土壤映射中显著提升预测和不确定性估计，平均R2提升约30%。


<details>
  <summary>Details</summary>
Motivation: 弥合地质统计学（依赖空间结构）与机器学习（依赖环境特征）之间的差异，通过将空间信息以特征形式融入ML模型，提高土壤属性预测与不确定性估计，尤其应对小样本情景。

Method: 通过从普通克里金插值得到的空间滞后特征（KpR）增强机器学习输入，使用TabPFN模型对六个LimeSoDa数据集（SOC、粘土、pH及遥感/近地测土特征）进行点与概率预测评估，并与回归/残差克里金和非空间ML算法（如随机森林）比较。

Result: KpR+TabPFN在平均R2上比无空间信息的ML算法提升约30%，并在不确定性估计上更可靠，优于回归/残差克里金及其他空间技术；提升来自TabPFN强预测能力与KpR提供的互补空间信息。

Conclusion: KpR结合TabPFN在小样本精度农业土壤映射任务中表现稳健，能显著提高预测精度和不确定性估计质量，尤其在原位传感器数据有限时补偿ML特征弱关系。

Abstract: Machine learning and geostatistics are two fundamentally different frameworks
for predicting and spatially mapping soil properties. Geostatistics leverages
the spatial structure of soil properties, while machine learning captures the
relationship between available environmental features and soil properties. We
propose a hybrid framework that enriches ML with spatial context through
engineering of 'spatial lag' features from ordinary kriging. We call this
approach 'kriging prior regression' (KpR), as it follows the inverse logic of
regression kriging. To evaluate this approach, we assessed both the point and
probabilistic prediction performance of KpR, using the TabPFN model across six
fieldscale datasets from LimeSoDa. These datasets included soil organic carbon,
clay content, and pH, along with features derived from remote sensing and
in-situ proximal soil sensing. KpR with TabPFN demonstrated reliable
uncertainty estimates and more accurate predictions in comparison to several
other spatial techniques (e.g., regression/residual kriging with TabPFN), as
well as to established non-spatial machine learning algorithms (e.g., random
forest). Most notably, it significantly improved the average R2 by around 30%
compared to machine learning algorithms without spatial context. This
improvement was due to the strong prediction performance of the TabPFN
algorithm itself and the complementary spatial information provided by KpR
features. TabPFN is particularly effective for prediction tasks with small
sample sizes, common in precision agriculture, whereas KpR can compensate for
weak relationships between sensing features and soil properties when proximal
soil sensing data are limited. Hence, we conclude that KpR with TabPFN is a
very robust and versatile modelling framework for digital soil mapping in
precision agriculture.

</details>


### [39] [Fused Lasso Improves Accuracy of Co-occurrence Network Inference in Grouped Samples](https://arxiv.org/abs/2509.09413)
*Daniel Agyapong,Briana H. Beatty,Peter G. Kennedy,Toby D. Hocking*

Main category: cs.LG

TL;DR: 将样本按生态位区分，提出SAC评估并用融合多任务方法（fuser）学习环境特异网络，在跨环境预测中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有共现网络推断多在单一环境静态样本上进行，且常将不同环境样本合并导致忽视环境间关联变化；需要评估算法在跨环境、跨时间条件下的泛化能力并构建能保留环境特异性同时借用共享信息的网络推断方法。

Method: 提出SAC评估框架并收集多位置多时间点的公开微生物丰度数据，比较现有方法（如glmnet）与新方法fuser的预测性能；fuser通过融合（fused）正则化或多任务学习思想使模型同时学习共享和环境特异参数，生成环境特定网络而非单一泛化网络；在Same与All场景下进行定量对比，展示fuser在All场景下的测试误差下降。

Result: 实验证明fuser在同一生态位内（Same）表现与glmnet等方法相当，而在跨生态位（All）场景下显著降低测试误差，说明其在保留子样本特异性并共享有用信息方面优势明显。

Conclusion: 该论文指出现有共现网络推断方法通常只在单一生态位的样本上构建静态网络，忽视了微生物群落在不同空间和时间环境下的动态适应性；提出并验证了同-全交叉验证（SAC）框架，区分在相同生态位内训练测试（Same）与跨生态位训练测试（All）的绩效差异；提出fuser方法，通过在保留子样本特异信号的同时跨环境共享信息，学习每个环境的专属预测网络，从而在跨环境测试中显著降低误差。

Abstract: Co-occurrence network inference algorithms have significantly advanced our
understanding of microbiome communities. However, these algorithms typically
analyze microbial associations within samples collected from a single
environmental niche, often capturing only static snapshots rather than dynamic
microbial processes. Previous studies have commonly grouped samples from
different environmental niches together without fully considering how microbial
communities adapt their associations when faced with varying ecological
conditions. Our study addresses this limitation by explicitly investigating
both spatial and temporal dynamics of microbial communities. We analyzed
publicly available microbiome abundance data across multiple locations and time
points, to evaluate algorithm performance in predicting microbial associations
using our proposed Same-All Cross-validation (SAC) framework. SAC evaluates
algorithms in two distinct scenarios: training and testing within the same
environmental niche (Same), and training and testing on combined data from
multiple environmental niches (All). To overcome the limitations of
conventional algorithms, we propose fuser, an algorithm that, while not
entirely new in machine learning, is novel for microbiome community network
inference. It retains subsample-specific signals while simultaneously sharing
relevant information across environments during training. Unlike standard
approaches that infer a single generalized network from combined data, fuser
generates distinct, environment-specific predictive networks. Our results
demonstrate that fuser achieves comparable predictive performance to existing
algorithms such as glmnet when evaluated within homogeneous environments
(Same), and notably reduces test error compared to baseline algorithms in
cross-environment (All) scenarios.

</details>


### [40] [Composable Score-based Graph Diffusion Model for Multi-Conditional Molecular Generation](https://arxiv.org/abs/2509.09451)
*Anjie Qiao,Zhen Wang,Chuan Chen,DeFu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: CSGD通过concrete scores将分数匹配推广到离散图，结合可组合引导与概率校准，实现灵活且高效的多条件分子生成，显著提高可控性并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有图扩散模型在多条件分子生成场景中受限于联合条件或连续松弛方法，这些方法会降低生成物对离散属性的忠实性，因此需要一种能在离散图上提供灵活且原则性的条件操控的方法。

Method: 引入了concrete scores将分数匹配应用于离散分子图，并提出两种技术：可组合引导（CoG）用于在采样时对任意子集条件进行细粒度控制；概率校准（PC）用于调整估计的转移概率以缓解训练-测试不匹配。

Result: 在四个分子数据集上的实验证明CSGD在可控性方面平均提升15.3%，同时在有效性与分布拟合度上保持较高水平，表明其在多属性分子设计上的实用优势。

Conclusion: 本文提出了一种基于分数匹配的可组合图扩散模型CSGD，通过将分数匹配扩展到离散图（使用concrete scores），实现对条件引导的灵活操控。

Abstract: Controllable molecular graph generation is essential for material and drug
discovery, where generated molecules must satisfy diverse property constraints.
While recent advances in graph diffusion models have improved generation
quality, their effectiveness in multi-conditional settings remains limited due
to reliance on joint conditioning or continuous relaxations that compromise
fidelity. To address these limitations, we propose Composable Score-based Graph
Diffusion model (CSGD), the first model that extends score matching to discrete
graphs via concrete scores, enabling flexible and principled manipulation of
conditional guidance. Building on this foundation, we introduce two score-based
techniques: Composable Guidance (CoG), which allows fine-grained control over
arbitrary subsets of conditions during sampling, and Probability Calibration
(PC), which adjusts estimated transition probabilities to mitigate train-test
mismatches. Empirical results on four molecular datasets show that CSGD
achieves state-of-the-art performance, with a 15.3% average improvement in
controllability over prior methods, while maintaining high validity and
distributional fidelity. Our findings highlight the practical advantages of
score-based modeling for discrete graph generation and its capacity for
flexible, multi-property molecular design.

</details>


### [41] [AquaCast: Urban Water Dynamics Forecasting with Precipitation-Informed Multi-Input Transformer](https://arxiv.org/abs/2509.09458)
*Golnoosh Abdollahinejad,Saleh Baghersalimi,Denisa-Andreea Constantinescu,Sergey Shevchik,David Atienza*

Main category: cs.LG

TL;DR: AquaCast通过嵌入层融合外源输入并建模变量间与时间依赖，实现了对城市水系统的高精度多点多步预测，在真实与合成数据上均优于基线。


<details>
  <summary>Details</summary>
Motivation: 解决城市水动力学预测中同时存在多变量交互和外源影响（如降水及预报）的问题，并提高泛化与扩展性。

Method: 提出多输入多输出深度学习模型AquaCast，使用嵌入层融合外源变量以避免对其直接预测，并同时建模变量间和时间依赖；在真实与三种合成数据集上进行评估。

Result: 在LausanneCity及三个人工生成的大规模数据集上，AquaCast优于现有基线方法，尤其在仅用内源变量时已达SOTA，加入外源信息进一步提升性能。

Conclusion: AquaCast在多个数据集上表现优越，尤其在仅使用内源变量时已达标；加入外源变量和预报报告能进一步提升性能；在合成和真实数据上均有稳健泛化能力。

Abstract: This work addresses the challenge of forecasting urban water dynamics by
developing a multi-input, multi-output deep learning model that incorporates
both endogenous variables (e.g., water height or discharge) and exogenous
factors (e.g., precipitation history and forecast reports). Unlike conventional
forecasting, the proposed model, AquaCast, captures both inter-variable and
temporal dependencies across all inputs, while focusing forecast solely on
endogenous variables. Exogenous inputs are fused via an embedding layer,
eliminating the need to forecast them and enabling the model to attend to their
short-term influences more effectively. We evaluate our approach on the
LausanneCity dataset, which includes measurements from four urban drainage
sensors, and demonstrate state-of-the-art performance when using only
endogenous variables. Performance also improves with the inclusion of exogenous
variables and forecast reports. To assess generalization and scalability, we
additionally test the model on three large-scale synthesized datasets,
generated from MeteoSwiss records, the Lorenz Attractors model, and the Random
Fields model, each representing a different level of temporal complexity across
100 nodes. The results confirm that our model consistently outperforms existing
baselines and maintains a robust and accurate forecast across both real and
synthetic datasets.

</details>


### [42] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: 提出一个利用AI代理+RPA的端到端自动化管道（Agent-E），能够高效发现特定地域的会议论文并自动完成后续操作，验证显示高召回与近乎完美准确率。


<details>
  <summary>Details</summary>
Motivation: 应对学术文献快速增长，减轻研究者、资助机构和学会在学术发现和后续繁琐行政操作上的人工负担。

Method: 构建名为Agent-E的专用AI代理，结合文献检索与规则/模型判定目标地域论文，然后通过RPA机器人执行预设操作（如提交提名表）。在586篇来自5个会议的论文上进行实验验证。

Result: 在586篇论文的验证集中，系统召回率为100%，准确率为99.4%，成功识别并对目标论文执行自动化操作。

Conclusion: 本文提出并验证了一个从文献发现到自动执行动作的端到端系统，表明任务型AI代理在学术工作流中能有效筛选并主动参与流程自动化。

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [43] [CountTRuCoLa: Rule Confidence Learning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2509.09474)
*Julia Gastinger,Christian Meilicke,Heiner Stuckenschmidt*

Main category: cs.LG

TL;DR: 提出基于四类时间规则和时间-频率置信度的可解释TKG预测方法，性能优异且可解释。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经网络的方法效果好但缺乏可解释性；近期工作用循环事实作为强基线，激发了用简单规则实现高性能且可解释的方案。

Method: 学习四种简单规则类型，使用考虑时间近期性和频率的置信度函数，并结合循环事实作为基线思想进行预测。

Result: 在九个数据集上与八个最先进模型和两个基线比较，匹配或超过其性能，并生成可解释的预测。

Conclusion: 提出了一种基于时间规则的可解释TKG预测方法，能够在多个数据集上与最先进方法匹敌或超越，同时提供可解释性。

Abstract: We address the task of temporal knowledge graph (TKG) forecasting by
introducing a fully explainable method based on temporal rules. Motivated by
recent work proposing a strong baseline using recurrent facts, our approach
learns four simple types of rules with a confidence function that considers
both recency and frequency. Evaluated on nine datasets, our method matches or
surpasses the performance of eight state-of-the-art models and two baselines,
while providing fully interpretable predictions.

</details>


### [44] [Balancing Utility and Privacy: Dynamically Private SGD with Random Projection](https://arxiv.org/abs/2509.09485)
*Zhanhong Jiang,Md Zahid Hasan,Nastaran Saadati,Aditya Balu,Chao Liu,Soumik Sarkar*

Main category: cs.LG

TL;DR: 论文提出D2P2-SGD：结合动态差分隐私（自动裁剪）与随机投影的SGD，理论与实验证明在隐私约束下可更高效地训练大模型并提高精度。


<details>
  <summary>Details</summary>
Motivation: 静态DPSGD噪声机制影响模型误差界，且随模型参数量增长，随机优化器效率下降；需在隐私保护与模型效用间取得更灵活的平衡并提升大模型学习效率。

Method: 提出D2P2-SGD优化器：整合动态差分隐私（自动梯度裁剪）与随机投影+SGD，允许动态调整隐私-效用折中，并给出收敛性证明。

Result: 理论上D2P2-SGD在多类目标函数上得到次线性（sub-linear）收敛率，与最优速率匹配；分析表明动态差分隐私可在牺牲隐私的情况下提升效用，随机投影提高学习效率；实验在多数据集上显示精度显著提升并保持隐私。

Conclusion: D2P2-SGD将动态差分隐私与随机投影结合，在保持隐私的同时提升了模型性能与优化效率。

Abstract: Stochastic optimization is a pivotal enabler in modern machine learning,
producing effective models for various tasks. However, several existing works
have shown that model parameters and gradient information are susceptible to
privacy leakage. Although Differentially Private SGD (DPSGD) addresses privacy
concerns, its static noise mechanism impacts the error bounds for model
performance. Additionally, with the exponential increase in model parameters,
efficient learning of these models using stochastic optimizers has become more
challenging. To address these concerns, we introduce the Dynamically
Differentially Private Projected SGD (D2P2-SGD) optimizer. In D2P2-SGD, we
combine two important ideas: (i) dynamic differential privacy (DDP) with
automatic gradient clipping and (ii) random projection with SGD, allowing
dynamic adjustment of the tradeoff between utility and privacy of the model. It
exhibits provably sub-linear convergence rates across different objective
functions, matching the best available rate. The theoretical analysis further
suggests that DDP leads to better utility at the cost of privacy, while random
projection enables more efficient model learning. Extensive experiments across
diverse datasets show that D2P2-SGD remarkably enhances accuracy while
maintaining privacy. Our code is available here.

</details>


### [45] [PIPES: A Meta-dataset of Machine Learning Pipelines](https://arxiv.org/abs/2509.09512)
*Cynthia Moreira Maia,Lucas B. V. de Amorim,George D. C. Cavalcanti,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: PIPES是一个针对流水线组合全面覆盖的实验集合：9,408流水线×300数据集，提供详细运行结果以补足OpenML在预处理多样性方面的不足，支持元学习研究。


<details>
  <summary>Details</summary>
Motivation: 当前OpenML记录在流水线多样性和预处理步骤代表性方面存在不足，导致元学习研究样本偏倚和覆盖不全，亟需一个更加全面且结构化的实验集合。

Method: 系统化生成并运行9,408条不同的机器学习流水线（涵盖数据预处理块的多种技术组合），在300个数据集上记录训练/测试时间、预测、性能指标与错误信息，形成可扩展的数据集。

Result: 构建并公开了PIPES：包含9,408条流水线在300个数据集上的实验结果，数据和代码已开源，支持更均衡、多样的元学习分析。

Conclusion: PIPES通过构建涵盖多种流水线组合的实验集合，有效弥补了OpenML在预处理步骤多样性和代表性上的不足，为算法选择等元学习研究提供了更全面的实验基准。

Abstract: Solutions to the Algorithm Selection Problem (ASP) in machine learning face
the challenge of high computational costs associated with evaluating various
algorithms' performances on a given dataset. To mitigate this cost, the
meta-learning field can leverage previously executed experiments shared in
online repositories such as OpenML. OpenML provides an extensive collection of
machine learning experiments. However, an analysis of OpenML's records reveals
limitations. It lacks diversity in pipelines, specifically when exploring data
preprocessing steps/blocks, such as scaling or imputation, resulting in limited
representation. Its experiments are often focused on a few popular techniques
within each pipeline block, leading to an imbalanced sample. To overcome the
observed limitations of OpenML, we propose PIPES, a collection of experiments
involving multiple pipelines designed to represent all combinations of the
selected sets of techniques, aiming at diversity and completeness. PIPES stores
the results of experiments performed applying 9,408 pipelines to 300 datasets.
It includes detailed information on the pipeline blocks, training and testing
times, predictions, performances, and the eventual error messages. This
comprehensive collection of results allows researchers to perform analyses
across diverse and representative pipelines and datasets. PIPES also offers
potential for expansion, as additional data and experiments can be incorporated
to support the meta-learning community further. The data, code, supplementary
material, and all experiments can be found at
https://github.com/cynthiamaia/PIPES.git.

</details>


### [46] [Cough Classification using Few-Shot Learning](https://arxiv.org/abs/2509.09515)
*Yoga Disha Sendhil Kumar,Manas V Shetty,Sudip Vhaduri*

Main category: cs.LG

TL;DR: 使用原型网络与咳嗽谱图，少样本学习在呼吸音疾病检测中可行：15个示例/类即可获得约75%多类准确率，二分类均>70%，多类与二类性能无显著差异。


<details>
  <summary>Details</summary>
Motivation: 医疗领域标注数据稀缺，传统深度学习需大量标注样本。研究旨在验证少样本学习是否可在有限标注条件下实现可靠的呼吸音疾病检测，并比较多类与二类模型性能差异。

Method: 使用咳嗽声谱图作为输入，采用Prototypical Networks进行少样本学习；设计多类（COVID-19/流感/健康）与二分类（类对）实验，设置每类15个Support示例评估模型性能；对比分类准确率并进行配对t检验与Wilcoxon检验。

Result: 多类分类在每类15个Support下达成74.87%准确率；各二分类任务准确率均超过70%；类别分析显示流感最易区分，健康样本最难区分；统计检验（配对t检验p=0.149，Wilcoxon p=0.125）未发现二类与多类模型性能显著差异。

Conclusion: 本研究表明在呼吸音（咳嗽）分类任务中，基于原型网络的少样本学习在样本稀缺时仍能达到与传统深度学习接近的性能，支持多类分类可行性。

Abstract: This paper investigates the effectiveness of few-shot learning for
respiratory sound classification, focusing on coughbased detection of COVID-19,
Flu, and healthy conditions. We leverage Prototypical Networks with spectrogram
representations of cough sounds to address the challenge of limited labeled
data. Our study evaluates whether few-shot learning can enable models to
achieve performance comparable to traditional deep learning approaches while
using significantly fewer training samples. Additionally, we compare
multi-class and binary classification models to assess whether multi-class
models can perform comparably to their binary counterparts. Experimental
findings show that few-shot learning models can achieve competitive accuracy.
Our model attains 74.87% accuracy in multi-class classification with only 15
support examples per class, while binary classification achieves over 70%
accuracy across all class pairs. Class-wise analysis reveals Flu as the most
distinguishable class, and Healthy as the most challenging. Statistical tests
(paired t-test p = 0.149, Wilcoxon p = 0.125) indicate no significant
performance difference between binary and multiclass models, supporting the
viability of multi-class classification in this setting. These results
highlight the feasibility of applying few-shot learning in medical diagnostics,
particularly when large labeled datasets are unavailable.

</details>


### [47] [Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication](https://arxiv.org/abs/2509.09597)
*Maysam Behmanesh,Erkan Turan,Maks Ovsjanikov*

Main category: cs.LG

TL;DR: 提出双通道谱编码+几何函数映射的无监督对齐框架，提升节点辨识度并校准潜在空间几何，实验证明在图与视觉语言对齐任务上表现优越且稳健。


<details>
  <summary>Details</summary>
Motivation: 现有无监督嵌入方法存在两大问题：GNN导致的过平滑使节点辨识度下降，以及结构噪声/特征异质性/训练不稳引起的潜在空间错位，导致对齐失败。

Method: 设计了一个双通道编码器（低通+高通谱滤波）以兼顾结构感知与高辨识度，同时引入基于几何的函数映射模块以学习双射且近等距的潜在空间变换，从而实现跨图几何关系一致性。

Result: 在多个图基准上，该方法在鲁棒性和准确率上均优于现有无监督对齐基线；在多种预训练视觉-语言模型上也展示了跨域对齐能力。

Conclusion: 该文提出了同时增强节点辨识度与强制潜在空间几何一致性的无监督图对齐框架，实验证明在图与视觉-语言表示对齐任务上均优于现有方法。

Abstract: Graph alignment-the problem of identifying corresponding nodes across
multiple graphs-is fundamental to numerous applications. Most existing
unsupervised methods embed node features into latent representations to enable
cross-graph comparison without ground-truth correspondences. However, these
methods suffer from two critical limitations: the degradation of node
distinctiveness due to oversmoothing in GNN-based embeddings, and the
misalignment of latent spaces across graphs caused by structural noise, feature
heterogeneity, and training instability, ultimately leading to unreliable node
correspondences. We propose a novel graph alignment framework that
simultaneously enhances node distinctiveness and enforces geometric consistency
across latent spaces. Our approach introduces a dual-pass encoder that combines
low-pass and high-pass spectral filters to generate embeddings that are both
structure-aware and highly discriminative. To address latent space
misalignment, we incorporate a geometry-aware functional map module that learns
bijective and isometric transformations between graph embeddings, ensuring
consistent geometric relationships across different representations. Extensive
experiments on graph benchmarks demonstrate that our method consistently
outperforms existing unsupervised alignment baselines, exhibiting superior
robustness to structural inconsistencies and challenging alignment scenarios.
Additionally, comprehensive evaluation on vision-language benchmarks using
diverse pretrained models shows that our framework effectively generalizes
beyond graph domains, enabling unsupervised alignment of vision and language
representations.

</details>


### [48] [Explaining Concept Drift through the Evolution of Group Counterfactuals](https://arxiv.org/abs/2509.09616)
*Ignacy Stępka,Jerzy Stefanowski*

Main category: cs.LG

TL;DR: 通过跟踪群体反事实解释的时序变化并结合数据层与模型层信息，提出一种可解释诊断概念漂移的三层框架，能识别不同漂移根因并展示决策逻辑演变。


<details>
  <summary>Details</summary>
Motivation: 尽管漂移检测已有研究，但解释模型决策为何改变仍然困难；通过可解释的GCE代理可以更直观地展示决策边界和推理机制的变化。

Method: 跟踪GCE簇中心及其反事实动作向量在漂移前后的变化，并构建三层框架（数据层、模型层、解释层）来综合诊断漂移原因。

Result: 方法能区分不同漂移根因（如空间数据分布变化与标签重标注），并提供对决策边界结构变化的可解释视角。

Conclusion: 本文提出通过分析基于群体的反事实解释（GCE）随时间演化来解释概念漂移，从而揭示模型决策逻辑的结构性变化。

Abstract: Machine learning models in dynamic environments often suffer from concept
drift, where changes in the data distribution degrade performance. While
detecting this drift is a well-studied topic, explaining how and why the
model's decision-making logic changes still remains a significant challenge. In
this paper, we introduce a novel methodology to explain concept drift by
analyzing the temporal evolution of group-based counterfactual explanations
(GCEs). Our approach tracks shifts in the GCEs' cluster centroids and their
associated counterfactual action vectors before and after a drift. These
evolving GCEs act as an interpretable proxy, revealing structural changes in
the model's decision boundary and its underlying rationale. We operationalize
this analysis within a three-layer framework that synergistically combines
insights from the data layer (distributional shifts), the model layer
(prediction disagreement), and our proposed explanation layer. We show that
such holistic view allows for a more comprehensive diagnosis of drift, making
it possible to distinguish between different root causes, such as a spatial
data shift versus a re-labeling of concepts.

</details>


### [49] [Feasibility-Guided Fair Adaptive Offline Reinforcement Learning for Medicaid Care Management](https://arxiv.org/abs/2509.09655)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: FG-FARL 通过为各子群自适应设置安全阈值，在保持策略价值的同时提升安全与公平；在真实医疗离线数据上优于或不劣于BC与HACO基线。


<details>
  <summary>Details</summary>
Motivation: 在医疗决策支持中，单一全局安全阈值可能导致对某些受保护群体不公平或更大风险；需要一种能同时确保安全与公平的离线策略学习方法。

Method: 提出FG-FARL：基于离线数据估计每个子群的安全阈值并在策略学习中强制满足这些阈值，同时优化指定的公平目标（覆盖率或损害均衡）。与行为克隆(BC)和全局保守HACO基线比较，使用引导自助法(bootstrap)估计离线价值并报告95%置信区间及子群差异的p值。

Result: 在对秘匿化的医疗纵向轨迹（Medicaid群体管理程序）实验中，FG-FARL 在总体价值上与基线可比，同时在公平性指标（覆盖率或损害差异）上有改善，统计分析显示差异有显著性。

Conclusion: FG-FARL 在离线RL设置下，通过为不同受保护子群体自适应校准安全阈值，实现了在不显著牺牲策略价值的前提下改善公平性与安全性。

Abstract: We introduce Feasibility-Guided Fair Adaptive Reinforcement Learning
(FG-FARL), an offline RL procedure that calibrates per-group safety thresholds
to reduce harm while equalizing a chosen fairness target (coverage or harm)
across protected subgroups. Using de-identified longitudinal trajectories from
a Medicaid population health management program, we evaluate FG-FARL against
behavior cloning (BC) and HACO (Hybrid Adaptive Conformal Offline RL; a global
conformal safety baseline). We report off-policy value estimates with bootstrap
95% confidence intervals and subgroup disparity analyses with p-values. FG-FARL
achieves comparable value to baselines while improving fairness metrics,
demonstrating a practical path to safer and more equitable decision support.

</details>


### [50] [Conditioning on PDE Parameters to Generalise Deep Learning Emulation of Stochastic and Chaotic Dynamics](https://arxiv.org/abs/2509.09599)
*Ira J. S. Shokar,Rich R. Kerswell,Peter H. Haynes*

Main category: cs.LG

TL;DR: 通过先小域预训练再小样本多域微调、结合局部注意力与概率化输出，提出了一个能条件化PDE参数并对混沌/随机时空系统进行快速且具不确定性估计的深度学习仿真器。


<details>
  <summary>Details</summary>
Motivation: 传统数值积分在探索参数空间与统计稀有事件时计算代价高昂，且直接训练在大域上昂贵，因此需要一种能泛化参数并在不同域规模上高效训练的神经仿真器。

Method: 先在单一参数域上进行预训练，再在小而多样的微调数据集上微调；使用局部注意力机制以适应不同域大小与分辨率；并构建概率化变体以估计不确定性。

Result: 在Kuramoto-Sivashinsky方程与随机驱动的贝塔平面湍流上验证，模型在插值参数处能重现动力学特征，显著加速仿真；概率化模型能提供不确定性估计用于稀有事件统计。

Conclusion: 该论文提出了一个基于深度学习的仿真器，能在考虑PDE参数值的条件下对随机和混沌时空系统进行高效模拟，并在插值参数值时仍能保持良好拟合与不确定性估计。

Abstract: We present a deep learning emulator for stochastic and chaotic
spatio-temporal systems, explicitly conditioned on the parameter values of the
underlying partial differential equations (PDEs). Our approach involves
pre-training the model on a single parameter domain, followed by fine-tuning on
a smaller, yet diverse dataset, enabling generalisation across a broad range of
parameter values. By incorporating local attention mechanisms, the network is
capable of handling varying domain sizes and resolutions. This enables
computationally efficient pre-training on smaller domains while requiring only
a small additional dataset to learn how to generalise to larger domain sizes.
We demonstrate the model's capabilities on the chaotic Kuramoto-Sivashinsky
equation and stochastically-forced beta-plane turbulence, showcasing its
ability to capture phenomena at interpolated parameter values. The emulator
provides significant computational speed-ups over conventional numerical
integration, facilitating efficient exploration of parameter space, while a
probabilistic variant of the emulator provides uncertainty quantification,
allowing for the statistical study of rare events.

</details>


### [51] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: 提出可学习的正交蝴蝶旋转和均匀性正则化，实现层自适应离群抑制，快速训练，2-bit量化下对LLaMA-2-7B显著提升（PPL 15.4）。


<details>
  <summary>Details</summary>
Motivation: 传统极端2-bit量化受激活离群值严重影响，且现有旋转方法（如Hadamard）为固定变换无法适应不同层的离群模式。

Method: 用连续Givens旋转参数化的蝴蝶变换保证正交性且可微，复杂度为O(n log n)，参数量为n log n /2；同时加入对变换后激活的均匀性正则化；使用128个校准样本进行快速学习。

Result: 在LLaMA-2-7B上进行2-bit量化时，ButterflyQuant将困惑度从QuaRot的22.1降到15.4，训练只需几分钟单GPU。

Conclusion: 本文提出的ButterflyQuant通过可学习的蝴蝶正交变换替代固定的Hadamard旋转，从而实现按层适配的激活离群值抑制，并在2-bit量化下显著优于先前方法。

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


### [52] [ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance](https://arxiv.org/abs/2509.09611)
*Haolan Zheng,Yanlai Chen,Jiequn Han,Yue Yu*

Main category: cs.LG

TL;DR: ReBaNO通过基于贪婪算法的Reduced Basis构建与任务特定激活函数的知识蒸馏，得到一款在数据稀少时泛化更好、在线更紧凑且实现离散化不变性的算子学习算法。


<details>
  <summary>Details</summary>
Motivation: 在有限数据情形下提升算子学习的泛化能力、缩小训练与测试（尤其OOD）之间的差距，并实现对网格/离散形式的不敏感（离散化不变性），同时保持低在线计算成本。

Method: 离线阶段：基于数学上有保证的贪婪算法自底向上构建Reduced Basis样板和网络结构；知识蒸馏：通过针对任务的激活函数将基函数信息蒸馏进紧凑神经网络；嵌入物理：结合Physics-Informed约束以保持物理一致性；在线阶段：使用小型网络进行快速推断，确保计算成本最小。

Result: 数值实验表明，ReBaNO在内部分布与外部分布测试上均显著优于PCA-Net、DeepONet、FNO和CNO，能够消除或缩小泛化差距，并且是唯一达到严格离散化不变性的算子学习方法。

Conclusion: ReBaNO提出了一种结合Reduced Basis方法与生成式预训练物理信息神经网络思想的新型算子学习算法，通过贪婪算法离线自适应构建网络，并使用任务专用激活函数进行知识蒸馏，最终在数据稀少情形下实现紧凑的在线架构和嵌入物理约束，声称在推广误差、离散不变性等方面优于PCA-Net、DeepONet、FNO和CNO。

Abstract: We propose a novel data-lean operator learning algorithm, the Reduced Basis
Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct
inputs. Inspired by the Reduced Basis Method and the recently introduced
Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a
mathematically rigorous greedy algorithm to build its network structure offline
adaptively from the ground up. Knowledge distillation via task-specific
activation function allows ReBaNO to have a compact architecture requiring
minimal computational cost online while embedding physics. In comparison to
state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO,
and CNO, numerical results demonstrate that ReBaNO significantly outperforms
them in terms of eliminating/shrinking the generalization gap for both in- and
out-of-distribution tests and being the only operator learning algorithm
achieving strict discretization invariance.

</details>


### [53] [Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction](https://arxiv.org/abs/2509.09619)
*Roshan Balaji,Joe Bobby,Nirav Pravinbhai Bhatt*

Main category: cs.LG

TL;DR: FGR用化学功能基与挖掘到的模式基构建可解释的分子表示，结合预训练与2D描述符，既提升了分子性质预测性能又实现了化学可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习分子表示在性能提升的同时往往缺乏化学可解释性，妨碍化学家接受与信任。利用功能基这一化学基础概念构建表示，可以使模型输出与可理解的化学子结构对齐，从而提高可解释性并保持或提升预测性能。

Method: 构建包含两类功能基的词典：人工整理的化学功能基（FG）与基于序列模式挖掘从大量分子中自动发现的模式功能基（MFG）；以这些功能基作为原子级或片段级编码单元，设计预训练任务在大规模无标注分子集上学习低维潜在表示，并可选地融合2D结构描述符；在下游33个基准数据集上微调或直接使用表示进行回归/分类任务。

Result: 在33个涵盖物理化学、生物物理、量子力学、生物活性与药动学的基准任务上，FGR取得了最先进的性能，同时模型表示能够被追溯到具体功能基，帮助解释预测并发现新的结构-性质联系。

Conclusion: 提出的FGR框架通过将化学功能基（FG）与从大分子语料库挖掘的模式功能基（MFG）结合，并利用预训练与二维描述符，既提升了分子性质预测性能又增强了解释性，是将化学先验整合到深度学习分子表示中的有效方案。

Abstract: Molecular property prediction using deep learning (DL) models has accelerated
drug and materials discovery, but the resulting DL models often lack
interpretability, hindering their adoption by chemists. This work proposes
developing molecule representations using the concept of Functional Groups (FG)
in chemistry. We introduce the Functional Group Representation (FGR) framework,
a novel approach to encoding molecules based on their fundamental chemical
substructures. Our method integrates two types of functional groups: those
curated from established chemical knowledge (FG), and those mined from a large
molecular corpus using sequential pattern mining (MFG). The resulting FGR
framework encodes molecules into a lower-dimensional latent space by leveraging
pre-training on a large dataset of unlabeled molecules. Furthermore, the
proposed framework allows the inclusion of 2D structure-based descriptors of
molecules. We demonstrate that the FGR framework achieves state-of-the-art
performance on a diverse range of 33 benchmark datasets spanning physical
chemistry, biophysics, quantum mechanics, biological activity, and
pharmacokinetics while enabling chemical interpretability. Crucially, the
model's representations are intrinsically aligned with established chemical
principles, allowing chemists to directly link predicted properties to specific
functional groups and facilitating novel insights into structure-property
relationships. Our work presents a significant step toward developing
high-performing, chemically interpretable DL models for molecular discovery.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [An Interval Type-2 Version of Bayes Theorem Derived from Interval Probability Range Estimates Provided by Subject Matter Experts](https://arxiv.org/abs/2509.08834)
*John T. Rickard,William A. Dembski,James Rickards*

Main category: cs.AI

TL;DR: 本文把贝叶斯定理推广到区间二型模糊集，提出保守的一致性处理方法与将专家区间编码为IT2隶属函数的算法，从而能在输入为区间概率时进行可靠的贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，输入概率常为专家提供的区间估计而非精确值，传统贝叶斯方法难以直接处理这种不确定性，需扩展以兼容区间型不确定信息。

Method: 提出一种保守的IT2贝叶斯定理推导，设计避免输入IT2隶属函数不一致导致无效输出的策略；并给出一种灵活算法，将专家提供的概率区间编码为IT2模糊隶属函数，推广了先前用于“以词计算”中区间编码的方法。

Result: 得到一个保证输出有效的IT2贝叶斯推理方案，并提供了可操作的区间到IT2隶属函数的编码算法，理论上扩展并改进了先前文献在区间编码方面的工作。

Conclusion: 作者提出了将贝叶斯定理扩展到区间二型（IT2）模糊集的框架，并主张其在处理专家仅能提供区间估计时的适用性。

Abstract: Bayesian inference is widely used in many different fields to test hypotheses
against observations. In most such applications, an assumption is made of
precise input values to produce a precise output value. However, this is
unrealistic for real-world applications. Often the best available information
from subject matter experts (SMEs) in a given field is interval range estimates
of the input probabilities involved in Bayes Theorem. This paper provides two
key contributions to extend Bayes Theorem to an interval type-2 (IT2) version.
First, we develop an IT2 version of Bayes Theorem that uses a novel and
conservative method to avoid potential inconsistencies in the input IT2 MFs
that otherwise might produce invalid output results. We then describe a novel
and flexible algorithm for encoding SME-provided intervals into IT2 fuzzy
membership functions (MFs), which we can use to specify the input probabilities
in Bayes Theorem. Our algorithm generalizes and extends previous work on this
problem that primarily addressed the encoding of intervals into word MFs for
Computing with Words applications.

</details>


### [55] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 本文提出一个将GDD自动转换为Unity原型的端到端系统：解析文档、抽取规格、用微调LLaMA-3生成C#并集成到Unity，实验显示在多个指标上优于现有模型，证明了该方向的实用性。


<details>
  <summary>Details</summary>
Motivation: 缩短游戏设计文档到可运行原型之间的实现周期，降低开发门槛，并验证多模态/大模型在实际工程化任务中的可行性。

Method: 构建端到端流水线：解析GDD、抽取结构化规格、用微调的LLaMA-3生成兼容Unity的C#代码，并通过自定义Unity包实现集成与自动化构建。

Result: 微调模型在多项指标（编译成功率、GDD符合度、最佳实践、模块化）上显著优于基线，平均评分4.8/5，生成模板在多种游戏类型中表现出高度符合设计文档。

Conclusion: 该论文提出了一个从GDD到Unity可运行原型的自动化框架，证明了通过定制化微调的大型模型和Unity集成工具，可以显著提升代码生成的可用性与规范符合性。

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [56] [Global Constraint LLM Agents for Text-to-Model Translation](https://arxiv.org/abs/2509.08970)
*Junyang Cai,Serdar Kadioglu,Bistra Dilkina*

Main category: cs.AI

TL;DR: 将建模任务按全局约束类型分解给多个专门LLM，再由汇编器整合，可简化推理并在初步实验中优于传统提示方法。


<details>
  <summary>Details</summary>
Motivation: 自然语言描述到MiniZinc模型的转换既需逻辑推理又需约束编程专长，单一LLM难以同时掌握两者，故通过按约束类型分解任务让各智能体聚焦子问题，降低复杂性并提升准确性。

Method: 提出一个多智能体框架：每个智能体负责检测并生成某一类全局约束的MiniZinc代码，最后由汇编智能体整合各约束片段成完整模型；并与一次性提示和链式思维提示进行初步实验比较。

Result: 初步实验显示该多智能体分解方法在若干LLM上优于一次性提示和链式思维提示基线。论文还提出了未来工作路线图以改进该框架。

Conclusion: 该论文提出了将复杂的自然语言到MiniZinc建模问题，通过多个专门化LLM智能体按全局约束类型分解任务，从而降低单模型推理难度并提高建模准确性。

Abstract: Natural language descriptions of optimization or satisfaction problems are
challenging to translate into correct MiniZinc models, as this process demands
both logical reasoning and constraint programming expertise. We introduce a
framework that addresses this challenge with an agentic approach: multiple
specialized large language model (LLM) agents decompose the modeling task by
global constraint type. Each agent is dedicated to detecting and generating
code for a specific class of global constraint, while a final assembler agent
integrates these constraint snippets into a complete MiniZinc model. By
dividing the problem into smaller, well-defined sub-tasks, each LLM handles a
simpler reasoning challenge, potentially reducing overall complexity. We
conduct initial experiments with several LLMs and show better performance
against baselines such as one-shot prompting and chain-of-thought prompting.
Finally, we outline a comprehensive roadmap for future work, highlighting
potential enhancements and directions for improvement.

</details>


### [57] [ForTIFAI: Fending Off Recursive Training Induced Failure for AI Models](https://arxiv.org/abs/2509.08972)
*Soheil Zibakhsh Shabgahi,Pedram Aghazadeh,Azalia Mirhosseini,Farinaz Koushanfar*

Main category: cs.AI

TL;DR: 提出TCE损失，通过弱化高置信度预测来对抗模型崩溃，理论与实验显示可将崩溃延后2.3倍以上，并在多模态上泛化。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI产出的大量机器生成数据，训练数据日益被模型自身生成，重复用合成数据训练会引发模型性能随代际下降的‘模型崩溃’问题，现有缓解方法有限，需要寻找简单而有效的策略。

Method: 提出一种名为Truncated Cross Entropy (TCE)的置信感知损失函数，通过对高置信度预测进行下调权重，在训练中减少模型对自生成数据的过度拟合；同时给出模型无关的理论框架，将损失函数设计与崩溃缓解联系起来，并在多模态任务上进行实证验证。

Result: TCE在递归训练实验中将模型的忠实区间（fidelity interval）延长超过2.3倍，并在不同模态下展示了良好泛化性能，理论与实证结果均支持该方法能显著推迟模型崩溃。

Conclusion: 本文认为模型在自生成数据上过度自信是模型崩溃的关键驱动因素，通过设计抑制高置信度预测的损失函数可以有效延缓崩溃并提高模型在递归训练中的稳健性。

Abstract: The increasing reliance on generative AI models has accelerated the
generation rate of synthetic data, with some projections suggesting that most
available new data for training could be machine-generated by 2030. This shift
to a mainly synthetic content presents a critical challenge: repeated training
in synthetic data leads to a phenomenon known as model collapse, where model
performance degrades over generations of training, eventually rendering the
models ineffective. Although prior studies have explored the causes and
detection of model collapse, existing mitigation strategies remain limited.
  In this paper, we identify model overconfidence in their self-generated data
as a key driver of collapse. Building on this observation, we propose a
confidence-aware loss function that downweights high-confidence predictions
during training. We introduce a novel loss function we call Truncated Cross
Entropy (TCE). We demonstrate that TCE significantly delays model collapse in
recursive training.
  We provide a model-agnostic framework that links the loss function design to
model collapse mitigation and validate our approach both theoretically and
empirically, showing that it can extend the model's fidelity interval before
collapse by more than 2.3x. Finally, we show that our method generalizes across
modalities. These findings suggest that the design of loss functions provides a
simple yet powerful tool for preserving the quality of generative models in the
era of increasing synthetic data.

</details>


### [58] [Uncertainty Awareness and Trust in Explainable AI- On Trust Calibration using Local and Global Explanations](https://arxiv.org/abs/2509.08989)
*Carina Newen,Daniel Bodemer,Sonja Glantz,Emmanuel Müller,Magdalena Wischnewski,Lenka Schnaubert*

Main category: cs.AI

TL;DR: 研究针对不确定性与全局XAI，测试了一个兼顾不确定性、鲁棒性与全局解释的算法，结果显示该算法可帮助校准信任且直观可视化解释在提升用户满意度方面有潜力，但效果受实现与用户差异影响。


<details>
  <summary>Details</summary>
Motivation: 现有XAI研究多聚焦局部解释或不涉及不确定性，导致用户对模型信任难以准确校准；因此需要研究不确定性解释和全局解释如何影响人类信任与可解释性评估。

Method: 选择并实现了一种涵盖不确定性、鲁棒性和全局解释的算法，通过用户研究（可能含实验或问卷）评估算法对信任的校准能力，同时比较基于直观视觉理解的解释方法与传统方法在用户满意度和可解释性上的差异。

Result: 作者发现该算法在一定程度上能够校准用户信任；此外，尽管直观的可视化解释更复杂，用户在主观满意度和可理解性评分上可能表现更好，但具体效果依赖于用户背景和实现细节。

Conclusion: 该论文探讨了可解释性人工智能（XAI）中的不确定性解释和全局可解释性，评估了一种能同时覆盖不确定性、鲁棒性和全局XAI概念的算法在校准信任方面的表现，并比较了复杂但更直观的可视化方法是否能提高用户满意度和可解释性。

Abstract: Explainable AI has become a common term in the literature, scrutinized by
computer scientists and statisticians and highlighted by psychological or
philosophical researchers. One major effort many researchers tackle is
constructing general guidelines for XAI schemes, which we derived from our
study. While some areas of XAI are well studied, we focus on uncertainty
explanations and consider global explanations, which are often left out. We
chose an algorithm that covers various concepts simultaneously, such as
uncertainty, robustness, and global XAI, and tested its ability to calibrate
trust. We then checked whether an algorithm that aims to provide more of an
intuitive visual understanding, despite being complicated to understand, can
provide higher user satisfaction and human interpretability.

</details>


### [59] [Instructional Prompt Optimization for Few-Shot LLM-Based Recommendations on Cold-Start Users](https://arxiv.org/abs/2509.09066)
*Haowei Yang,Yushang Zhao,Sitao Min,Bo Su,Chao Yao,Wei Xu*

Main category: cs.AI

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: 

Method: 

Result: 

Conclusion: 

Abstract: The cold-start user issue further compromises the effectiveness of
recommender systems in limiting access to the historical behavioral
information. It is an effective pipeline to optimize instructional prompts on a
few-shot large language model (LLM) used in recommender tasks. We introduce a
context-conditioned prompt formulation method P(u,\ Ds)\ \rightarrow\
R\widehat, where u is a cold-start user profile, Ds is a curated support set,
and R\widehat is the predicted ranked list of items. Based on systematic
experimentation with transformer-based autoregressive LLMs (BioGPT, LLaMA-2,
GPT-4), we provide empirical evidence that optimal exemplar injection and
instruction structuring can significantly improve the precision@k and NDCG
scores of such models in low-data settings. The pipeline uses token-level
alignments and embedding space regularization with a greater semantic fidelity.
Our findings not only show that timely composition is not merely syntactic but
also functional as it is in direct control of attention scales and decoder
conduct through inference. This paper shows that prompt-based adaptation may be
considered one of the ways to address cold-start recommendation issues in
LLM-based pipelines.

</details>


### [60] [Understanding Economic Tradeoffs Between Human and AI Agents in Bargaining Games](https://arxiv.org/abs/2509.09071)
*Crystal Qian,Kehang Zhu,John Horton,Benjamin S. Manning,Vivian Tsai,James Wexler,Nithum Thain*

Main category: cs.AI

TL;DR: 比较人类、LLM与贝叶斯代理在动态谈判任务中，发现相同绩效下存在显著谈判过程差异：贝叶斯高剩余高拒绝，LLM保守低拒绝，人类更具战略性与公平考量。


<details>
  <summary>Details</summary>
Motivation: 随着协调任务由自主代理承担，除了评估最终绩效外，还需要理解不同代理在动态多智能体环境中的谈判过程和行为差异，尤其当统计代理与LLM在优势上存在互补时。

Method: 在统一的动态谈判环境中，分别让人类（N=216）、两种大型语言模型（GPT-4o, Gemini 1.5 Pro）和基于贝叶斯的统计代理执行相同任务，记录结果与行为轨迹以便横向比较。

Result: 贝叶斯代理通过激进优化获取最高剩余但伴随高拒绝率；LLM倾向保守与让步，拒绝少；人类表现出更多战略性、冒险与公平导向的行为。总体剩余上人类与LLM相近，但行为机制不同。

Conclusion: 性能相当掩盖了过程与对齐的根本差异，部署时需考虑行为动态和拒绝率等因素。

Abstract: Coordination tasks traditionally performed by humans are increasingly being
delegated to autonomous agents. As this pattern progresses, it becomes critical
to evaluate not only these agents' performance but also the processes through
which they negotiate in dynamic, multi-agent environments. Furthermore,
different agents exhibit distinct advantages: traditional statistical agents,
such as Bayesian models, may excel under well-specified conditions, whereas
large language models (LLMs) can generalize across contexts. In this work, we
compare humans (N = 216), LLMs (GPT-4o, Gemini 1.5 Pro), and Bayesian agents in
a dynamic negotiation setting that enables direct, identical-condition
comparisons across populations, capturing both outcomes and behavioral
dynamics. Bayesian agents extract the highest surplus through aggressive
optimization, at the cost of frequent trade rejections. Humans and LLMs can
achieve similar overall surplus, but through distinct behaviors: LLMs favor
conservative, concessionary trades with few rejections, while humans employ
more strategic, risk-taking, and fairness-oriented behaviors. Thus, we find
that performance parity -- a common benchmark in agent evaluation -- can
conceal fundamental differences in process and alignment, which are critical
for practical deployment in real-world coordination tasks.

</details>


### [61] [Anti-Money Laundering Machine Learning Pipelines; A Technical Analysis on Identifying High-risk Bank Clients with Supervised Learning](https://arxiv.org/abs/2509.09127)
*Khashayar Namdar,Pin-Chien Wang,Tushar Raju,Steven Zheng,Fiona Li,Safwat Tahmin Khan*

Main category: cs.AI

TL;DR: 本文构建了一个16步、基于SQLite的可解释ML流水线用于AML任务，达到了0.961的平均AUROC并获竞赛亚军。


<details>
  <summary>Details</summary>
Motivation: 反洗钱是金融机构重点任务，机器学习在AML任务中具有高潜力，因此构建稳健的ML流水线用于识别高风险客户。

Method: 采用16步设计与统计分析，包括将数据组织为SQLite数据库、使用SQL实现特征工程、预训练模型并连接数据库以实现推理、以及集成XAI模块解释特征重要性。

Result: 在包含195,789个客户ID的数据集上，流水线实现了平均AUROC 0.961（SD 0.005），并在IMI大数据与AI竞赛中获得第二名。

Conclusion: 本文提出了一种系统化的机器学习流水线，用于在银行客户数据中识别高风险客户，最终在竞赛中获得第二名，模型性能优秀。

Abstract: Anti-money laundering (AML) actions and measurements are among the priorities
of financial institutions, for which machine learning (ML) has shown to have a
high potential. In this paper, we propose a comprehensive and systematic
approach for developing ML pipelines to identify high-risk bank clients in a
dataset curated for Task 1 of the University of Toronto 2023-2024 Institute for
Management and Innovation (IMI) Big Data and Artificial Intelligence
Competition. The dataset included 195,789 customer IDs, and we employed a
16-step design and statistical analysis to ensure the final pipeline was
robust. We also framed the data in a SQLite database, developed SQL-based
feature engineering algorithms, connected our pre-trained model to the
database, and made it inference-ready, and provided explainable artificial
intelligence (XAI) modules to derive feature importance. Our pipeline achieved
a mean area under the receiver operating characteristic curve (AUROC) of 0.961
with a standard deviation (SD) of 0.005. The proposed pipeline achieved second
place in the competition.

</details>


### [62] [Mind Meets Space: Rethinking Agentic Spatial Intelligence from a Neuroscience-inspired Perspective](https://arxiv.org/abs/2509.09154)
*Bui Duc Manh,Soumyaratna Debnath,Zetong Zhang,Shriram Damodaran,Arvind Kumar,Yueyi Zhang,Lu Mi,Erik Cambria,Lin Wang*

Main category: cs.AI

TL;DR: 提出一个基于神经科学的六模块计算框架，用以提升代理AI的空间推理能力，分析现有工作与基准，指出关键缺口并给出未来研究路线。


<details>
  <summary>Details</summary>
Motivation: 当前代理AI在空间推理上受限于符号和序列处理，而人类的空间智能依赖多感官综合、空间记忆与认知地图，故需要以神经科学原理为指导提升代理的空间能力以更好地与物理三维世界交互。

Method: 通过回顾空间神经模型，提出包含六个计算模块的框架：仿生多模态感知、多感官融合、自我中心-外部参考系转换、人工认知地图、空间记忆和空间推理，并基于该框架对现有方法、基准与数据集进行逐模块分析以识别空白点。

Result: 框架为评估和发展神经科学驱动的空间推理模块提供了结构性视角，识别了关键技术缺口并指出了未来研究方向，同时讨论了相关基准、数据集与应用场景，如虚拟环境和机器人。

Conclusion: 本文提出以神经科学为基础的框架，旨在弥合现有自主代理AI在空间推理方面的不足，强调整合多感官感知与认知地图等生物学功能，以提升在三维物理环境中的表现。

Abstract: Recent advances in agentic AI have led to systems capable of autonomous task
execution and language-based reasoning, yet their spatial reasoning abilities
remain limited and underexplored, largely constrained to symbolic and
sequential processing. In contrast, human spatial intelligence, rooted in
integrated multisensory perception, spatial memory, and cognitive maps, enables
flexible, context-aware decision-making in unstructured environments.
Therefore, bridging this gap is critical for advancing Agentic Spatial
Intelligence toward better interaction with the physical 3D world. To this end,
we first start from scrutinizing the spatial neural models as studied in
computational neuroscience, and accordingly introduce a novel computational
framework grounded in neuroscience principles. This framework maps core
biological functions to six essential computation modules: bio-inspired
multimodal sensing, multi-sensory integration, egocentric-allocentric
conversion, an artificial cognitive map, spatial memory, and spatial reasoning.
Together, these modules form a perspective landscape for agentic spatial
reasoning capability across both virtual and physical environments. On top, we
conduct a framework-guided analysis of recent methods, evaluating their
relevance to each module and identifying critical gaps that hinder the
development of more neuroscience-grounded spatial reasoning modules. We further
examine emerging benchmarks and datasets and explore potential application
domains ranging from virtual to embodied systems, such as robotics. Finally, we
outline potential research directions, emphasizing the promising roadmap that
can generalize spatial reasoning across dynamic or unstructured environments.
We hope this work will benefit the research community with a
neuroscience-grounded perspective and a structured pathway. Our project page
can be found at Github.

</details>


### [63] [ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting](https://arxiv.org/abs/2509.09210)
*Xing Gao,Zherui Huang,Weiyao Lin,Xiao Sun*

Main category: cs.AI

TL;DR: 提出ProgD：用动态异构图进���场景随时间演化的交互，并通过因子化架构和多尺度渐进解码逐步细化预测，显著提升多体运动预测性能，在两个主流基准上取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有多体互动预测方法忽视了交互关系随时间演化的特性，导致对未来不确定性的建模不足。作者提出ProgD以显式建模交互演变并逐步消除不确定性，从而提高联合预测精度。

Method: 基于动态异构图表示未来场景，论文设计了因子化架构来处理未来场景中的时空依赖，并在图展开过程中进行渐进建模；同时引入多尺度解码以提高场景建模和预测一致性；在训练/推理中可能采用多阶段逐步细化的解码器，将高层粗糙预测逐步细化为低层精确轨迹。

Result: 在INTERACTION多体预测基准和Argoverse 2多世界预测基准上实现最先进性能，INTERACTION榜单排名第1，表明在多体交互场景下的预测准确性和一致性均有显著提升。

Conclusion: 该论文提出了用于多车互动运动预测的渐进式多尺度解码策略（ProgD），结合动态异构图进行场景建模，旨在显式抓取随时间演化的社会交互并逐步消除未来运动的不确定性。

Abstract: Accurate motion prediction of surrounding agents is crucial for the safe
planning of autonomous vehicles. Recent advancements have extended prediction
techniques from individual agents to joint predictions of multiple interacting
agents, with various strategies to address complex interactions within future
motions of agents. However, these methods overlook the evolving nature of these
interactions. To address this limitation, we propose a novel progressive
multi-scale decoding strategy, termed ProgD, with the help of dynamic
heterogeneous graph-based scenario modeling. In particular, to explicitly and
comprehensively capture the evolving social interactions in future scenarios,
given their inherent uncertainty, we design a progressive modeling of scenarios
with dynamic heterogeneous graphs. With the unfolding of such dynamic
heterogeneous graphs, a factorized architecture is designed to process the
spatio-temporal dependencies within future scenarios and progressively
eliminate uncertainty in future motions of multiple agents. Furthermore, a
multi-scale decoding procedure is incorporated to improve on the future
scenario modeling and consistent prediction of agents' future motion. The
proposed ProgD achieves state-of-the-art performance on the INTERACTION
multi-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2
multi-world forecasting benchmark.

</details>


### [64] [Enabling Regulatory Multi-Agent Collaboration: Architecture, Challenges, and Solutions](https://arxiv.org/abs/2509.09215)
*Qinnan Hu,Yuntao Wang,Yuan Gao,Zhou Su,Linkang Du*

Main category: cs.AI

TL;DR: 论文提出基于区块链的三层监管架构及三大功能模块，旨在为大规模LLM自治体协作提供可追溯、可信与可扩展的监管解决方案，但缺乏实证评估。


<details>
  <summary>Details</summary>
Motivation: 解决LLM自治体在协作中因行为不可预测、能力异质而带来的治理和问责难题，提升监管透明性与系统韧性。

Method: 构建三层架构（代理层、区块链数据层、监管应用层），并设计三个模块：行为追踪与仲裁模块、动态声誉评估模块、恶意行为预测模块。利用区块链作为不可篡改的数据底层来记录行为日志和仲裁结果，并结合动态信誉评分与预测模型实现早期预警。

Result: 提出的框架在设计上能实现自动化问责、基于声誉的信任评估与恶意行为的早期检测，理论上可提升多代理系统的可信度与可扩展性，但论文未给出详细实验证明或性能评估。

Conclusion: 该论文提出了一种基于区块链的分层架构，用于监管大规模LLM自治体之间的协作，从而增强可追溯性、问责性和信任机制。

Abstract: Large language models (LLMs)-empowered autonomous agents are transforming
both digital and physical environments by enabling adaptive, multi-agent
collaboration. While these agents offer significant opportunities across
domains such as finance, healthcare, and smart manufacturing, their
unpredictable behaviors and heterogeneous capabilities pose substantial
governance and accountability challenges. In this paper, we propose a
blockchain-enabled layered architecture for regulatory agent collaboration,
comprising an agent layer, a blockchain data layer, and a regulatory
application layer. Within this framework, we design three key modules: (i) an
agent behavior tracing and arbitration module for automated accountability,
(ii) a dynamic reputation evaluation module for trust assessment in
collaborative scenarios, and (iii) a malicious behavior forecasting module for
early detection of adversarial activities. Our approach establishes a
systematic foundation for trustworthy, resilient, and scalable regulatory
mechanisms in large-scale agent ecosystems. Finally, we discuss the future
research directions for blockchain-enabled regulatory frameworks in multi-agent
systems.

</details>


### [65] [Jupiter: Enhancing LLM Data Analysis Capabilities via Notebook and Inference-Time Value-Guided Search](https://arxiv.org/abs/2509.09245)
*Shuocheng Li,Yihao Liu,Silin Du,Wenxuan Zeng,Zhe Xu,Mengyu Zhou,Yeye He,Haoyu Dong,Shi Han,Dongmei Zhang*

Main category: cs.AI

TL;DR: 通过从真实Jupyter笔记本提取可执行多步数据分析任务构建NbQA数据集，并提出基于MCTS的Jupiter框架学习价值模型与路径搜索，显著提升模型在复杂工具型多步数据分析任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在多步推理和工具使用（如调用实际数据处理代码）方面表现有限，难以胜任复杂的数据分析任务；需要基于真实场景的高质量训练数据和更强的多步推理/工具使用方法。

Method: 构建可扩展管道从Jupyter notebooks和数据文件中抽取标准化的任务-解法对；发布NbQA数据集；提出Jupiter框架：将数据分析建模为搜索，使用蒙特卡洛树搜索（MCTS）生成多步工具使用轨迹进行价值模型训练；推理时结合价值模型和节点访问计数减少搜索步骤，生成可执行多步计划。

Result: 在NbQA上，Qwen2.5-7B和14B-Instruct通过Jupiter分别在InfiAgent-DABench上解决77.82%和86.38%任务，达到或超过GPT-4o和先进agent框架的表现；表现出更好的泛化能力和在多步工具使用推理任务上的优势。

Conclusion: 该论文提出了从真实Jupyter笔记本中提取可执行、多步的数据分析任务与解法，并基于此构建大规模数据集NbQA；提出Jupiter框架，将数据分析视为搜索问题，使用MCTS生成多样解法轨迹并学习价值模型，推理时结合价值模型与访问计数高效获取可执行计划。

Abstract: Large language models (LLMs) have shown great promise in automating data
science workflows, but existing models still struggle with multi-step reasoning
and tool use, which limits their effectiveness on complex data analysis tasks.
To address this, we propose a scalable pipeline that extracts high-quality,
tool-based data analysis tasks and their executable multi-step solutions from
real-world Jupyter notebooks and associated data files. Using this pipeline, we
introduce NbQA, a large-scale dataset of standardized task-solution pairs that
reflect authentic tool-use patterns in practical data science scenarios. To
further enhance multi-step reasoning, we present Jupiter, a framework that
formulates data analysis as a search problem and applies Monte Carlo Tree
Search (MCTS) to generate diverse solution trajectories for value model
learning. During inference, Jupiter combines the value model and node visit
counts to efficiently collect executable multi-step plans with minimal search
steps. Experimental results show that Qwen2.5-7B and 14B-Instruct models on
NbQA solve 77.82% and 86.38% of tasks on InfiAgent-DABench,
respectively-matching or surpassing GPT-4o and advanced agent frameworks.
Further evaluations demonstrate improved generalization and stronger tool-use
reasoning across diverse multi-step reasoning tasks.

</details>


### [66] [Fusing Knowledge and Language: A Comparative Study of Knowledge Graph-Based Question Answering with LLMs](https://arxiv.org/abs/2509.09272)
*Vaibhav Chaudhary,Neha Soni,Narotam Singh,Amita Kapoor*

Main category: cs.AI

TL;DR: 本文比较了spaCy、CoreNLP-OpenIE与GraphRAG三种构建知识图谱并与LLM结合用于问答的方法：OpenIE覆盖最好但噪声多，GraphRAG推理能力最强，spaCy更轻量，建议未来关注三元组质量净化与图-LLM融合优化。


<details>
  <summary>Details</summary>
Motivation: 传统RAG擅长基于局部事实的检索生成，但在处理长篇复杂文本的主题性与整体性理解时能力有限；知识图谱以结构化三元组补充文本语义，期望提高LLM在复杂问答场景的推理与连贯性。

Method: 比较实验基于spaCy的依存句法/实体关系抽取、Stanford CoreNLP-OpenIE的开放信息抽取以及GraphRAG构建的图检索增强生成流程；每种方法生成三元组后通过统一管线索引并供LLM检索与回答，评估指标包括三元组覆盖率、准确率、RAG问答准确性与推理题表现。

Result: 实验显示OpenIE生成的三元组数量与覆盖率最高但噪声亦大；spaCy在速度与部署复杂度最小；GraphRAG尽管三元组覆盖低于OpenIE，但凭借图结构检索与多跳推理机制在问答质量（特别是推理类问题）上领先。三方法在不同任务上各有优势，融合策略与三元组净化可进一步提升整体性能。

Conclusion: 三种方法各有优劣：OpenIE在三者中三元组覆盖最全面，GraphRAG在推理能力上表现最佳，spaCy在轻量部署与速度上有优势，但三者在语义完整性与噪声控制方面存在不足。结合知识图谱与LLM能显著提升主题性与整体性问题的回答效果，但仍需改进三元组抽取质量、上下文关联与推理融合策略。

Abstract: Knowledge graphs, a powerful tool for structuring information through
relational triplets, have recently become the new front-runner in enhancing
question-answering systems. While traditional Retrieval Augmented Generation
(RAG) approaches are proficient in fact-based and local context-based
extraction from concise texts, they encounter limitations when addressing the
thematic and holistic understanding of complex, extensive texts, requiring a
deeper analysis of both text and context. This paper presents a comprehensive
technical comparative study of three different methodologies for constructing
knowledge graph triplets and integrating them with Large Language Models (LLMs)
for question answering: spaCy, Stanford CoreNLP-OpenIE, and GraphRAG, all
leveraging open source technologies. We evaluate the effectiveness,
feasibility, and adaptability of these methods by analyzing their capabilities,
state of development, and their impact on the performance of LLM-based question
answering. Experimental results indicate that while OpenIE provides the most
comprehensive coverage of triplets, GraphRAG demonstrates superior reasoning
abilities among the three. We conclude with a discussion on the strengths and
limitations of each method and provide insights into future directions for
improving knowledge graph-based question answering.

</details>


### [67] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 将MCTS回合的部分展开作为前缀条件奖励引入GRPO，形成树形优势估计以改进偏好RL的策略优化，能提高稳定性与组合质量反映，但存在优势饱和和奖励坍缩，需要进一步方法改进。


<details>
  <summary>Details</summary>
Motivation: 受MCTS在LLM推理（尤其数学和符号推理）中生成高质量中间轨迹的成功启发，探索能否将这些轨迹转用于偏好强化学习中的策略优化，弥合用于训练价值/奖励模型的MCTS轨迹与直接用于策略更新的需求之间的差距，尤其针对GRPO这类不依赖价值网络的方法。

Method: 构建分阶段GRPO训练范式：在每个阶段使用部分揭示的MCTS回合生成不同长度的完成（前缀到完整轨迹），并在树形回合上定义基于前缀的优势估计器，将这些带有前缀条件的奖励信号用于策略更新；同时提出启发式和统计方法（如归一化、截断、方差校正）缓解信号退化。

Result: 初步实验和理论分析表明：树形前缀条件优势估计能在一定程度上稳定策略更新并更好反映组合推理质量，但也暴露出优势饱和（优势值集中）和奖励信号坍缩（信号强度下降）问题；提出的若干启发式和统计修正方法在一定程度上缓解了问题，但仍有改进空间。

Conclusion: 本文提出在偏好强化学习中将MCTS生成的轨迹用于策略优化，特别是针对无需价值网络的GRPO算法，通过分阶段训练和利用部分展开的MCTS回合作为完成，为优势估计引入树形结构的前缀条件奖励信号。实验表明该方法在稳定性和反映组合推理质量方面有潜力，但面临优势饱和和奖励坍缩问题。

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


### [68] [LightAgent: Production-level Open-source Agentic AI Framework](https://arxiv.org/abs/2509.09292)
*Weige Cai,Tong Zhu,Jinyi Niu,Ruiqi Hu,Lingyao Li,Tenglong Wang,Xiaowu Dai,Weining Shen,Liwen Zhang*

Main category: cs.AI

TL;DR: LightAgent：一个集成Memory、工具与ToT的轻量级开源多智能体框架，面向易用性与部署效率的折中方案。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统在通用性、鲁棒性和部署效率上存在权衡，许多框架要么功能臃肿难以扩展，要么过于简化无法满足复杂任务需求，LightAgent旨在解决这一矛盾。

Method: 在框架设计上整合了Memory(mem0)、Tools以及Tree of Thought(ToT)等核心能力，保持极简化的架构，并提供开源实现与主流聊天平台集成接口，便于开发者构建自学习代理。

Result: LightAgent作为开源项目已发布，声称在功能集成和轻量化之间取得平衡，支持自学习代理的快速构建与部署，但摘要未提供详细的实验评估数据或基准比较。

Conclusion: LightAgent提出了一种轻量但功能完备的多智能体框架，旨在在灵活性与简洁性之间取得平衡，适合快速部署和扩展。

Abstract: With the rapid advancement of large language models (LLMs), Multi-agent
Systems (MAS) have achieved significant progress in various application
scenarios. However, substantial challenges remain in designing versatile,
robust, and efficient platforms for agent deployment. To address these
limitations, we propose \textbf{LightAgent}, a lightweight yet powerful agentic
framework, effectively resolving the trade-off between flexibility and
simplicity found in existing frameworks. LightAgent integrates core
functionalities such as Memory (mem0), Tools, and Tree of Thought (ToT), while
maintaining an extremely lightweight structure. As a fully open-source
solution, it seamlessly integrates with mainstream chat platforms, enabling
developers to easily build self-learning agents. We have released LightAgent at
\href{https://github.com/wxai-space/LightAgent}{https://github.com/wxai-space/LightAgent}

</details>


### [69] [Explaining Tournament Solutions with Minimal Supports](https://arxiv.org/abs/2509.09312)
*Clément Contet,Umberto Grandi,Jérôme Mengin*

Main category: cs.AI

TL;DR: 定义并研究了锦标赛中“最小支撑”作为赢家的可证明确认解释，给出多数常见规则下的大小界与算法，其中weighted uncovered set情形为NP完全，其余情形可多项式求解。


<details>
  <summary>Details</summary>
Motivation: 在可解释人工智能中，理解并证明“为何某个候选人是赢家”是核心问题。锦标赛广泛用于表示两两优越性，因而需要为锦标赛解提供紧凑且可证实的解释，最小支撑即为这一需求的形式化实现。

Method: 作者将问题形式化为在子锦标赛中候选人成为必要赢家的最小子集问题，针对不同锦标赛解法（top cycle、uncovered set、Copeland、Borda、maximin、weighted uncovered set）进行了理论分析，证明最小支撑的最小规模上下界，并为除weighted uncovered set外的规则设计了多项式时间算法；同时对weighted uncovered set证明了NP完全性。

Result: 给出各规则下最小支撑的大小界：在某些规则下最小支撑可能很小并可在多项式时间内构造；在weighted uncovered set下求解最小支撑为NP完全问题。作者还展示了如何用最小支撑生成紧凑、直观且有证书的解释。

Conclusion: 本文提出并研究了在锦标赛模型下为赢家提供可证明解释的框架，定义并分析了“最小支撑”（minimal supports）这一概念，展示其对应于可证伪的解释（abductive explanation），并给出多种常见锦标赛规则下最小支撑的大小界和算法复杂性结论。

Abstract: Tournaments are widely used models to represent pairwise dominance between
candidates, alternatives, or teams. We study the problem of providing certified
explanations for why a candidate appears among the winners under various
tournament rules. To this end, we identify minimal supports, minimal
sub-tournaments in which the candidate is guaranteed to win regardless of how
the rest of the tournament is completed (that is, the candidate is a necessary
winner of the sub-tournament). This notion corresponds to an abductive
explanation for the question,"Why does the winner win the tournament", a
central concept in formal explainable AI. We focus on common tournament
solutions: the top cycle, the uncovered set, the Copeland rule, the Borda rule,
the maximin rule, and the weighted uncovered set. For each rule we determine
the size of the smallest minimal supports, and we present polynomial-time
algorithms to compute them for all but the weighted uncovered set, for which
the problem is NP-complete. Finally, we show how minimal supports can serve to
produce compact, certified, and intuitive explanations.

</details>


### [70] [Measuring Implicit Spatial Coordination in Teams: Effects on Collective Intelligence and Performance](https://arxiv.org/abs/2509.09314)
*Thuy Ngoc Nguyen,Anita Williams Woolley,Cleotilde Gonzalez*

Main category: cs.AI

TL;DR: 在受限沟通的角色化搜救任务中，成员在空间上的专业化有利绩效，而对同伴距离的适度动态调整最优，时间演化指标可预示团队成败。


<details>
  <summary>Details</summary>
Motivation: 研究在缺乏视觉线索与显式沟通的情境下，团队如何通过空间移动模式实现隐式协调，补充以往重共处/知识型工作的研究空白，服务高风险实地团队的训练与辅助系统设计。

Method: 构建在线搜救仿真任务，34个四人队（136人），限制显式通信，分配专门角色；提取空间靠近性、分布模式、移动对齐等时序度量，使用回归分析与时间序列比较高低绩效队伍。

Result: 空间专业化显著正向预测绩效；自适应空间接近性呈边际倒U（适中水平最优）；时间动态特征可区分高低绩效队伍。

Conclusion: 空间专业化提高团队表现；自适应空间接近性呈现边际倒U关系，适度最佳；探索多样性影响复杂，需与角色分配协同。

Abstract: Coordinated teamwork is essential in fast-paced decision-making environments
that require dynamic adaptation, often without an opportunity for explicit
communication. Although implicit coordination has been extensively considered
in the existing literature, the majority of work has focused on co-located,
synchronous teamwork (such as sports teams) or, in distributed teams, primarily
on coordination of knowledge work. However, many teams (firefighters, military,
law enforcement, emergency response) must coordinate their movements in
physical space without the benefit of visual cues or extensive explicit
communication. This paper investigates how three dimensions of spatial
coordination, namely exploration diversity, movement specialization, and
adaptive spatial proximity, influence team performance in a collaborative
online search and rescue task where explicit communication is restricted and
team members rely on movement patterns to infer others' intentions and
coordinate actions. Our metrics capture the relational aspects of teamwork by
measuring spatial proximity, distribution patterns, and alignment of movements
within shared environments. We analyze data from 34 four-person teams (136
participants) assigned to specialized roles in a search and rescue task.
Results show that spatial specialization positively predicts performance, while
adaptive spatial proximity exhibits a marginal inverted U-shaped relationship,
suggesting moderate levels of adaptation are optimal. Furthermore, the temporal
dynamics of these metrics differentiate high- from low-performing teams over
time. These findings provide insights into implicit spatial coordination in
role-based teamwork and highlight the importance of balanced adaptive
strategies, with implications for training and AI-assisted team support
systems.

</details>


### [71] [Towards Adaptive ML Benchmarks: Web-Agent-Driven Construction, Domain Expansion, and Metric Optimization](https://arxiv.org/abs/2509.09321)
*Hangyi Jia,Yuxi Qian,Hanwen Tong,Xinhui Wu,Lin Chen,Feng Wei*

Main category: cs.AI

TL;DR: TAM Bench 是一个通过自动采集和排行榜驱动难度建模，面向 LLM 自动化 ML 代理的多模态、多维度、可扩展基准，包含 Lite/Medium/Full 三个任务集以适应不同评测需求。


<details>
  <summary>Details</summary>
Motivation: 现有基准在任务覆盖、领域多样性、难度建模和评估严谨性上存在不足，难以全面衡量 LLM-based 端到端 ML 代理在现实场景下的能力。

Method: 采用浏览器自动化与 LLM 协同抓取来自 Kaggle、AIcrowd、Biendata 等平台的挑战，并用排行榜参与者数与分数离散度估计任务难度；设计多维评估指标（性能、格式合规、约束遵守、泛化能力）；构建 Lite/Medium/Full 三个任务集合。

Result: 基于 150 个精选 AutoML 任务，构建了三个规模的基准子集，并提供了 Lite（18 任务）用于日常测试，覆盖多模态与多难度级别；提出的难度估计与多维评估被用于更客观、可扩展的标定与比较。

Conclusion: TAM Bench 提供了一个覆盖更广、接近实际的端到端 ML 任务基准，通过自动采集、多维评估和基于排行榜的难度建模，可以更客观地评估 LLM 驱动的自动化 ML 代理的能力。

Abstract: Recent advances in large language models (LLMs) have enabled the emergence of
general-purpose agents for automating end-to-end machine learning (ML)
workflows, including data analysis, feature engineering, model training, and
competition solving. However, existing benchmarks remain limited in task
coverage, domain diversity, difficulty modeling, and evaluation rigor, failing
to capture the full capabilities of such agents in realistic settings. We
present TAM Bench, a diverse, realistic, and structured benchmark for
evaluating LLM-based agents on end-to-end ML tasks. TAM Bench features three
key innovations: (1) A browser automation and LLM-based task acquisition system
that automatically collects and structures ML challenges from platforms such as
Kaggle, AIcrowd, and Biendata, spanning multiple task types and data modalities
(e.g., tabular, text, image, graph, audio); (2) A leaderboard-driven difficulty
modeling mechanism that estimates task complexity using participant counts and
score dispersion, enabling scalable and objective task calibration; (3) A
multi-dimensional evaluation framework incorporating performance, format
compliance, constraint adherence, and task generalization. Based on 150 curated
AutoML tasks, we construct three benchmark subsets of different sizes -- Lite,
Medium, and Full -- designed for varying evaluation scenarios. The Lite
version, with 18 tasks and balanced coverage across modalities and difficulty
levels, serves as a practical testbed for daily benchmarking and comparative
studies.

</details>


### [72] [Curriculum-Based Multi-Tier Semantic Exploration via Deep Reinforcement Learning](https://arxiv.org/abs/2509.09356)
*Abdel Hakim Drid,Vincenzo Suriani,Daniele Nardi,Abderrezzak Debilou*

Main category: cs.AI

TL;DR: 通过把VLM查询当作动作并用分层奖励与课程学习训练智能体，达成资源高效的语义探索，提升目标发现率并学会何时调用外部常识。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习策略缺乏高阶认知和常识推理能力，导致在语义探索中效率低、且常依赖人工控制；因此需要将常识性语义理解嵌入到资源受限的智能体决策过程中。

Method: 将VLM查询建模为智能体的一种动作，结合分层奖励（鼓励发现语义信息和惩罚频繁查询）与课程学习（从易到难逐步训练），在DRL训练中优化策略以实现高效探索与语义导航。

Result: 实验显示，所提方法在目标发现率与语义区域导航效率上有显著提升，同时学会了节约性地调用VLM查询，验证了方法的可扩展性与实用性。

Conclusion: 本文提出了一种将视觉-语言模型（VLM）作为“查询动作”并通过分层奖励与课程学习相结合的深度强化学习框架，用于资源高效的语义探索，结论是该方法可显著提升目标发现率并学会在何时调用外部常识查询以节省资源。

Abstract: Navigating and understanding complex and unknown environments autonomously
demands more than just basic perception and movement from embodied agents.
Truly effective exploration requires agents to possess higher-level cognitive
abilities, the ability to reason about their surroundings, and make more
informed decisions regarding exploration strategies. However, traditional RL
approaches struggle to balance efficient exploration and semantic understanding
due to limited cognitive capabilities embedded in the small policies for the
agents, leading often to human drivers when dealing with semantic exploration.
In this paper, we address this challenge by presenting a novel Deep
Reinforcement Learning (DRL) architecture that is specifically designed for
resource efficient semantic exploration. A key methodological contribution is
the integration of a Vision-Language Model (VLM) common-sense through a layered
reward function. The VLM query is modeled as a dedicated action, allowing the
agent to strategically query the VLM only when deemed necessary for gaining
external guidance, thereby conserving resources. This mechanism is combined
with a curriculum learning strategy designed to guide learning at different
levels of complexity to ensure robust and stable learning. Our experimental
evaluation results convincingly demonstrate that our agent achieves
significantly enhanced object discovery rates and develops a learned capability
to effectively navigate towards semantically rich regions. Furthermore, it also
shows a strategic mastery of when to prompt for external environmental
information. By demonstrating a practical and scalable method for embedding
common-sense semantic reasoning with autonomous agents, this research provides
a novel approach to pursuing a fully intelligent and self-guided exploration in
robotics.

</details>


### [73] [TORSO: Template-Oriented Reasoning Towards General Tasks](https://arxiv.org/abs/2509.09448)
*Minhyuk Kim,Seungyoon Lee,Heuiseok Lim*

Main category: cs.AI

TL;DR: TORSO 用通用模板替代 few-shot 示例，激发 LLMs 的内部推理能力，实现跨任务强性能，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 现有基于 few-shot 的推理提示方法高度依赖示例，成本高且在不同任务间不一致，限制了对模型内在推理能力的充分利用。TORSO 的动机是减少对手工示例的依赖，提升跨任务的一致性与可迁移性。

Method: TORSO 通过设计通用模板（而非任务特定的 few-shot 示例）来提示模型，促使模型内部调动推理链路并产出步骤化的推理与答案。具体实现可能包括模板格式的提示、启发模型产生中间论证（rationales），从而利用模型本身的推理能力。

Result: 实验结果表明，TORSO 在多个 LLM 基准上取得了强劲表现，并生成合理的推理（rationales），证明无需手工 few-shot 示例也能有效激发模型的推理能力。

Conclusion: TORSO 提出了一种基于模板的推理引导方法，旨在在无须手工设计 few-shot 示例的情况下，激发 LLMs 的内部推理能力以生成跨任务的合理答案。实验显示 TORSO 在多个基准上具有较强性能并能生成合理的推理过程。

Abstract: The approaches that guide Large Language Models (LLMs) to emulate human
reasoning during response generation have emerged as an effective method for
enabling them to solve complex problems in a step-by-step manner, thereby
achieving superior performance. However, most existing approaches using
few-shot prompts to generate responses heavily depend on the provided examples,
limiting the utilization of the model's inherent reasoning capabilities.
Moreover, constructing task-specific few-shot prompts is often costly and may
lead to inconsistencies across different tasks. In this work, we introduce
Template-Oriented Reasoning (TORSO), which elicits the model to utilize
internal reasoning abilities to generate proper responses across various tasks
without the need for manually crafted few-shot examples. Our experimental
results demonstrate that TORSO achieves strong performance on diverse LLMs
benchmarks with reasonable rationales.

</details>


### [74] [Inteligencia Artificial jurídica y el desafío de la veracidad: análisis de alucinaciones, optimización de RAG y principios para una integración responsable](https://arxiv.org/abs/2509.09467)
*Alex Dantart*

Main category: cs.AI

TL;DR: 法律领域LLM的幻觉问题不可忽视，RAG有用但不足；推荐转向“咨询式”AI，强调可追溯证据、可解释生成与必需的人类监督与监管保障。


<details>
  <summary>Details</summary>
Motivation: 法律场景对可证性与可靠性要求极高，而LLM的幻觉风险在法律应用中带来严重后果，亟需系统性评估现有缓解策略（如RAG）并提出可行替代范式以保障专业判断与公众利益。

Method: 通过问题分析与文献/系统性评估，审视幻觉成因（模型固有不确定性、数据偏差、检索器与索引缺陷、提示与温度设置、评估集偏差）、RAG（检索增强生成）策略的流程与极限，并提出从数据层、检索层、生成层、用户界面和监管层的整体性优化措施；结合伦理、监管分析和人机协作设计原则。

Result: 确认RAG在降低幻觉方面有显著帮助但存在检索噪声、错配来源、证据溯源不充分、实时性与覆盖性问题；提出优化措施包括严格引用规范、证据评分与置信度校准、多检索器融合、增强索引与法律本体、端到端可验证流水线、人类在环验证与责任界定；并倡议政策层面要求透明度、可审计日志与最低可接受精确度标准。

Conclusion: 解决法律领域LLM“幻觉”问题不能仅依赖生成模型的逐步改进，而应采用以证据可追溯性与真实性为核心的“咨询式”AI范式，使AI作为放大专业判断的工具而非替代。

Abstract: This technical report analyzes the challenge of "hallucinations" (false
information) in LLMs applied to law. It examines their causes, manifestations,
and the effectiveness of the RAG mitigation strategy, highlighting its
limitations and proposing holistic optimizations. The paper explores the
ethical and regulatory implications, emphasizing human oversight as an
irreplaceable role. It concludes that the solution lies not in incrementally
improving generative models, but in adopting a "consultative" AI paradigm that
prioritizes veracity and traceability, acting as a tool to amplify, not
replace, professional judgment.
  --
  Este informe t\'ecnico analiza el desaf\'io de las "alucinaciones"
(informaci\'on falsa) en los LLMs aplicados al derecho. Se examinan sus causas,
manifestaciones y la efectividad de la estrategia de mitigaci\'on RAG,
exponiendo sus limitaciones y proponiendo optimizaciones hol\'isticas. Se
exploran las implicaciones \'eticas y regulatorias, enfatizando la
supervisi\'on humana como un rol insustituible. El documento concluye que la
soluci\'on no reside en mejorar incrementalmente los modelos generativos, sino
en adoptar un paradigma de IA "consultiva" que priorice la veracidad y la
trazabilidad, actuando como una herramienta para amplificar, y no sustituir, el
juicio profesional.

</details>


### [75] [SEDM: Scalable Self-Evolving Distributed Memory for Agents](https://arxiv.org/abs/2509.09498)
*Haoran Xu,Jiacong Hu,Ke Zhang,Lei Yu,Yuxin Tang,Xinyuan Song,Yiqun Duan,Lynn Ai,Bill Shi*

Main category: cs.AI

TL;DR: SEDM是一种可验证、自我进化的分布式记忆框架，通过写入验证、动态调度与跨域知识扩散，解决长期多智能体记忆噪声与膨胀问题，提升推理性能并降低存储成本。


<details>
  <summary>Details</summary>
Motivation: 长期多智能体系统产生大量轨迹与交互历史，现有矢量检索与层级存储方法存在噪声累积、内存不受控膨胀与跨域泛化差等问题，需一种可验证、可自适应且能跨域迁移的记忆机制。

Method: 提出可验证写入（基于可重现回放）、自调度记忆控制器（动态排序与合并条目）、跨域知识扩散（抽象可复用见解以支持异构任务迁移）；在基准数据集上与强基线比较，评估推理准确性与token开销，并演示从事实验证蒸馏知识用于多跳推理。

Result: 在基准数据集上，SEDM在提高推理准确率的同时减少token开销，且通过事实验证蒸馏能增强多跳推理能力，展示出可扩展且可持续的长期记忆管理效果。

Conclusion: SEDM将记忆从被动存储转为主动、自我优化的组件，通过可验证的写入准入、自调度的记忆控制器和跨域知识扩散，有效提升长期多智能体系统的推理准确率并降低存储开销。

Abstract: Long-term multi-agent systems inevitably generate vast amounts of
trajectories and historical interactions, which makes efficient memory
management essential for both performance and scalability. Existing methods
typically depend on vector retrieval and hierarchical storage, yet they are
prone to noise accumulation, uncontrolled memory expansion, and limited
generalization across domains. To address these challenges, we present SEDM,
Self-Evolving Distributed Memory, a verifiable and adaptive framework that
transforms memory from a passive repository into an active, self-optimizing
component. SEDM integrates verifiable write admission based on reproducible
replay, a self-scheduling memory controller that dynamically ranks and
consolidates entries according to empirical utility, and cross-domain knowledge
diffusion that abstracts reusable insights to support transfer across
heterogeneous tasks. Evaluations on benchmark datasets demonstrate that SEDM
improves reasoning accuracy while reducing token overhead compared with strong
memory baselines, and further enables knowledge distilled from fact
verification to enhance multi-hop reasoning. The results highlight SEDM as a
scalable and sustainable memory mechanism for open-ended multi-agent
collaboration. The code will be released in the later stage of this project.

</details>


### [76] [Compositional Concept Generalization with Variational Quantum Circuits](https://arxiv.org/abs/2509.09541)
*Hala Hawashin,Mina Abbaszadeh,Nicholas Joseph,Beth Pearson,Martha Lewis,Mehrnoosh sadrzadeh*

Main category: cs.AI

TL;DR: 本文将张量组合语义嵌入到量子态并用VQC学习，MHE编码下效果显著，CLIP编码下表现混合但优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型缺乏组合泛化，先前基于张量的句子语义方法效果不佳，作者猜测量子模型更高的训练效率可能改善这种泛化能力。

Method: 将组合张量语义表示映射到希尔伯特空间，训练变分量子电路（VQC）在图像-字幕任务上学习这些表示；使用两种图像编码：多热编码（MHE）和基于CLIP向量的角/幅度编码。

Result: 使用噪声多热编码取得了良好的概念验证结果；对CLIP向量的角/幅度编码结果表现参差，但总体仍优于经典的组合模型。

Conclusion: 量子模型在组合泛化的图像描述任务上能提供一定优势，但结果依编码方式而异。

Abstract: Compositional generalization is a key facet of human cognition, but lacking
in current AI tools such as vision-language models. Previous work examined
whether a compositional tensor-based sentence semantics can overcome the
challenge, but led to negative results. We conjecture that the increased
training efficiency of quantum models will improve performance in these tasks.
We interpret the representations of compositional tensor-based models in
Hilbert spaces and train Variational Quantum Circuits to learn these
representations on an image captioning task requiring compositional
generalization. We used two image encoding techniques: a multi-hot encoding
(MHE) on binary image vectors and an angle/amplitude encoding on image vectors
taken from the vision-language model CLIP. We achieve good proof-of-concept
results using noisy MHE encodings. Performance on CLIP image vectors was more
mixed, but still outperformed classical compositional models.

</details>


### [77] [Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution](https://arxiv.org/abs/2509.09560)
*Shulai Zhang,Ao Xu,Quan Chen,Han Zhao,Weihao Cui,Ningxin Zheng,Haibin Lin,Xin Liu,Minyi Guo*

Main category: cs.AI

TL;DR: Auras通过解耦感知与生成并引入公共上下文的受控流水线并行，实现了在不牺牲准确率的前提下显著提高具身AI推理吞吐（2.54x，准确率102.7%）。


<details>
  <summary>Details</summary>
Motivation: 具身AI在动态环境中需要同时处理高频输入输出，传统串行计算虽然准确但无法满足实时“思考”频率，故需新的并行化框架来提高推理频率而不牺牲准确性。

Method: 提出算法-系统协同的推理框架Auras：将感知和生成模块分离并进行受控的流水线并行；设计公共上下文以在模块间共享最新信息以缓解数据陈旧；在系统层面优化调度以实现高且稳定的吞吐。

Result: 在实验中，Auras平均将吞吐提升2.54倍，同时保持102.7%的原始准确率，证明了在增加并行度时可通过共享上下文维持或提升性能。

Conclusion: Auras通过感知与生成模块的解耦与受控流水线并行，显著提升了具身AI系统的推理频率，同时通过共享公共上下文缓解并行带来的数据陈旧问题，从而在提高吞吐的同时保持甚至略优于原有准确率。

Abstract: Embodied AI systems operate in dynamic environments, requiring seamless
integration of perception and generation modules to process high-frequency
input and output demands. Traditional sequential computation patterns, while
effective in ensuring accuracy, face significant limitations in achieving the
necessary "thinking" frequency for real-world applications. In this work, we
present Auras, an algorithm-system co-designed inference framework to optimize
the inference frequency of embodied AI agents. Auras disaggregates the
perception and generation and provides controlled pipeline parallelism for them
to achieve high and stable throughput. Faced with the data staleness problem
that appears when the parallelism is increased, Auras establishes a public
context for perception and generation to share, thereby promising the accuracy
of embodied agents. Experimental results show that Auras improves throughput by
2.54x on average while achieving 102.7% of the original accuracy, demonstrating
its efficacy in overcoming the constraints of sequential computation and
providing high throughput.

</details>


### [78] [The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs](https://arxiv.org/abs/2509.09677)
*Akshit Sinha,Arvindh Arun,Shashwat Goel,Steffen Staab,Jonas Geiping*

Main category: cs.AI

TL;DR: 单步微小改进在多步任务中会指数放大；问题主要是执行错误的累积与自我条件化；增大模型与使用思维型模型或序列化测试时计算能显著提升长时任务执行能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM继续扩展是否边际收益递减，并解释为何模型能解决复杂推理但在延长简单任务时失败，强调执行错误的复合效应可能导致任务长度呈指数下降。

Method: 通过隔离执行能力：在输入中显式提供知识与计划，然后测试不同规模模型在多步执行任务中的每步准确率和总体任务完成长度；比较常规模型与思维型模型，观察在包含前一步错误的上下文中模型出错概率的变化。

Result: 发现较大模型在执行多步任务时能正确执行更多步骤，即便小型模型在单步上达100%准确率；每步准确率随步骤数增加而下降，且存在自我条件化效应：含错误上下文会增加后续错误；思维型模型不表现出自我条件化并能单回合执行更长任务。

Conclusion: 作者结论是：执行能力（execution）而非推理能力是长任务失败的主要原因，随着模型规模和序列测试时计算力（sequential test-time compute）增加，能显著提高长时任务可执行长度；思维型模型（thinking models）能避免自我条件化（self-conditioning）并执行更长任务。

Abstract: Does continued scaling of large language models (LLMs) yield diminishing
returns? Real-world value often stems from the length of task an agent can
complete. We start this work by observing the simple but counterintuitive fact
that marginal gains in single-step accuracy can compound into exponential
improvements in the length of a task a model can successfully complete. Then,
we argue that failures of LLMs when simple tasks are made longer arise from
mistakes in execution, rather than an inability to reason. We propose isolating
execution capability, by explicitly providing the knowledge and plan needed to
solve a long-horizon task. We find that larger models can correctly execute
significantly more turns even when small models have 100\% single-turn
accuracy. We observe that the per-step accuracy of models degrades as the
number of steps increases. This is not just due to long-context limitations --
curiously, we observe a self-conditioning effect -- models become more likely
to make mistakes when the context contains their errors from prior turns.
Self-conditioning does not reduce by just scaling the model size. In contrast,
recent thinking models do not self-condition, and can also execute much longer
tasks in a single turn. We conclude by benchmarking frontier thinking models on
the length of task they can execute in a single turn. Overall, by focusing on
the ability to execute, we hope to reconcile debates on how LLMs can solve
complex reasoning problems yet fail at simple tasks when made longer, and
highlight the massive benefits of scaling model size and sequential test-time
compute for long-horizon tasks.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [79] [A Comparative Analysis of Identifier Schemes: UUIDv4, UUIDv7, and ULID for Distributed Systems](https://arxiv.org/abs/2509.08969)
*Nima Karimian Kakolaki*

Main category: cs.DC

TL;DR: 本文通过理论与实测比较UUIDv4、UUIDv7和ULID，得出ULID在碰撞风险、生成速率与网络开销上均优于其他两者，推荐用于可扩展分布式系统。


<details>
  <summary>Details</summary>
Motivation: 分布式系统需可靠且可扩展的唯一标识符方案，权衡碰撞风险、生成性能和网络开销以支持多节点高吞吐场景。

Method: 结合数学碰撞概率计算与模拟分布式环境下的实证实验，测量生成速度和网络传输开销；并进行统计分析比较不同标识符的碰撞风险。

Result: 实验证明ULID比UUIDv4和UUIDv7在网络开销上降低83.7%，生成速度提高97.32%；统计分析显示ULID的碰撞风险比UUIDv7低98.42%，在高生成率下碰撞概率仍可忽略。

Conclusion: ULIDs在高性能分布式系统中表现最佳，提供了高效、按时间排序且支持词典序的标识符，碰撞风险和网络开销显著低于UUIDv4和UUIDv7。

Abstract: Distributed systems require robust, scalable identifier schemes to ensure
data uniqueness and efficient indexing across multiple nodes. This paper
presents a comprehensive analysis of the evolution of distributed identifiers,
comparing traditional auto-increment keys with UUIDv4, UUIDv7, and ULIDs. We
combine mathematical calculation of collision probabilities with empirical
experiments measuring generation speed and network transmission overhead in a
simulated distributed environment. Results demonstrate that ULIDs significantly
outperform UUIDv4 and UUIDv7, reducing network overhead by 83.7% and increasing
generation speed by 97.32%. statistical analysis further shows ULIDs offer a
98.42% lower collision risk compared to UUIDv7, while maintaining negligible
collision probabilities even at high generation rates. These findings highlight
ULIDs as an optimal choice for high-performance distributed systems, providing
efficient, time-ordered, and lexicographically sortable identifiers suitable
for scalable applications. All source code, datasets, and analysis scripts
utilized in this research are publicly available in our dedicated repository at
https://github.com/nimakarimiank/uids-comparison. This repository contains
comprehensive documentation of the experimental setup, including configuration
files for the distributed environment, producer and consumer implementations,
and message broker integration. Additionally, it provides the data scripts and
datasets. Researchers and practitioners are encouraged to explore the
repository for full reproducibility of the experiments and to facilitate
further investigation or extension of the presented work.

</details>


### [80] [Optimizing the Variant Calling Pipeline Execution on Human Genomes Using GPU-Enabled Machines](https://arxiv.org/abs/2509.09058)
*Ajay Kumar,Praveen Rao,Peter Sanders*

Main category: cs.DC

TL;DR: 用机器学习预测变异检测各阶段耗时，再把调度建模为柔性作业车间问题生成执行计划，在多GPU云环境下可显著加速基因组变异检测工作负载。


<details>
  <summary>Details</summary>
Motivation: 变异检测计算量大、资源需求高，云端GPU机器按需付费模式虽灵活但需高效调度以降低总完成时间与成本；因此希望通过预测与计划生成来提升并行资源的利用与作业调度效率。

Method: 方法包括两部分：1) 使用机器学习模型（基于基因组序列特征如序列大小、测序质量、重复读百分比、平均读长等）预测变异检测管线中各阶段的执行时间；2) 将调度问题建模为类似柔性作业车间调度问题，利用预测时间生成跨多台GPU机器的最优执行计划，并通过精细同步机制执行这些计划。

Result: 在公开基因组数据集与不同GPU硬件的测试平台上，ML模型能有效预测管线阶段耗时；相较于基于ML预测但采用贪心分配的方案，本方法平均实现约2倍加速；相比于不使用时间预测、仅按资源可用性动态调度的方法，本方法平均实现约1.6倍加速。

Conclusion: 本论文提出了一种基于机器学习的调度方法，用于在GPU加速云环境下高效执行多人类基因组的变异检测流程，通过预测各阶段执行时间并据此生成近似最优的执行计划，能显著缩短总执行时间。

Abstract: Variant calling is the first step in analyzing a human genome and aims to
detect variants in an individual's genome compared to a reference genome. Due
to the computationally-intensive nature of variant calling, genomic data are
increasingly processed in cloud environments as large amounts of compute and
storage resources can be acquired with the pay-as-you-go pricing model. In this
paper, we address the problem of efficiently executing a variant calling
pipeline for a workload of human genomes on graphics processing unit
(GPU)-enabled machines. We propose a novel machine learning (ML)-based approach
for optimizing the workload execution to minimize the total execution time. Our
approach encompasses two key techniques: The first technique employs ML to
predict the execution times of different stages in a variant calling pipeline
based on the characteristics of a genome sequence. Using the predicted times,
the second technique generates optimal execution plans for the machines by
drawing inspiration from the flexible job shop scheduling problem. The plans
are executed via careful synchronization across different machines. We
evaluated our approach on a workload of publicly available genome sequences
using a testbed with different types of GPU hardware. We observed that our
approach was effective in predicting the execution times of variant calling
pipeline stages using ML on features such as sequence size, read quality,
percentage of duplicate reads, and average read length. In addition, our
approach achieved 2X speedup (on an average) over a greedy approach that also
used ML for predicting the execution times on the tested workload of sequences.
Finally, our approach achieved 1.6X speedup (on an average) over a dynamic
approach that executed the workload based on availability of resources without
using any ML-based time predictions.

</details>


### [81] [Coherence-Aware Task Graph Modeling for Realistic Application](https://arxiv.org/abs/2509.09094)
*Guochu Xiong,Xiangzhong Luo,Weichen Liu*

Main category: cs.DC

TL;DR: CoTAM是一个面向真实工作负载的一致性感知任务图生成框架，通过分离一致性影响、学习权重量化并推断依赖，生成反映运行时一致性交互的通用显式任务图，从而提高系统级分析的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有任务图建模方法要么不生成显式任务图，要么依赖固定调度模型，且往往忽视缓存一致性与任务间交互，导致设计与实际运行不匹配。需要一种既能从真实工作负载中得到通用显式任务图，又能考虑一致性影响的方法。

Method: CoTAM通过三步实现：1）分析并分离缓存一致性对执行的影响（将一致性相关延迟与基线执行分离）；2）基于学习的加权机制量化一致性影响，为不同依赖赋予权重；3）根据量化结果推断任务间的依赖并生成一致性感知的任务图。

Result: 大量实验表明，CoTAM优于基于隐式的方法，在桥接动态工作负载行为与系统设计之间的差距上具有明显优势，且强调将缓存一致性纳入任务图建模对准确性和泛化性的重要性。

Conclusion: CoTAM提出了一种关注缓存一致性影响的任务图建模框架，通过分离一致性影响并用学习到的权重量化，从运行时行为中推断任务间依赖关系，生成统一的任务图，以更真实地反映多核系统中一致性与调度交互的动态行为。

Abstract: As multicore systems continue to scale, cache coherence has emerged as a
critical determinant of system performance, with coherence behavior and task
execution closely intertwined, reshaping inter-task dependencies. Task graph
modeling provides a structured way to capture such dependencies and serves as
the foundation for many system-level design strategies. However, these
strategies typically rely on predefined task graphs, while many real-world
applications lack explicit graphs and exhibit dynamic, data-dependent behavior,
limiting the effectiveness of static approaches. To address this, several task
graph modeling methods for realistic workloads have been developed. Yet, they
either rely on implicit techniques that use application-specific features
without producing explicit graphs, or they generate graphs tailored to fixed
scheduling models, which limits generality. More importantly, they often
overlook coherence interactions, creating a gap between design assumptions and
actual runtime behavior. To overcome these limitations, we propose CoTAM, a
Coherence-Aware Task Graph Modeling framework for realistic workloads that
constructs a unified task graph reflecting runtime behavior. CoTAM analyzes the
impact of coherence by decoupling its effects from overall execution,
quantifies its influence through a learned weighting scheme, and infers
inter-task dependencies for coherence-aware graph generation. Extensive
experiments show that CoTAM outperforms implicit methods, bridging the gap
between dynamic workload behavior and existing designs while demonstrating the
importance of incorporating cache coherence into task graph modeling for
accurate and generalizable system-level analysis.

</details>


### [82] [WebAssembly and Unikernels: A Comparative Study for Serverless at the Edge](https://arxiv.org/abs/2509.09400)
*Valerio Besozzi,Enrico Fiasco,Marco Danelutto,Patrizio Dazzi*

Main category: cs.DC

TL;DR: WebAssembly适合超短、轻量的边缘函数以最小冷启动；Firecracker基MicroVM在复杂与I/O密集场景有更佳稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘Serverless与紧急边缘计算（UEC）场景下，减小冷启动延时至关重要，因此需要评估轻量级执行环境（WebAssembly与MicroVM）哪个更适合。

Method: 实现并比较了两个运行环境：基于Wasmtime的WebAssembly运行时Limes与基于Firecracker的SPARE MicroVM环境，测量冷启动时间与执行性能，包含轻量与复杂I/O密集型工作负载。

Result: 实验结果显示：WebAssembly在轻量函数上冷启动显著更低；但在复杂计算或I/O密集型任务中，Firecracker MicroVM表现出更高且更稳定的冷启动以及更好的执行性能。

Conclusion: WebAssembly在轻量级函数上比MicroVM冷启动更快，但在复杂或I/O密集型任务上性能和稳定性不及基于Firecracker的MicroVM。两者在UEC场景中各有优劣，应根据函数特性选择。

Abstract: Serverless computing at the edge requires lightweight execution environments
to minimize cold start latency, especially in Urgent Edge Computing (UEC). This
paper compares WebAssembly and unikernel-based MicroVMs for serverless
workloads. We present Limes, a WebAssembly runtime built on Wasmtime, and
evaluate it against the Firecracker-based environment used in SPARE. Results
show that WebAssembly offers lower cold start times for lightweight functions
but suffers with complex workloads, while Firecracker provides higher, but
stable, cold starts and better execution performance, particularly for
I/O-heavy tasks.

</details>


### [83] [Barycentric Coded Distributed Computing with Flexible Recovery Threshold for Collaborative Mobile Edge Computing](https://arxiv.org/abs/2509.09435)
*Houming Qiu,Kun Zhu,Dusit Niyato,Nguyen Cong Luong,Changyan Yi,Chen Dai*

Main category: cs.DC

TL;DR: 用重心有理插值构建的近似CDC，支持任意返回结果解码、无极点高数值稳定性并可调精度，结合BRI梯度编码在抗慢节点和训练加速上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有CDC方案要求达到固定恢复阈值才能解码且编码/解码函数存在极点导致数值不稳定，严重限制了在存在慢节点的MEC系统中的鲁棒性和灵活性，亟需一种可利用任意返回结果、数值稳定且支持精度调控的近似CDC方法。

Method: 提出将重心有理插值应用于CDC，设计无极点的编码/解码函数，并在此基础上构建BRI梯度编码算法；理论分析数值稳定性与近似误差，推导用任意数量返回结果的解码过程和精度控制方法；在仿真或真实平台上与传统CDC方法（如多项式插值等）进行性能比较，评估等待时间、解码准确率和训练加速效果。

Result: BRI-CDC能在任意数量返回结果下进行解码，支持有限域与实数域计算；去除极点后提升数值稳定性并能通过参数调节近似精度；BRI梯度编码在训练任务中减少等待时间并对慢节点更鲁棒；实验结果显示等待时间和近似精度优于传统CDC方案。

Conclusion: 该论文提出基于重心有理插值（BRI）的近似编码分布式计算（CDC）方案，以解决现有CDC方案在恢复阈值刚性、数值不稳定及编码/解码函数存在极点等问题。新方案能够利用任意返回结果解码、支持有限域与实数域计算、消除极点保证数值稳定并可调精度，同时提出基于BRI的梯度编码算法以加速训练并具备抗慢节点能力。实验显示该方案在等待时间和近似精度上均优于现有方法。

Abstract: Collaborative mobile edge computing (MEC) has emerged as a promising paradigm
to enable low-capability edge nodes to cooperatively execute
computation-intensive tasks. However, straggling edge nodes (stragglers)
significantly degrade the performance of MEC systems by prolonging computation
latency. While coded distributed computing (CDC) as an effective technique is
widely adopted to mitigate straggler effects, existing CDC schemes exhibit two
critical limitations: (i) They cannot successfully decode the final result
unless the number of received results reaches a fixed recovery threshold, which
seriously restricts their flexibility; (ii) They suffer from inherent poles in
their encoding/decoding functions, leading to decoding inaccuracies and
numerical instability in the computational results. To address these
limitations, this paper proposes an approximated CDC scheme based on
barycentric rational interpolation. The proposed CDC scheme offers several
outstanding advantages. Firstly, it can decode the final result leveraging any
returned results from workers. Secondly, it supports computations over both
finite and real fields while ensuring numerical stability. Thirdly, its
encoding/decoding functions are free of poles, which not only enhances
approximation accuracy but also achieves flexible accuracy tuning. Fourthly, it
integrates a novel BRI-based gradient coding algorithm accelerating the
training process while providing robustness against stragglers. Finally,
experimental results reveal that the proposed scheme is superior to existing
CDC schemes in both waiting time and approximate accuracy.

</details>


### [84] [Weaker Assumptions for Asymmetric Trust](https://arxiv.org/abs/2509.09493)
*Ignacio Amores-Sesar,Christian Cachin,Juan Villacis*

Main category: cs.DC

TL;DR: 在不对称信任系统中，提出了更弱假设下可行的可靠广播和共识算法，通过新的问题刻画保留不对称信任优势，优于先前过强的假设。


<details>
  <summary>Details</summary>
Motivation: 现有模型或假设要么导致不可解，要么通过强假设恢复可解性但失去不对称信任的优势；因此需要更精细的刻画与更弱的可解性假设。

Method: 通过重新刻画不对称问题（引入新的必要条件/结构）并设计基于该刻画的算法，证明这些算法在比先前工作更弱的假设下仍能实现可靠广播和共识；并讨论可推广性。

Result: 给出新的刻画框架与相应算法，证明比先前方法要求的信任/交叉条件更弱，从而保留更多不对称性；并说明方法可扩展到其他核心问题。

Conclusion: 作者指出在不对称信任模型中，若仅满足传统的一致性与可用性性质，则可靠广播与共识不可解；现有增强假设过于严格，削弱不对称信任的优势。提出新的问题刻画方法并据此给出更弱假设下的可靠广播与共识算法。

Abstract: In distributed systems with asymmetric trust, each participant is free to
make its own trust assumptions about others, captured by an asymmetric quorum
system. This contrasts with ordinary, symmetric quorum systems and threshold
models, where trust assumptions are uniformly shared among participants.
Fundamental problems like reliable broadcast and consensus are unsolvable in
the asymmetric model if quorum systems satisfy only the classical properties of
consistency and availability. Existing approaches overcome this by introducing
stronger assumptions. We show that some of these assumptions are overly
restrictive, so much so that they effectively eliminate the benefits of
asymmetric trust. To address this, we propose a new approach to characterize
asymmetric problems and, building upon it, present algorithms for reliable
broadcast and consensus that require weaker assumptions than previous
solutions. Our methods are general and can be extended to other core problems
in systems with asymmetric trust.

</details>


### [85] [TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes](https://arxiv.org/abs/2509.09525)
*Jialiang Huang,Teng Ma,Zheng Liu,Sixing Lin,Kang Chen,Jinlei Jiang,Xia Liao,Yingdi Shan,Yongwei Wu,Ning Zhang,Mengting Lu,Tao Ma,Haifeng Gong,Mingxing Zhang*

Main category: cs.DC

TL;DR: TrEnv为LLM代理定制高密度无服务器平台，通过环境复用与VM优化，大幅降低延迟与内存开销。


<details>
  <summary>Details</summary>
Motivation: LLM代理具有不可预测的调用模式和可变资源需求，导致在传统无服务器平台上基础设施开销显著，甚至占到LLM API调用成本的较大比例，亟需更高效的高密度无服务器方案。

Method: 提出一种可同时支持容器和VM的协同设计平台，利用可复用沙箱与内存模板实现环境快速重用与恢复；对VM场景进一步引入浏览器共享和页缓存绕过以减少额外开销。

Result: 在容器场景下，P99延迟最多降低7倍，内存使用降低48%；在VM场景下相比E2B，P99延迟降低最多58%，内存节省61%。

Conclusion: TrEnv通过复用沙箱、内存模板、浏览器共享及页面缓存绕过等技术，显著降低了面向LLM代理的无服务器平台的启动延迟和内存开销，提高了资源密度与响应性能。

Abstract: Serverless computing provides dynamic scalability, but its infrastructure
overhead becomes a bottleneck for emerging workloads such as LLM agents, which
exhibit unpredictable invocation patterns and variable resource demands. Our
analysis shows that for these agents, the cost of running on serverless
platforms can reach up to 70% of the cost of LLM API calls. This finding
motivates the need for a more efficient, high-density serverless platform. We
present TrEnv, a co-designed serverless platform that supports both container-
and VM-based environments, optimized for the unique demands of LLM agents.
TrEnv reduces startup latency and memory usage through repurposable sandboxes
and memory templates, which enable fast reuse and restoration of execution
environments. To further reduce overhead in VM-based agent workloads, TrEnv
leverages browser sharing and a page cache bypassing mechanism. Evaluations
show that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in
container-based settings, and achieves up to 58% lower P99 latency and 61%
memory savings for VM-based agents compared to state-of-the-art systems like
E2B.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [86] [Cross-Service Token: Finding Attacks in 5G Core Networks](https://arxiv.org/abs/2509.08992)
*Anqi Chen,Riccardo Preatoni,Alessandro Brighente,Mauro Conti,Cristina Nita-Rotaru*

Main category: cs.CR

TL;DR: 提出用于5G SBI的语法驱动模糊测试框架FivGeeFuzz，通过自动派生3GPP语法并对free5GC测试，发现并推动修复了8个关键安全漏洞，表明SBA环境下需重视跨服务令牌与访问控制缺陷。


<details>
  <summary>Details</summary>
Motivation: 5G SBA架构中核心网络功能通过HTTP SBIs模块化部署于云端，云环境易受内部攻击，需评估被攻陷网络功能能否越权访问资源与滥用权限。

Method: 设计并实现了FivGeeFuzz：自动从3GPP API规范提取语法，生成畸形与语义不一致输入进行模糊测试；结合自动化漏洞检测与人工校验与漏洞根因分析。对free5GC（开源且实现了R17 SBIs与访问控制）的实际测试与验证。

Result: 在free5GC中发现8个新漏洞，导致运行时崩溃、错误处理不当与未授权资源访问；发现的Cross-Service Token Attack特别严重；7个漏洞已修补，1个正在开发补丁。

Conclusion: 该论文证明了基于语法的模糊测试能有效发现5G核心网服务基于HTTP API的安全漏洞，尤其在云环境下通过被攻陷的网络功能滥用访问资源的风险。

Abstract: 5G marks a major departure from previous cellular architectures, by
transitioning from a monolithic design of the core network to a Service-Based
Architecture (SBA) where services are modularized as Network Functions (NFs)
which communicate with each other via standard-defined HTTP-based APIs called
Service-Based Interfaces (SBIs). These NFs are deployed in private and public
cloud infrastructure, and an access control framework based on OAuth restricts
how they communicate with each other and obtain access to resources. Given the
increased vulnerabilities of clouds to insiders, it is important to study the
security of the 5G Core services for vulnerabilities that allow attackers to
use compromised NFs to obtain unauthorized access to resources.
  We present FivGeeFuzz, a grammar-based fuzzing framework designed to uncover
security flaws in 5G core SBIs. FivGeeFuzz automatically derives grammars from
3GPP API specifications to generate malformed, unexpected, or semantically
inconsistent inputs, and it integrates automated bug detection with manual
validation and root-cause analysis. We evaluate our approach on free5GC, the
only open-source 5G core implementing Release 17-compliant SBIs with an access
control mechanism. Using FivGeeFuzz, we discovered 8 previously unknown
vulnerabilities in free5GC, leading to runtime crashes, improper error
handling, and unauthorized access to resources, including a very severe attack
we call Cross-Service Token Attack. All bugs were confirmed by the free5GC
team, 7 have already been patched, and the remaining one has a patch under
development.

</details>


### [87] [When FinTech Meets Privacy: Securing Financial LLMs with Differential Private Fine-Tuning](https://arxiv.org/abs/2509.08995)
*Sichen Zhu,Hoyeung Leung,Xiaoyi Wang,Jia Wei,Honghui Xu*

Main category: cs.CR

TL;DR: 提出一种面向设备端金融应用的轻量差分隐私LLM（DPFinLLM），在保证隐私的同时实现接近全微调模型的效果。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在FinTech的广泛应用，边缘部署带来了数据隐私风险；因此需要一种既能在设备端高效运行又能提供强隐私保障的模型。

Method: 提出将差分隐私机制与轻量化LLM架构结合，采用精简模型结构（受现有先进模型启发）并在训练过程中注入差分隐私噪声，以实现对金融数据的本地化、安全处理。

Result: 在多个金融情感数据集上的广泛实验表明，DPFinLLM在严格隐私预算下仍能实现与完全微调模型相当的性能，验证了方法的有效性。

Conclusion: DPFinLLM在严格差分隐私约束下仍能在金融情感分析任务中达到接近完全微调模型的性能，证明了其在边缘设备上实现隐私保护与高效性能的可行性。

Abstract: The integration of Large Language Models (LLMs) into financial technology
(FinTech) has revolutionized the analysis and processing of complex financial
data, driving advancements in real-time decision-making and analytics. With the
growing trend of deploying AI models on edge devices for financial
applications, ensuring the privacy of sensitive financial data has become a
significant challenge. To address this, we propose DPFinLLM, a
privacy-enhanced, lightweight LLM specifically designed for on-device financial
applications. DPFinLLM combines a robust differential privacy mechanism with a
streamlined architecture inspired by state-of-the-art models, enabling secure
and efficient processing of financial data. This proposed DPFinLLM can not only
safeguard user data from privacy breaches but also ensure high performance
across diverse financial tasks. Extensive experiments on multiple financial
sentiment datasets validate the effectiveness of DPFinLLM, demonstrating its
ability to achieve performance comparable to fully fine-tuned models, even
under strict privacy constraints.

</details>


### [88] [Beyond Tag Collision: Cluster-based Memory Management for Tag-based Sanitizers](https://arxiv.org/abs/2509.09089)
*Mengfei Xie,Yan Lin,Hongtao Wu,Jianming Fu,Chenke Luo,Guojun Peng*

Main category: cs.CR

TL;DR: ClusterTag通过簇化分配与簇级随机化，均衡缓解标签碰撞的时间与空间问题，提升检测确定性且性能损耗极低。


<details>
  <summary>Details</summary>
Motivation: 由于标签编码空间有限，现有标签分配策略在时间/空间维度上难以避免碰撞，导致检测失误和不确定性，需设计能同时缓解两种维度碰撞的分配方案。

Method: 提出一种簇式内存分配器：将对象划分为多个独立簇，每簇内部限定碰撞范围；在簇间引入随机地址间隔（簇级随机化）以增加地址熵，打破标签空间限制；实现为独立分配器并与HWASan等标签消毒器集成。

Result: 实现的ClusterTag在不同随机化密度下维持与原有分配器相当的性能开销（约1%以内）；在Juliet数据集上500次重复测试结果确定性强（5652条检测到、1530条漏报），而现有三种策略存在概率性假阴性；在最小、平均、不可预测性三种碰撞距离度量上均表现平衡性改进。

Conclusion: ClusterTag通过将内存对象分簇并结合簇粒度的堆随机化，有效减轻了标签碰撞在时间和空间维度上的负面影响，从而提升了基于标签的内存消毒器的检测确定性。

Abstract: Tag-based sanitizers attach a small "key" to each pointer and a matching
"lock" tag to its target memory object, enabling runtime verification of
pointer-object consistency and helping developers to detect potential memory
violations. However, the limited tag encoding space challenges existing studies
in assigning distinct tags to memory objects across temporal and spatial
dimensions, leading to potential tag collisions. In this paper, we present
ClusterTag, a novel cluster-based memory allocator aimed at simultaneously
mitigating tag collisions in both temporal and spatial dimensions. The core
design of ClusterTag effectively balances the significant mismatch between tag
encoding space and memory objects: it divides memory objects into multiple
independent clusters, thereby limiting tag collisions to finite chunks within
each cluster. To mitigate tag collisions across clusters, we design a
cluster-grained heap randomization scheme. This approach introduces random
address intervals between clusters and further breaks the entropy limitation of
the tag space. ClusterTag has been implemented as an independent memory
allocator that seamlessly integrates with tag-based sanitizers such as HWASan,
and maintains comparable performance overhead (within 1%) at various
randomization densities. Security evaluations on the Juliet dataset indicate
that ClusterTag exhibits deterministic results across 500 repeated tests (5,652
reported and 1,530 missed), while the existing three types of tag assignment
strategies all exhibit probabilistic false negatives due to tag collisions.
Quantitative analysis across three tag collision distance metrics-minimum,
average, and unpredictability-demonstrates that ClusterTag achieves balanced
improvements across all three, whereas prior tag assignment schemes (random,
staggered, fixed) show significant trade-offs in at least one metric.

</details>


### [89] [Towards Confidential and Efficient LLM Inference with Dual Privacy Protection](https://arxiv.org/abs/2509.09091)
*Honglan Yu,Yibin Wang,Feifei Dai,Dong Liu,Haihui Fan,Xiaoyan Gu*

Main category: cs.CR

TL;DR: CMIF将Embedding放在客户端TEE、其余放GPU，并改进Report-Noisy-Max，实现低延迟、低通信开销的私有化LLM推理，兼顾隐私与性能。


<details>
  <summary>Details</summary>
Motivation: 现有CPU TEE推理延迟高，研究采用分割部署把线性部分下放GPU，但LLM的稠密非线性层导致TEE与GPU间通信开销大；差分隐私通过加噪保障隐私但损害模型性能与语义理解。

Method: 提出分层部署：将Embedding层保存在客户端CPU TEE，后续密集非线性层在GPU服务器运行；设计并优化Report-Noisy-Max机制以在少量性能损失下提供隐私保护；减少TEE与GPU之间的通信量以降低延迟。

Result: 在Llama系列模型上的大量实验表明，CMIF显著减少了TEE中的额外推理开销并保留用户数据隐私，同时仅带来轻微的模型性能下降。

Conclusion: CMIF在客户端TEE保存Embedding层，服务器GPU运行其余层，通过优化Report-Noisy-Max在提供差分隐私保护的同时将TEE推理开销和通信延迟降到最低，并在Llama系列模型上验证了有效性。

Abstract: CPU-based trusted execution environments (TEEs) and differential privacy (DP)
have gained wide applications for private inference. Due to high inference
latency in TEEs, researchers use partition-based approaches that offload linear
model components to GPUs. However, dense nonlinear layers of large language
models (LLMs) result in significant communication overhead between TEEs and
GPUs. DP-based approaches apply random noise to protect data privacy, but this
compromises LLM performance and semantic understanding. To overcome the above
drawbacks, this paper proposes CMIF, a Confidential and efficient Model
Inference Framework. CMIF confidentially deploys the embedding layer in the
client-side TEE and subsequent layers on GPU servers. Meanwhile, it optimizes
the Report-Noisy-Max mechanism to protect sensitive inputs with a slight
decrease in model performance. Extensive experiments on Llama-series models
demonstrate that CMIF reduces additional inference overhead in TEEs while
preserving user data privacy.

</details>


### [90] [DP-FedLoRA: Privacy-Enhanced Federated Fine-Tuning for On-Device Large Language Models](https://arxiv.org/abs/2509.09097)
*Honghui Xu,Shiva Shrestha,Wei Chen,Zhiyuan Li,Zhipeng Cai*

Main category: cs.CR

TL;DR: 提出 DP-FedLoRA：结合 LoRA 与差分隐私的通信高效联邦微调框架，客户端裁剪并添加高斯噪声满足 (ε,δ)-DP，理论与实验证明实用性。


<details>
  <summary>Details</summary>
Motivation: 在设备侧部署 LLM 时，需要在不泄露用户敏感数据的前提下进行个性化微调，因此将差分隐私引入联邦 LoRA 微调中以保护隐私并保持通信效率。

Method: 在客户端对 LoRA 矩阵进行裁剪并加入高斯噪声以满足 (ε, δ)-差分隐私，集中式服务器聚合去偏置更新。

Result: 理论证明了加入噪声后更新为无偏估计，并给出噪声引入的方差上界；实验证明在主流基准上取得了有竞争力的性能和隐私保证。

Conclusion: DP-FedLoRA 在保持差分隐私同时实现 LoRA 的联邦微调是可行的。

Abstract: As on-device large language model (LLM) systems become increasingly
prevalent, federated fine-tuning enables advanced language understanding and
generation directly on edge devices; however, it also involves processing
sensitive, user-specific data, raising significant privacy concerns within the
federated learning framework. To address these challenges, we propose
DP-FedLoRA, a privacy-enhanced federated fine-tuning framework that integrates
LoRA-based adaptation with differential privacy in a communication-efficient
setting. Each client locally clips and perturbs its LoRA matrices using
Gaussian noise to satisfy ($\epsilon$, $\delta$)-differential privacy. We
further provide a theoretical analysis demonstrating the unbiased nature of the
updates and deriving bounds on the variance introduced by noise, offering
practical guidance for privacy-budget calibration. Experimental results across
mainstream benchmarks show that DP-FedLoRA delivers competitive performance
while offering strong privacy guarantees, paving the way for scalable and
privacy-preserving LLM deployment in on-device environments.

</details>


### [91] [AgriSentinel: Privacy-Enhanced Embedded-LLM Crop Disease Alerting System](https://arxiv.org/abs/2509.09103)
*Chanti Raju Mylay,Bobin Deng,Zhipeng Cai,Honghui Xu*

Main category: cs.CR

TL;DR: 提出一种在移动设备上运行、具差分隐私保护且内置微调LLM的作物病害预警系统，兼顾隐私、可用性与可操作建议。


<details>
  <summary>Details</summary>
Motivation: 现有基于AI的病害预警系统常忽视数据隐私、市场定价影响和对农户友好的可用性，造成隐私泄露与经济剥削风险，且缺乏直接可执行的管理建议。AgriSentinel旨在填补这些空白，为农户提供安全、可用且经济友好的病害预警与管理工具。

Method: 系统包含三部分：1) 使用差分隐私机制处理作物图像数据以保护敏感信息；2) 设计并优化轻量级深度学习分类模型以在移动设备上运行，保证高效、准确的病害识别；3) 在设备端部署微调的LLM，并结合整理的知识库生成具体、可执行的防治建议。

Result: 实验表明AgriSentinel在保护数据隐私的同时保持了较高的分类性能，并能在移动设备上实时运行；嵌入式LLM结合知识池提供了具体可行的病害处置策略，提升了农户决策效率与作物产量潜力。

Conclusion: AgriSentinel通过在本地设备上结合差分隐私、轻量级疾病分类模型与微调的嵌入式LLM，实现了隐私保护与可用性兼顾的作物病害预警和可操作管理建议。

Abstract: Crop diseases pose significant threats to global food security, agricultural
productivity, and sustainable farming practices, directly affecting farmers'
livelihoods and economic stability. To address the growing need for effective
crop disease management, AI-based disease alerting systems have emerged as
promising tools by providing early detection and actionable insights for timely
intervention. However, existing systems often overlook critical aspects such as
data privacy, market pricing power, and farmer-friendly usability, leaving
farmers vulnerable to privacy breaches and economic exploitation. To bridge
these gaps, we propose AgriSentinel, the first Privacy-Enhanced Embedded-LLM
Crop Disease Alerting System. AgriSentinel incorporates a differential privacy
mechanism to protect sensitive crop image data while maintaining classification
accuracy. Its lightweight deep learning-based crop disease classification model
is optimized for mobile devices, ensuring accessibility and usability for
farmers. Additionally, the system includes a fine-tuned, on-device large
language model (LLM) that leverages a curated knowledge pool to provide farmers
with specific, actionable suggestions for managing crop diseases, going beyond
simple alerting. Comprehensive experiments validate the effectiveness of
AgriSentinel, demonstrating its ability to safeguard data privacy, maintain
high classification performance, and deliver practical, actionable disease
management strategies. AgriSentinel offers a robust, farmer-friendly solution
for automating crop disease alerting and management, ultimately contributing to
improved agricultural decision-making and enhanced crop productivity.

</details>


### [92] [CryptGNN: Enabling Secure Inference for Graph Neural Networks](https://arxiv.org/abs/2509.09107)
*Pritam Sen,Yao Ma,Cristian Borcea*

Main category: cs.CR

TL;DR: CryptGNN 使用分布式 SMPC 实现对第三方 GNN 的隐私保护推理：保护客户端数据与图、保护模型参数、无可信服务器、在最多 P-1 串通下仍安全，并在理论与实验上验证其有效性与效率。


<details>
  <summary>Details</summary>
Motivation: 在 MLaaS 场景下，客户端希望保护输入数据和图结构，模型提供方希望保护其模型参数，而云提供方不能被完全信任，因而需要一种既能保持性能又能提供强隐私保证的 GNN 安全推理方案。

Method: 基于分布式 SMPC，设计了安全的消息传递和特征变换操作；支持任意数量的 SMPC 方，不依赖可信服务器；协议在最多 P-1 方串通时仍保持可证明的安全性。

Result: 理论证明了在 P 方中最多 P-1 方串通下的安全性，并通过实验证明 CryptGNN 在效率上具有可行性，表明其在安全与性能间取得了良好折中。

Conclusion: CryptGNN 提供了一种在云中对第三方 GNN 模型进行安全推理的方案，通过分布式安全多方计算（SMPC）在消息传递和特征变换层实现加密计算，达到客户端数据与图结构、模型参数之间的相互隐私保护。

Abstract: We present CryptGNN, a secure and effective inference solution for
third-party graph neural network (GNN) models in the cloud, which are accessed
by clients as ML as a service (MLaaS). The main novelty of CryptGNN is its
secure message passing and feature transformation layers using distributed
secure multi-party computation (SMPC) techniques. CryptGNN protects the
client's input data and graph structure from the cloud provider and the
third-party model owner, and it protects the model parameters from the cloud
provider and the clients. CryptGNN works with any number of SMPC parties, does
not require a trusted server, and is provably secure even if P-1 out of P
parties in the cloud collude. Theoretical analysis and empirical experiments
demonstrate the security and efficiency of CryptGNN.

</details>


### [93] [Character-Level Perturbations Disrupt LLM Watermarks](https://arxiv.org/abs/2509.09112)
*Zhaoxi Zhang,Xiaomei Zhang,Yanjun Zhang,He Zhang,Shirui Pan,Bo Liu,Asif Qumer Gill,Leo Yu Zhang*

Main category: cs.CR

TL;DR: 在现实受限查询场景下，字符级扰动（如typo、homoglyph）结合遗传算法能高效移除LLM水印，固定防御易被绕过，需新型鲁棒水印方案。


<details>
  <summary>Details</summary>
Motivation: 纠正现有研究对水印可移除性的误判，展示在更现实的受限访问威胁模型下，水印移除比此前所认为的更容易，呼吁构建更鲁棒的水印机制。

Method: 形式化水印系统与两类实际威胁模型，分析扰动对tokenization的影响，引入字符级扰动（错字、替换、同形异体字等）并设计基于遗传算法的引导移除攻击，提出自适应复合字符级攻击应对防御。

Result: 实验证明字符级扰动在受限威胁模型下显著优于词或句子级扰动；基于遗传算法的黑盒优化在有限查询下能有效降低水印检测率；自适应复合攻击能成功绕过现有防御。

Conclusion: 水印方案在受限查询和实际攻击下存在显著脆弱性；字符级扰动结合遗传算法能高效移除水印；固定防御易被针对性扰动绕过。

Abstract: Large Language Model (LLM) watermarking embeds detectable signals into
generated text for copyright protection, misuse prevention, and content
detection. While prior studies evaluate robustness using watermark removal
attacks, these methods are often suboptimal, creating the misconception that
effective removal requires large perturbations or powerful adversaries.
  To bridge the gap, we first formalize the system model for LLM watermark, and
characterize two realistic threat models constrained on limited access to the
watermark detector. We then analyze how different types of perturbation vary in
their attack range, i.e., the number of tokens they can affect with a single
edit. We observe that character-level perturbations (e.g., typos, swaps,
deletions, homoglyphs) can influence multiple tokens simultaneously by
disrupting the tokenization process. We demonstrate that character-level
perturbations are significantly more effective for watermark removal under the
most restrictive threat model. We further propose guided removal attacks based
on the Genetic Algorithm (GA) that uses a reference detector for optimization.
Under a practical threat model with limited black-box queries to the watermark
detector, our method demonstrates strong removal performance. Experiments
confirm the superiority of character-level perturbations and the effectiveness
of the GA in removing watermarks under realistic constraints. Additionally, we
argue there is an adversarial dilemma when considering potential defenses: any
fixed defense can be bypassed by a suitable perturbation strategy. Motivated by
this principle, we propose an adaptive compound character-level attack.
Experimental results show that this approach can effectively defeat the
defenses. Our findings highlight significant vulnerabilities in existing LLM
watermark schemes and underline the urgency for the development of new robust
mechanisms.

</details>


### [94] [IoTFuzzSentry: A Protocol Guided Mutation Based Fuzzer for Automatic Vulnerability Testing in Commercial IoT Devices](https://arxiv.org/abs/2509.09158)
*Priyanka Rushikesh Chaudhary,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: 论文提出IoTFuzzSentry——一个集成到Cotopaxi的变异协议模糊测试工具，自动注入传输/应用层数据包以发现并利用物联网设备中的凭证泄露、视频/图片泄露与命令注入等漏洞，已在多款商用设备上验证并负责任披露。


<details>
  <summary>Details</summary>
Motivation: 物联网设备在运行轻量级服务器并处理用户交互时，传输层或应用层安全机制的实现缺陷会导致未授权访问和数据泄露，传统手工审计难以规模化发现此类缺陷，故需自动化模糊测试方法来检测实际部署设备中的漏洞。

Method: 提出并实现了基于变异的协议模糊测试器IoTFuzzSentry，将其集成到现有测试框架Cotopaxi中，通过注入精心构造的传输层和应用层数据包对设备进行灰盒/黑盒测试，并对发现的漏洞进行漏洞利用验证。测试还扩展到额外六台设备以评估普适性。

Result: 在对商用设备的评估中，工具发现4类漏洞（凭证泄露、实时视频流泄漏、实时图片泄漏、命令注入），在三台设备上成功演示了利用场景，已提交并公布部分CVE，且对另外六台设备的流量分析显示存在类似风险。

Conclusion: 该论文通过基于变异的协议模糊测试工具IoTFuzzSentry，有效发现并验证了商用物联网设备（如IP摄像头、智能插座）中与传输和应用层相关的严重安全漏洞，展现出对实际威胁场景的利用可能性，并已负责披露部分漏洞（含两项CVE）。

Abstract: Protocol fuzzing is a scalable and cost-effective technique for identifying
security vulnerabilities in deployed Internet of Things devices. During their
operational phase, IoT devices often run lightweight servers to handle user
interactions, such as video streaming or image capture in smart cameras.
Implementation flaws in transport or application-layer security mechanisms can
expose IoT devices to a range of threats, including unauthorized access and
data leakage. This paper addresses the challenge of uncovering such
vulnerabilities by leveraging protocol fuzzing techniques that inject crafted
transport and application-layer packets into IoT communications. We present a
mutation-based fuzzing tool, named IoTFuzzSentry, to identify specific
non-trivial vulnerabilities in commercial IoT devices. We further demonstrate
how these vulnerabilities can be exploited in real-world scenarios. We
integrated our fuzzing tool into a well-known testing tool Cotopaxi and
evaluated it with commercial-off-the-shelf IoT devices such as IP cameras and
Smart Plug. Our evaluation revealed vulnerabilities categorized into 4 types
(IoT Access Credential Leakage, Sneak IoT Live Video Stream, Creep IoT Live
Image, IoT Command Injection) and we show their exploits using three IoT
devices. We have responsibly disclosed all these vulnerabilities to the
respective vendors. So far, we have published two CVEs, CVE-2024-41623 and
CVE-2024-42531, and one is awaiting. To extend the applicability, we have
investigated the traffic of six additional IoT devices and our analysis shows
that these devices can have similar vulnerabilities, due to the presence of a
similar set of application protocols. We believe that IoTFuzzSentry has the
potential to discover unconventional security threats and allow IoT vendors to
strengthen the security of their commercialized IoT devices automatically with
negligible overhead.

</details>


### [95] [A Cyber-Twin Based Honeypot for Gathering Threat Intelligence](https://arxiv.org/abs/2509.09222)
*Muhammad Azmi Umer,Zhan Xuna,Yan Lin Aung,Aditya P. Mathur,Jianying Zhou*

Main category: cs.CR

TL;DR: 基于网络孪生的水厂蜜罐可有效吸引并记录对水处理厂的真实网络攻击，产生可用于改进防护的威胁情报；作者已部署并验证了该系统，并分析了实际发生的赎金软件攻击。


<details>
  <summary>Details</summary>
Motivation: 关键基础设施（如水处理厂）面临日益严重的网络威胁，传统防护手段难以全面预测攻击者策略。通过构建逼真的蜜罐，可以主动诱捕攻击者，获取真实攻击数据，从而提升威胁情报质量和防护能力。

Method: 构建一个运行中的水处理厂网络孪生蜜罐，复现工业控制系统（ICS）设备与通信协议；部署监控与日志记录机制以捕获攻击者行为；对捕获的攻击样本（如赎金软件事件）进行取证与行为分析；将分析结果整理为威胁情报并分享给水厂管理者。

Result: 成功部署了一个运营中的水厂蜜罐，并记录到多次真实攻击事件。文中详细描述了至少一次赎金软件攻击的过程与分析，证明该蜜罐可以生成有价值的威胁情报供管理者用于加强防护措施。

Conclusion: 本文提出并实现了基于网络孪生（cyber twin）的水厂蜜罐，用于模拟真实水处理厂以吸引并记录针对关键基础设施的网络攻击。通过记录与分析攻击行为获取威胁情报，并将情报反馈给水厂管理方以改进防护。实装蜜罐已多次遭受攻击（包括赎金软件），并对其中一次攻击进行了详细描述。

Abstract: Critical Infrastructure (CI) is prone to cyberattacks. Several techniques
have been developed to protect CI against such attacks. In this work, we
describe a honeypot based on a cyber twin for a water treatment plant. The
honeypot is intended to serve as a realistic replica of a water treatment plant
that attracts potential attackers. The attacks launched on the honeypot are
recorded and analyzed for threat intelligence. The intelligence so obtained is
shared with the management of water treatment plants, who in turn may use it to
improve plant protection systems. The honeypot used here is operational and has
been attacked on several occasions using, for example, a ransomware attack that
is described in detail.

</details>


### [96] [Enhancing Cyber Threat Hunting -- A Visual Approach with the Forensic Visualization Toolkit](https://arxiv.org/abs/2509.09185)
*Jihane Najar,Marinos Tsantekidis,Aris Sotiropoulos,Vassilis Prevelakis*

Main category: cs.CR

TL;DR: FVT是面向威胁狩猎与数字取证的交互式可视化工具，已在真实场景和EU项目中验证能强化安全分析与响应。


<details>
  <summary>Details</summary>
Motivation: 现有自动化安全防护可能漏报高阶威胁，需主动威胁狩猎与直观交互可视化工具来提升态势感知与风险管理。

Method: 提出并实现了Forensic Visualization Toolkit，通过多种可视化组件与交互分析工具支持威胁狩猎和数字取证，并在真实场景中验证其功能。

Result: FVT在实际案例中显著提高分析效率和威胁发现能力，且被多项EU项目采纳与持续完善。

Conclusion: FVT增强了对高级威胁的检测与响应能力，提升可视化与取证分析效率，并在多个欧盟资助项目中持续集成与改进。

Abstract: In today's dynamic cyber threat landscape, organizations must take proactive
steps to bolster their cybersecurity defenses. Cyber threat hunting is a
proactive and iterative process aimed at identifying and mitigating advanced
threats that may go undetected by traditional security measures. Rather than
waiting for automated security systems to flag potential threats, threat
hunting involves actively searching for signs of malicious activity within an
organization's network. In this paper, we present the Forensic Visualization
Toolkit, a powerful tool designed for digital forensics investigations,
analysis of digital evidence, and advanced visualizations to enhance
cybersecurity situational awareness and risk management and empower security
analysts with an intuitive and interactive tool. Through practical, real-world
scenarios, we demonstrate how FVT significantly amplifies the capabilities of
cybersecurity professionals, enabling them to effectively identify, analyze,
and respond to threats. Furthermore, it is important to highlight that FVT has
been integrated into, utilized, and continually enhanced within various
EU-funded research projects over recent years.

</details>


### [97] [What You Code Is What We Prove: Translating BLE App Logic into Formal Models with LLMs for Vulnerability Detection](https://arxiv.org/abs/2509.09291)
*Biwei Yan,Yue Zhang,Minghui Xu,Runyu Pan,Jinku Li,Xiuzhen Cheng*

Main category: cs.CR

TL;DR: 把LLM用作从真实BLE代码到可验证进程模型的翻译器，结合静态分析与符号验证，VerifiaBLE实现了对1050款安卓BLE应用的可扩展安全检查，发现大多数应用缺乏加密、认证和随机性保护。


<details>
  <summary>Details</summary>
Motivation: 手动构建形式化模型工作量大，不适合大规模分析；BLE应用层安全易被忽视，需要可扩展、自动化的验证方法。

Method: 提出将BLE应用安全分析视为从代码到形式模型的语义翻译问题；实现系统VerifiaBLE，结合静态分析定位相关代码、基于提示工程引导LLM将BLE代码翻译为ProVerif可接受的进程模型，最后使用符号验证工具检查加密、随机性与认证三类属性。

Result: 在1050个安卓BLE应用上评估：仅10.2%实现了三项保护，53.9%完全缺失；表明系统能发现系统性弱点并证明将LLM作为结构化翻译器可降低形式化方法门槛。

Conclusion: BLE应用层普遍缺乏加密、认证与新鲜性保护，导致大规模安全问题；将LLM作为语义翻译器能把真实代码转为可验证模型，从而实现可扩展的形式化验证。

Abstract: The application layer of Bluetooth Low Energy (BLE) is a growing source of
security vulnerabilities, as developers often neglect to implement critical
protections such as encryption, authentication, and freshness. While formal
verification offers a principled way to check these properties, the manual
effort of constructing formal models makes it impractical for large-scale
analysis. This paper introduces a key insight: BLE application security
analysis can be reframed as a semantic translation problem, i.e., from
real-world code to formal models. We leverage large language models (LLMs) not
to directly detect vulnerabilities, but to serve as translators that convert
BLE-specific code into process models verifiable by tools like ProVerif. We
implement this idea in VerifiaBLE, a system that combines static analysis,
prompt-guided LLM translation, and symbolic verification to check three core
security features: encryption, randomness, and authentication. Applied to 1,050
Android BLE apps, VerifiaBLE uncovers systemic weaknesses: only 10.2\% of apps
implement all three protections, while 53.9\% omit them entirely. Our work
demonstrates that using LLMs as structured translators can lower the barrier to
formal methods, unlocking scalable verification across security-critical
domains.

</details>


### [98] [Shell or Nothing: Real-World Benchmarks and Memory-Activated Agents for Automated Penetration Testing](https://arxiv.org/abs/2509.09207)
*Wuyuao Mai,Geng Hong,Qi Liu,Jinsong Chen,Jiarun Dai,Xudong Pan,Yuan Zhang,Min Yang*

Main category: cs.CR

TL;DR: 提出第一个真实场景的自动化渗透测试基准TermiBench和多智能体框架TermiAgent，通过定位记忆激活和结构化代码理解显著提升真实渗透测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动渗透测试的评估多依赖简化的CTF设置，嵌入先验知识并降低复杂性，导致性能评估与实际差距大，需要真实、面向代理的基准与更鲁棒的agent方法。

Method: 构建了包含510台主机、25类服务和30个CVE的真实场景基准，并提出TermiAgent：引入Located Memory Activation机制缓解长上下文遗忘，通过结构化代码理解构建可靠的利用代码库，多智能体协同完成侦察、识别和漏洞利用。

Result: 在TermiBench上实验表明：现有系统难以在真实条件下获取系统shell；TermiAgent在渗透能力、执行时间、成本三方面均优于最先进方法，并可在笔记本级别部署。

Conclusion: 本文提出了面向真实场景的自动化渗透测试基准TermiBench，并设计了多智能体渗透测试框架TermiAgent，在真实主机服务和CVE集合下进行评估，证明现有方法在真实条件下效果较差且TermiAgent能显著改进。

Abstract: Penetration testing is critical for identifying and mitigating security
vulnerabilities, yet traditional approaches remain expensive, time-consuming,
and dependent on expert human labor. Recent work has explored AI-driven
pentesting agents, but their evaluation relies on oversimplified
capture-the-flag (CTF) settings that embed prior knowledge and reduce
complexity, leading to performance estimates far from real-world practice. We
close this gap by introducing the first real-world, agent-oriented pentesting
benchmark, TermiBench, which shifts the goal from 'flag finding' to achieving
full system control. The benchmark spans 510 hosts across 25 services and 30
CVEs, with realistic environments that require autonomous reconnaissance,
discrimination between benign and exploitable services, and robust exploit
execution. Using this benchmark, we find that existing systems can hardly
obtain system shells under realistic conditions.
  To address these challenges, we propose TermiAgent, a multi-agent penetration
testing framework. TermiAgent mitigates long-context forgetting with a Located
Memory Activation mechanism and builds a reliable exploit arsenal via
structured code understanding rather than naive retrieval. In evaluations, our
work outperforms state-of-the-art agents, exhibiting stronger penetration
testing capability, reducing execution time and financial cost, and
demonstrating practicality even on laptop-scale deployments. Our work delivers
both the first open-source benchmark for real-world autonomous pentesting and a
novel agent framework that establishes a milestone for AI-driven penetration
testing.

</details>


### [99] [On the Security of SSH Client Signatures](https://arxiv.org/abs/2509.09331)
*Fabian Bäumer,Marcus Brinkmann,Maximilian Radoy,Jörg Schwenk,Juraj Somorovsky*

Main category: cs.CR

TL;DR: 收集并长时间分析了3,162万SSH客户端公钥，结合黑盒签名实验，发现少量因短密钥、弱随机和小因子受损，且揭示PuTTY的确定性ECDSA nonce实现可致私钥泄露。


<details>
  <summary>Details</summary>
Motivation: 尽管互联网上可扫描服务器端SSH，客户端密钥无法通过扫描测量；研究旨在填补这一缺口，评估公开客户端公钥的安全性并测试客户端签名实现是否安全。

Method: 一是从GitHub/GitLab等公开平台收集并纵向分析3.16亿?（实际为31,622,338）公钥，二是对24款主流SSH客户端（Linux/Windows/macOS）进行黑盒签名实现分析与漏洞复现测试。

Result: 发现逐步从RSA迁移到EdDSA；多数密钥安全，但检出98个损坏短密钥、139个因弱随机性生成的密钥、149个具有常见或小因子的密钥。此外首次发现确定性nonce在ECDSA下可被利用：PuTTY在P-521下用确定性nonce导致私钥可由58个签名恢复（CVE-2024-31497）。

Conclusion: 本文填补了SSH客户端公钥测量的空白，展示了对公开平台收集密钥的长期安全测试并通过黑盒实验发现多款客户端实现缺陷。

Abstract: Administrators and developers use SSH client keys and signatures for
authentication, for example, to access internet backbone servers or to commit
new code on platforms like GitHub. However, unlike servers, SSH clients cannot
be measured through internet scans. We close this gap in two steps. First, we
collect SSH client public keys. Such keys are regularly published by their
owners on open development platforms like GitHub and GitLab. We systematize
previous non-academic work by subjecting these keys to various security tests
in a longitudinal study. Second, in a series of black-box lab experiments, we
analyze the implementations of algorithms for SSH client signatures in 24
popular SSH clients for Linux, Windows, and macOS.
  We extracted 31,622,338 keys from three public sources in two scans. Compared
to previous work, we see a clear tendency to abandon RSA signatures in favor of
EdDSA signatures. Still, in January 2025, we found 98 broken short keys, 139
keys generated from weak randomness, and 149 keys with common or small
factors-the large majority of the retrieved keys exposed no weakness.
  Weak randomness can not only compromise a secret key through its public key,
but also through signatures. It is well-known that a bias in random nonces in
ECDSA can reveal the secret key through public signatures. For the first time,
we show that the use of deterministic nonces in ECDSA can also be dangerous:
The private signing key of a PuTTY client can be recovered from just 58 valid
signatures if ECDSA with NIST curve P-521 is used. PuTTY acknowledged our
finding in CVE-2024-31497, and they subsequently replaced the nonce generation
algorithm.

</details>


### [100] [[Extended] Ethics in Computer Security Research: A Data-Driven Assessment of the Past, the Present, and the Possible Future](https://arxiv.org/abs/2509.09351)
*Harshini Sri Ramulu,Helen Schmitt,Bogdan Rerich,Rachel Gonzalez Rodriguez,Tadayoshi Kohno,Yasemin Acar*

Main category: cs.CR

TL;DR: 对1154篇论文的系统回顾与24位研究者访谈表明：计算机安全研究在伦理报告与决策上缺乏一致性，更多关注手续性合规而非价值权衡，需建立更明确的伦理指南与标准化流程。


<details>
  <summary>Details</summary>
Motivation: 动机是填补计算机安全研究中缺乏明确指导和一致性在伦理决策、记录与评估方面的空白，特别是在道德问题不清晰时，帮助提升研究的伦理质量与透明度。

Method: 作者通过两部分方法：1) 系统性回顾2024年发表的1154篇顶会/顶刊安全论文，评估伦理相关报告的现状；2) 对24名计算机安全与隐私领域研究者（包括审稿人、伦理委员会成员和程序主席）进行半结构化访谈，分析他们作为作者和审稿人在伦理决策中的实践与观点。

Result: 结果显示：伦理报告水准不一，偏重伦理审查和人类受试者保护、责任披露，但很少讨论利益-伤害权衡；访谈表明研究者重视伦理却在价值考虑、伦理框架、决策过程和结果上缺乏一致性。作者基于此提出改进建议。

Conclusion: 本文结论是：计算机安全研究中关于伦理的报告和决策存在显著不一致，通常侧重于机构/伦理委员会批准、人类受试者保护和负责任披露，但缺乏对权衡伤害与利益的讨论。研究者有意做好伦理但缺乏统一的价值考量与框架；因此需改进伦理讨论与标准化流程。

Abstract: Ethical questions are discussed regularly in computer security. Still,
researchers in computer security lack clear guidance on how to make, document,
and assess ethical decisions in research when what is morally right or
acceptable is not clear-cut. In this work, we give an overview of the
discussion of ethical implications in current published work in computer
security by reviewing all 1154 top-tier security papers published in 2024,
finding inconsistent levels of ethics reporting with a strong focus of
reporting institutional or ethics board approval, human subjects protection,
and responsible disclosure, and a lack of discussion of balancing harms and
benefits. We further report on the results of a semi-structured interview study
with 24 computer security and privacy researchers (among whom were also:
reviewers, ethics committee members, and/or program chairs) and their ethical
decision-making both as authors and during peer review, finding a strong desire
for ethical research, but a lack of consistency in considered values, ethical
frameworks (if articulated), decision-making, and outcomes. We present an
overview of the current state of the discussion of ethics and current de-facto
standards in computer security research, and contribute suggestions to improve
the state of ethics in computer security research.

</details>


### [101] [ENSI: Efficient Non-Interactive Secure Inference for Large Language Models](https://arxiv.org/abs/2509.09424)
*Zhiyu He,Maojiang Wang,Xinwen Gao,Yuchuan Luo,Lin Liu,Shaojing Fu*

Main category: cs.CR

TL;DR: ENSI通过协议与模型协同设计，结合CKKS、轻量化BitNet、sigmoid attention和将Bootstrapping嵌入RMSNorm，显著提升在HE下的LLM非交互式安全推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前将同态加密等密码学协议与大规模LLM结合面临计算复杂度高、模型规模大和架构复杂性导致的实用性限制，亟需在加密协议与模型架构上协同优化以实现可行的安全推理。

Method: 提出ENSI框架：1) 用优化编码策略将CKKS与轻量化模型BitNet结合，降低加密矩阵乘法复杂度；2) 用sigmoid attention替代softmax以避免在HE下高昂的softmax开销，实现不需重训练的替换；3) 将Bootstrapping嵌入RMSNorm以高效刷新密文并大幅减少Bootstrapping调用频次。

Result: 实验表明，在CPU上相较于最先进方法，ENSI在矩阵乘法上约8倍加速，softmax推理上2.6倍加速，且Bootstrapping占比仅1%。

Conclusion: ENSI通过协同设计加密协议与轻量化LLM架构，实现了在同态加密下高效的非交互式安全推理，显著降低了加密矩阵乘法和注意力计算的开销，并通过将Bootstrapping嵌入RMSNorm减少了昂贵的刷新频次，从而在CPU上相比现有方法分别实现了近8倍矩阵乘法加速和2.6倍softmax推理加速，Bootstrap比例降至1%。

Abstract: Secure inference enables privacy-preserving machine learning by leveraging
cryptographic protocols that support computations on sensitive user data
without exposing it. However, integrating cryptographic protocols with large
language models (LLMs) presents significant challenges, as the inherent
complexity of these protocols, together with LLMs' massive parameter scale and
sophisticated architectures, severely limits practical usability. In this work,
we propose ENSI, a novel non-interactive secure inference framework for LLMs,
based on the principle of co-designing the cryptographic protocols and LLM
architecture. ENSI employs an optimized encoding strategy that seamlessly
integrates CKKS scheme with a lightweight LLM variant, BitNet, significantly
reducing the computational complexity of encrypted matrix multiplications. In
response to the prohibitive computational demands of softmax under homomorphic
encryption (HE), we pioneer the integration of the sigmoid attention mechanism
with HE as a seamless, retraining-free alternative. Furthermore, by embedding
the Bootstrapping operation within the RMSNorm process, we efficiently refresh
ciphertexts while markedly decreasing the frequency of costly bootstrapping
invocations. Experimental evaluations demonstrate that ENSI achieves
approximately an 8x acceleration in matrix multiplications and a 2.6x speedup
in softmax inference on CPU compared to state-of-the-art method, with the
proportion of bootstrapping is reduced to just 1%.

</details>


### [102] [Prompt Pirates Need a Map: Stealing Seeds helps Stealing Prompts](https://arxiv.org/abs/2509.09488)
*Felix Mächtle,Ashwath Shetty,Jonas Sander,Nils Loose,Sören Pirk,Thomas Eisenbarth*

Main category: cs.CR

TL;DR: 作者利用PyTorch在CPU上对种子范围的限制发现噪声生成漏洞，构建SeedSnitch暴力恢复种子，并基于恢复的种子用遗传算法（PromptPirate）实现更准确的提示窃取，显著优于现有方法，并提出有效防御。


<details>
  <summary>Details</summary>
Motivation: 保护扩散模型生成图片所含的高价值提示不被窃取，揭示现有提示恢复方法的局限，发现并利用框架层面的随机数生成弱点以实现更高效的提示窃取，同时推动修补该漏洞以提升安全性。

Method: 从理论分析入手，指出优化方法忽视了初始噪声；发现PyTorch在CPU上将种子限制到2^{32}导致的可枚举空间漏洞；实现SeedSnitch进行大规模暴力搜索并在CivitAI数据集上实证种子恢复率；基于恢复的种子开发遗传算法PromptPirate用于提示窃取，并与PromptStealer、P2HP和CLIP-Interrogator等进行了LPIPS对比评估；最后提出并验证防御策略。

Result: 在CivitAI图像上约95%的种子可在140分钟内被恢复；PromptPirate在LPIPS相似性上较现有最优方法提升8–11%；已经与开发者协调披露并提出有效防护措施，使基于种子的优化攻击不可行。

Conclusion: 本文揭示并实证了针对扩散模型的提示窃取攻击的严重性，指出现有基于数值优化的提示恢复方法忽视了生成初始随机噪声这一关键因素，从而存在根本性局限。作者发现并利用了一个来自PyTorch对CPU端随机种子限制（2^{32}）导致的噪声生成漏洞（CWE-339），并构建了能在约140分钟内暴力恢复95%图像种子的工具SeedSnitch。基于恢复的种子，提出了基于遗传算法的PromptPirate，在LPIPS相似性上比现有方法提升8–11%。作者同时提供了简单有效的防护措施并已进行负责任披露。

Abstract: Diffusion models have significantly advanced text-to-image generation,
enabling the creation of highly realistic images conditioned on textual prompts
and seeds. Given the considerable intellectual and economic value embedded in
such prompts, prompt theft poses a critical security and privacy concern. In
this paper, we investigate prompt-stealing attacks targeting diffusion models.
We reveal that numerical optimization-based prompt recovery methods are
fundamentally limited as they do not account for the initial random noise used
during image generation. We identify and exploit a noise-generation
vulnerability (CWE-339), prevalent in major image-generation frameworks,
originating from PyTorch's restriction of seed values to a range of $2^{32}$
when generating the initial random noise on CPUs. Through a large-scale
empirical analysis conducted on images shared via the popular platform CivitAI,
we demonstrate that approximately 95% of these images' seed values can be
effectively brute-forced in 140 minutes per seed using our seed-recovery tool,
SeedSnitch. Leveraging the recovered seed, we propose PromptPirate, a genetic
algorithm-based optimization method explicitly designed for prompt stealing.
PromptPirate surpasses state-of-the-art methods, i.e., PromptStealer, P2HP, and
CLIP-Interrogator, achieving an 8-11% improvement in LPIPS similarity.
Furthermore, we introduce straightforward and effective countermeasures that
render seed stealing, and thus optimization-based prompt stealing, ineffective.
We have disclosed our findings responsibly and initiated coordinated mitigation
efforts with the developers to address this critical vulnerability.

</details>


### [103] [What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets](https://arxiv.org/abs/2509.09564)
*Meghan Wilkinson,Robert H Thomson*

Main category: cs.CR

TL;DR: 将数据集中大的benign类用无监督聚类细分，能揭示内部子类并在多数情况下提高多分类入侵检测效果。


<details>
  <summary>Details</summary>
Motivation: 当前入侵检测研究常把所有非攻击流量合并为单一“良性”类，可能忽视其内部异质性，导致监督学习模型性能受限或误判。

Method: 对NSL-KDD、UNSW-NB15、CIC-IDS2017中标注为benign的样本应用多种无监督聚类算法（如HDBSCAN、Mean Shift等），比较聚类数、簇内紧致性、簇间分离度，并在聚类细分后用常见监督学习模型评估多分类性能变化。

Result: 不同聚类算法对良性流量的划分存在差异，但均发现若干可解释的子类（如浏览流量、P2P、扫描背景噪声等）；将这些子类作为标签进行训练，常见分类器的宏平均和微平均指标在多数场景下有所提升。

Conclusion: 数据集中“良性”类内存在多样性，直接当作单一类使用可能掩盖重要结构；通过聚类分解良性流量，可提升多分类检测性能和解释性。

Abstract: Supervised machine learning techniques rely on labeled data to achieve high
task performance, but this requires the labels to capture some meaningful
differences in the underlying data structure. For training network intrusion
detection algorithms, most datasets contain a series of attack classes and a
single large benign class which captures all non-attack network traffic. A
review of intrusion detection papers and guides that explicitly state their
data preprocessing steps identified that the majority took the labeled
categories of the dataset at face value when training their algorithms. The
present paper evaluates the structure of benign traffic in several common
intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and
determines whether there are meaningful sub-categories within this traffic
which may improve overall multi-classification performance using common machine
learning techniques. We present an overview of some unsupervised clustering
techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they
differentially cluster the benign traffic space.

</details>


### [104] [Bridging the Gap in Phishing Detection: A Comprehensive Phishing Dataset Collector](https://arxiv.org/abs/2509.09592)
*Aditya Kulkarni,Shahil Manishbhai Patel,Shivam Pradip Tirmare,Vivek Balachandran,Tamal Das*

Main category: cs.CR

TL;DR: 作者开发了一个基于PhishTank的网页资源采集工具，能抓取URL对应的源码、截图及多类静态资源，并发布了包含约9.7k条（5,666钓鱼、4,056合法）样本的示例数据集，旨在提高钓鱼检测研究的数据多样性与资源完整性。


<details>
  <summary>Details</summary>
Motivation: 现有钓鱼检测依赖机器学习/深度学习，而模型性能高度依赖数据集的多样性与完整性。钓鱼网页短暂存在且资源分散，导致缺乏统一且全面的钓鱼网页资源仓库，进而影响检测模型的泛化能力。

Method: 工具以PhishTank作为主要钓鱼URL来源，针对每个URL抓取网页源码、截图、CSS、JavaScript、favicon、图片等多种资源；并与现有的PyWebCopy库进行比较，强调其抓取资源更全面。作者还利用该工具构建了包含4,056个合法URL和5,666个钓鱼URL的样本数据集。

Result: 工具成功采集到比PyWebCopy更多的网页资源，并生成一个样本数据集；作者还分析了数据集中与类别标签相关性较高的钓鱼特征（未详细列出具体特征名称）。该资源可帮助研究者开发更有效的钓鱼检测方法。

Conclusion: 该论文提出并实现了一个用于收集与URL相关联的网页资源的工具，旨在为反钓鱼研究提供多样且全面的数据集。

Abstract: To combat phishing attacks -- aimed at luring web users to divulge their
sensitive information -- various phishing detection approaches have been
proposed. As attackers focus on devising new tactics to bypass existing
detection solutions, researchers have adapted by integrating machine learning
and deep learning into phishing detection. Phishing dataset collection is vital
to developing effective phishing detection approaches, which highly depend on
the diversity of the gathered datasets. The lack of diversity in the dataset
results in a biased model. Since phishing websites are often short-lived,
collecting them is also a challenge. Consequently, very few phishing webpage
dataset repositories exist to date. No single repository comprehensively
consolidates all phishing elements corresponding to a phishing webpage, namely,
URL, webpage source code, screenshot, and related webpage resources. This paper
introduces a resource collection tool designed to gather various resources
associated with a URL, such as CSS, Javascript, favicons, webpage images, and
screenshots. Our tool leverages PhishTank as the primary source for obtaining
active phishing URLs. Our tool fetches several additional webpage resources
compared to PyWebCopy Python library, which provides webpage content for a
given URL. Additionally, we share a sample dataset generated using our tool
comprising 4,056 legitimate and 5,666 phishing URLs along with their associated
resources. We also remark on the top correlated phishing features with their
associated class label found in our dataset. Our tool offers a comprehensive
resource set that can aid researchers in developing effective phishing
detection approaches.

</details>


### [105] [CryptoGuard: An AI-Based Cryptojacking Detection Dashboard Prototype](https://arxiv.org/abs/2509.09638)
*Amitabh Chakravorty,Jess Kropczynski,Nelly Elsayed*

Main category: cs.CR

TL;DR: CryptoGuard是一个面向非技术用户的AI概念加密钱包安全仪表盘高保真原型，强调用户中心设计与可用性启发式，以便于识别与应对加密劫持，但目前仅为设计概念，缺后端与实证评估。


<details>
  <summary>Details</summary>
Motivation: 随着加密货币普及，加密劫持威胁增加，现有研究多聚焦检测算法而忽视普通用户界面的风险沟通与可操作性。本工作旨在填补检测研究与人机界面设计之间的空隙，提升用户在真实威胁下的快速自信决策能力。

Method: 采用以用户为中心的设计流程，从Figma高保真原型制作可点击模型，模拟关键交互；在设计过程中应用可用性启发式原则来优化信息呈现、告警可见性与决策支持；AI功能为概念性展示，包括可视化异常标记与报告入口。

Result: 构建了一个高保真点击式前端原型，展示了可视化告警、活动监控与内嵌操作路径的设计方案；通过设计示例证明可用性启发式原则能直接增强工具在威胁下的决策支持能力，但未进行实证用户测试或后端检测实现。

Conclusion: 本文提出了CryptoGuard，一种面向普通用户的基于AI概念的加密钱包前端安全仪表盘设计原型，旨在通过可视化告警与操作引导提高用户对加密劫持风险的识别与响应能力。

Abstract: With the widespread adoption of cryptocurrencies, cryptojacking has become a
significant security threat to crypto wallet users. This paper presents a
front-end prototype of an AI-powered security dashboard, namely, CryptoGuard.
Developed through a user-centered design process, the prototype was constructed
as a high-fidelity, click-through model from Figma mockups to simulate key user
interactions. It is designed to assist users in monitoring their login and
transaction activity, identifying any suspicious behavior, and enabling them to
take action directly within the wallet interface. The dashboard is designed for
a general audience, prioritizing an intuitive user experience for non-technical
individuals. Although its AI functionality is conceptual, the prototype
demonstrates features like visual alerts and reporting. This work is positioned
explicitly as a design concept, bridging cryptojacking detection research with
human-centered interface design. This paper also demonstrates how usability
heuristics can directly inform a tool's ability to support rapid and confident
decision-making under real-world threats. This paper argues that practical
security tools require not only robust backend functionality but also a
user-centric design that communicates risk and empowers users to take
meaningful action.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [106] [Fingerprinting Deep Packet Inspection Devices by Their Ambiguities](https://arxiv.org/abs/2509.09081)
*Diwen Xue,Armin Huremagic,Wayne Wang,Ram Sundara Raman,Roya Ensafi*

Main category: cs.NI

TL;DR: dMAP通过差异化模糊测试自动生成探针，远程指纹化DPI设备，实现对全球DPI部署的可区分与聚类，少量探针足以高效识别多种实现。


<details>
  <summary>Details</summary>
Motivation: 随着DPI设备商品化与可获得性增加，审查、限速与拦截等干扰行为在全球扩散，但受限于中间盒的不可直接扫描性和厂商的隐匿策略，当前对DPI部署的理解不足，亟需新的远程识别手段。

Method: 利用差异化模糊测试自动生成并筛选能触发DPI实现差异的专门探针，通过远程探测这些探针并观测外部可见的响应差异来构建指纹；随后使用这些指纹对全球DPI部署进行聚类与识别。

Result: 证明了dMAP的实用性：在实际互联网环境中，20-40个判别性探针即可区分多种DPI实现，并能识别出主要国家级审查基础设施与商业DPI产品；方法可推广到其他有针对性的干扰形式。

Conclusion: 本文提出了dMAP，一种基于差异化模糊测试的远程测量框架，用于构建DPI（深度包检测）设备的行为指纹，从而在大规模互联网上区分与聚类中间盒设备。实验表明少量（20-40）探针即可可靠区分多种DPI实现，包括国家级审查与商业产品。

Abstract: Users around the world face escalating network interference such as
censorship, throttling, and interception, largely driven by the commoditization
and growing availability of Deep Packet Inspection (DPI) devices. Once reserved
for a few well-resourced nation-state actors, the ability to interfere with
traffic at scale is now within reach of nearly any network operator. Despite
this proliferation, our understanding of DPIs and their deployments on the
Internet remains limited -- being network intermediary leaves DPI unresponsive
to conventional host-based scanning tools, and DPI vendors actively obscuring
their products further complicates measurement efforts.
  In this work, we present a remote measurement framework, dMAP (DPI Mapper),
that derives behavioral fingerprints for DPIs to differentiate and cluster
these otherwise indistinguishable middleboxes at scale, as a first step toward
active reconnaissance of DPIs on the Internet. Our key insight is that parsing
and interpreting traffic as network intermediaries inherently involves
ambiguities -- from under-specified protocol behaviors to differing RFC
interpretations -- forcing DPI vendors into independent implementation choices
that create measurable variance among DPIs. Based on differential fuzzing, dMAP
systematically discovers, selects, and deploys specialized probes that
translate DPI internal parsing behaviors into externally observable
fingerprints. Applying dMAP to DPI deployments globally, we demonstrate its
practical feasibility, showing that even a modest set of 20-40 discriminative
probes reliably differentiates a wide range of DPI implementations, including
major nation-state censorship infrastructures and commercial DPI products. We
discuss how our fingerprinting methodology generalizes beyond censorship to
other forms of targeted interference.

</details>


### [107] [AI Reasoning for Wireless Communications and Networking: A Survey and Perspectives](https://arxiv.org/abs/2509.09193)
*Haoxiang Luo,Yu Yan,Yanhui Bian,Wenjiao Feng,Ruichen Zhang,Yinqiu Liu,Jiacheng Wang,Gang Sun,Dusit Niyato,Hongfang Yu,Abbas Jamalipour,Shiwen Mao*

Main category: cs.NI

TL;DR: 综述了将LLM及其他推理方法应用于无线通信的研究现状与展望，强调推理能力、长期规划与跨层自治能显著提升网络优化效果，并提出关键挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在无线网络中常为“黑盒”，缺乏结构化推理与多步决策能力；随着网络复杂性与自治需求增加，需要引入具备推理、规划与工具使用能力的新型AI。

Method: 通过回顾智能无线网络发展、传统AI的局限，介绍AI推理技术（如链式思维、强化学习与LLM代理等），构建一套任务分类体系，并对物理层至应用层逐层分析这些技术的应用与挑战。

Result: 论文总结了多种AI推理方法在各层的潜在改进点与挑战，展示LLM代理在跨层控制与长期优化中的前景，并提出未来研究方向如高效模型、可靠性、安全性与跨层协同。

Conclusion: 本文综述认为将推理能力引入无线网络的AI系统（特别是基于LLM的智能体）可显著提升复杂、多步决策任务的性能，能实现跨层自治控制、长期规划、记忆与工具使用，从而动态优化网络操作并减少人工干预。

Abstract: Artificial Intelligence (AI) techniques play a pivotal role in optimizing
wireless communication networks. However, traditional deep learning approaches
often act as closed boxes, lacking the structured reasoning abilities needed to
tackle complex, multi-step decision problems. This survey provides a
comprehensive review and outlook of reasoning-enabled AI in wireless
communication networks, with a focus on Large Language Models (LLMs) and other
advanced reasoning paradigms. In particular, LLM-based agents can combine
reasoning with long-term planning, memory, tool utilization, and autonomous
cross-layer control to dynamically optimize network operations with minimal
human intervention. We begin by outlining the evolution of intelligent wireless
networking and the limitations of conventional AI methods. We then introduce
emerging AI reasoning techniques. Furthermore, we establish a classification
system applicable to wireless network tasks. We also present a layer-by-layer
examination for AI reasoning, covering the physical, data link, network,
transport, and application layers. For each part, we identify key challenges
and illustrate how AI reasoning methods can improve AI-based wireless
communication performance. Finally, we discuss key research directions for AI
reasoning toward future wireless communication networks. By combining insights
from both communications and AI, this survey aims to chart a path for
integrating reasoning techniques into the next-generation wireless networks.

</details>


### [108] [Joint Optimisation of Load Balancing and Energy Efficiency for O-RAN Deployments](https://arxiv.org/abs/2509.09343)
*Mohammed M. H. Qazzaz,Abdelaziz Salama,Maryam Hafeez,Syed A. R. Zaidi*

Main category: cs.NI

TL;DR: 本文用随机森林构建了一个多类分类器，在O-RAN中预测RU开关策略以兼顾能效与负载均衡，使用多阈值策略满足不同运营需求，并在4.26M条仿真数据上取得98.3% F1-macro的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有动态睡眠方法通常通过迁移UE来实现节能，导致网络负载不均衡并影响被迁移UE的吞吐；因此需要在不改变PRB分配的前提下，同时兼顾能效与负载平衡。

Method: 使用以随机森林为主的ML模型对4.26百万条仿真网络测量数据进行训练，将问题形式化为三类（Well Balanced, Moderately Balanced, Imbalanced）分类，并采用三档阈值策略（Conservative, Moderate, Aggressive）在节能与性能之间权衡。

Result: 在仿真数据上，所提随机森林模型达到98.3%的F1-macro，相较传统基线方法提升约195%。

Conclusion: 提出了一个基于机器学习的框架，在O-RAN环境下联合优化负载均衡和能效，通过多类分类预测RU配置并映射为三类负载平衡标签，从而在保持PRB分配的同时实现节能。

Abstract: Open Radio Access Network (O-RAN) architecture provides an intrinsic
capability to exploit key performance monitoring (KPM) within Radio
Intelligence Controller (RIC) to derive network optimisation through xApps.
These xApps can leverage KPM knowledge to dynamically switch on/off the
associated RUs where such a function is supported over the E2 interface.
Several existing studies employ artificial intelligence (AI)/Machine Learning
(ML) based approaches to realise such dynamic sleeping for increased energy
efficiency (EE). Nevertheless, most of these approaches rely upon offloading
user equipment (UE) to carve out a sleeping opportunity. Such an approach
inherently creates load imbalance across the network. Such load imbalance may
impact the throughput performance of offloaded UEs as they might be allocated a
lower number of physical resource blocks (PRBs). Maintaining the same PRB
allocation while addressing the EE at the network level is a challenging task.
To that end, in this article, we present a comprehensive ML-based framework for
joint optimisation of load balancing and EE for ORAN deployments. We formulate
the problem as a multi-class classification system that predictively evaluates
potential RU configurations before optimising the EE, mapping network
conditions to three load balance categories (Well Balanced, Moderately
Balanced, Imbalanced). Our multi-threshold approach (Conservative, Moderate,
Aggressive) accommodates different operational priorities between energy
savings and performance assurance. Experimental evaluation using 4.26 million
real network measurements from simulations demonstrates that our Random Forest
model achieves 98.3% F1-macro performance, representing 195% improvement over
traditional baseline strategies.

</details>


### [109] [Toward quantum-safe scalable networks: an open, standards-aware key management framework](https://arxiv.org/abs/2509.09453)
*Ane Sanz,Asier Atutxa,David Franco,Jasone Astorga,Eduardo Jacob,Diego López*

Main category: cs.NI

TL;DR: 本文提出基于SDN的QKD网络架构：在节点部署vKMS并引入QuSeC以统一KMS抽象和全局路径计算，提升中继路径发现与网络可扩展性，同时进行了安全分析。


<details>
  <summary>Details</summary>
Motivation: 当前QKD网络在多节点及远距离扩展中受限，特别是中继节点路径选择与KMS识别问题尚无良好解决方案，亟需可扩展且便于部署的体系结构。

Method: 在节点内部抽象出高层vKMS以对用户隐藏底层多个KMS的选择，QuSeC基于全局网络拓扑与状态信息为vKMS计算端到端中继路径并施加安全策略，实现基于策略的路径发现和管理。

Result: 提出架构设计、路径发现机制和安全分析，明确各部分的安全级别与网络安全属性，展示该方案在可扩展性与管理简化方面的优势（未给出具体实验数据）。

Conclusion: 提出了一种基于SDN的QKD网络新架构，通过在每个节点部署虚拟KMS(vKMS)并引入量子安全控制器(QuSeC)，解决了KMS识别、转发路径发现及可扩展性问题。

Abstract: With the advent of quantum computing, the increasing threats to security
poses a great challenge to communication networks. Recent innovations in this
field resulted in promising technologies such as Quantum Key Distribution
(QKD), which enables the generation of unconditionally secure keys,
establishing secure communications between remote nodes. Additionally, QKD
networks enable the interconnection of multinode architectures, extending the
point-to-point nature of QKD. However, due to the limitations of the current
state of technology, the scalability of QKD networks remains a challenge toward
feasible implementations. When it comes to long-distance implementations,
trusted relay nodes partially solve the distance issue through the forwarding
of the distributed keys, allowing applications that do not have a direct QKD
link to securely share key material. Even though the relay procedure itself has
been extensively studied, the establishment of the relaying node path still
lacks a solution. This paper proposes an innovative network architecture that
solves the challenges of Key Management System (KMS) identification, relay path
discovery, and scalability of QKD networks by integrating Software-Defined
Networking (SDN) principles, and establishing high-level virtual KMSs (vKMS) in
each node and creating a new entity called the Quantum Security Controller
(QuSeC). The vKMS serves the end-user key requests, managing the multiple KMSs
within the node and abstracting the user from discovering the correct KMS.
Additionally, based on the high-level view of the network topology and status,
the QuSeC serves the path discovery requests from vKMSs, computing the
end-to-end (E2E) relay path and applying security policies. The paper also
provides a security analysis of the proposal, identifying the security levels
of the architecture and analyzing the core networking security properties.

</details>


### [110] [PARROT: Portable Android Reproducible traffic Observation Tool](https://arxiv.org/abs/2509.09537)
*Andrea Jimenez-Berenguel,Celeste Campo,Marta Moure-Garrido,Carlos Garcia-Rubio,Daniel Díaz-Sanchez,Florina Almenares*

Main category: cs.NI

TL;DR: 提出PARROT系统用于可复现的Android应用流量采集与解密；数据集（2025）显示TLSv1.3、QUIC和DoT显著增长，反映移动应用通信安全协议的演进。


<details>
  <summary>Details</summary>
Motivation: 移动安全协议快速演进与当前公开数据集不足，限制了流量分析研究，需要一个可复现且易于部署的采集系统来生成最新、可解密的应用流量数据集。

Method: 通过使用Android Virtual Devices自动化环境部署、mitmproxy集成与SSL/TLS密钥提取、以及人工交互的标注流程，采集并解密应用流量，支持多种Android版本与拦截模式。

Result: 构建了包含80款应用与对应SSL密钥的数据集，并与2021年MAppGraph数据进行对比，发现TLSv1.3与QUIC广泛普及，DoT取代Do53成为主流DNS传输协议。

Conclusion: PARROT提供了一个可复现、可移植的Android虚拟设备流量采集系统，并通过数据集比较展示了移动应用安全协议的显著演进。

Abstract: The rapid evolution of mobile security protocols and limited availability of
current datasets constrains research in app traffic analysis. This paper
presents PARROT, a reproducible and portable traffic capture system for
systematic app traffic collection using Android Virtual Devices. The system
provides automated environment setup, configurable Android versions, traffic
recording management, and labeled captures extraction with human-in-the-loop
app interaction. PARROT integrates mitmproxy for optional traffic decryption
with automated SSL/TLS key extraction, supporting flexible capture modes with
or without traffic interception. We collected a dataset of 80 apps selected
from the MAppGraph dataset list, providing traffic captures with corresponding
SSL keys for decryption analysis. Our comparative analysis between the
MAppGraph dataset (2021) and our dataset (2025) reveals app traffic pattern
evolution across 50 common apps. Key findings include migration from TLSv1.2 to
TLSv1.3 protocol, with TLSv1.3 comprising 90.0\% of TCP encrypted traffic in
2025 compared to 6.7\% in 2021. QUIC protocol adoption increased substantially,
with all 50 common apps generating QUIC traffic under normal network conditions
compared to 30 apps in 2021. DNS communications evolved from predominantly
unencrypted Do53 protocol (91.0\% in 2021) to encrypted DoT protocol (81.1\% in
2025). The open-source PARROT system enables reproducible app traffic capture
for research community adoption and provides insights into app security
protocol evolution.

</details>
