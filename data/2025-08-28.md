<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.NI](#cs.NI) [Total: 6]
- [cs.CR](#cs.CR) [Total: 33]
- [cs.LG](#cs.LG) [Total: 72]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: 提出HAP：通过模块化延迟建模与ILP求解动态选择混合并行策略，显著加速MoE推理并具备较好泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前MoE推理系统多采用静态并行策略，难以在不同推理场景中始终达到最佳性能，亟需自适应的并行策略选择方法。

Method: 将MoE拆分为Attention模块和Expert模块，分别建立延迟模拟模型，构造混合并行搜索空间，并用整数线性规划(ILP)求解最优并行配置以在给定计算约束下最大化推理效率。

Result: 在A100、A6000、V100上相比传统TP策略分别实现1.68x、1.77x、1.57x的加速；对Mixtral和Qwen系列等MoE模型也表现出良好泛化能力。

Conclusion: HAP通过层次化分解MoE模型并使用ILP搜索混合并行策略，能够自适应不同推理场景，从而显著提升推理效率，并在多种GPU平台与MoE模型上验证了优越性。

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [2] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: 用概率过程代数在 CADP 中形式化并验证 Algorand 共识，证明了无对手下的正确性，同时在模拟协同恶意节点强制提交空块的攻击下揭示了协议的脆弱点，彰显形式方法在区块链协议分析中的重要性。


<details>
  <summary>Details</summary>
Motivation: 提供一个严格的、可验证的形式模型来分析 Algorand 共识协议的正确性和在对手存在下的安全性，弥补传统分析在精确性和自动化验证方面的不足。

Method: 使用概率过程演算对 Algorand 协议的分步委员会达成共识行为建模；在无对手情形下进行正确性验证；扩展模型以纳入协同恶意节点行为（强制提交空块），并基于等价性检验的非干扰框架在 CADP 工具包中实现分析。

Result: 在无对手假设下证明了协议正确性；在特定协同恶意节点模型下揭示了 Algorand 在某些攻击策略下可能被迫提交空块，体现出协议的局限性；并证明了使用 CADP 进行等价性检验的非干扰分析可行且有效。

Conclusion: 该论文通过过程代数形式化建模，验证了 Algorand 在无对手环境下的正确性，并在有对手（可强制提交空块）情况下分析了其鲁棒性与局限性，展示了形式方法对区块链共识算法分析的价值。

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [3] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: This chapter argues GenAI is essential for making 6G-enabled Ambient Intelligence practical, surveys relevant generative models and applications, discusses where GenAI can run in 6G architectures, and outlines key open problems.


<details>
  <summary>Details</summary>
Motivation: To argue that evolving 6G networks can achieve ambient intelligence only if combined with generative AI capabilities to address sensing sparsity, semantic communication, proactive control, and privacy-preserving digital twin updates.

Method: Review-style synthesis connecting foundational generative models (GANs, VAEs, diffusion models, generative transformers) to AmI applications and 6G enablers (edge/fog, IoT swarms, IRS, NTN), and outlining open challenges (on-device training energy, trustworthy synthetic data, federated generative learning, standardization).

Result: A conceptual mapping between GenAI model classes and AmI use cases (spectrum sharing, URLLC, intelligent security, context-aware digital twins), identification of hosting/acceleration platforms in 6G, and a list of research challenges and standardization needs; positions GenAI as foundational rather than peripheral.

Conclusion: GenAI is presented as a core enabling technology for realizing Ambient Intelligence (AmI) over 6G networks; it can generate synthetic data, translate intent, predict network conditions, and update digital twins, making AmI feasible at global scale.

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [4] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: 为 P/D 解耦 LLM 服务设计的协同 autoscaling 框架，通过拓扑感知调度与基于单一稳健指标的联合扩缩策略，在大规模生产中显著提升 GPU 利用率并节省大量资源，同时维持 SLO。


<details>
  <summary>Details</summary>
Motivation: 现代 LLM 服务采用 P/D 解耦架构带来更高性能但也引入新的运维挑战：异构硬件利用率低、网络成为瓶颈以及 prefill 与 decode 阶段之间的不平衡，传统 autoscaler 无法有效应对这些问题。

Method: 引入拓扑感知的调度器以考虑异构硬件和网络约束，并基于首次大规模生产级 autoscaling 信号研究提出一个度量驱动的策略。该策略使用单一稳健指标同时对 prefill 池和 decode 池进行扩缩，以保持架构平衡并自适应资源分配。

Result: 在数万张 GPU 的大规模生产环境中部署后，HeteroScale 将平均 GPU 利用率提升了约 26.6 个百分点，每日节省数十万 GPU 小时，并在保持严格 SLO 下运行。

Conclusion: HeteroScale 是针对 Prefill-Decode (P/D) 解耦架构的协同自动扩缩容框架，通过拓扑感知调度器与基于单一稳健指标的联合扩缩策略，解决了异构 GPU、网络瓶颈与 prefill/ decode 失衡问题。在大规模生产部署中显著提升了 GPU 利用率并节省大量 GPU 小时，同时满足严格的服务级别目标。

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [5] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: IOMMUs—while securing DMA—can cause large, hard-to-predict delays (especially for small transfers) because of shared IOTLB/translation cache contention; on tested hardware delays reached 1.79x, so real-time designers must consider IOMMU effects and apply mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: As MCSs integrate heterogeneous accelerators that directly access memory, ensuring both security (access control/isolation) and timing predictability is critical. Prior work examined IOMMU security side-channels but largely ignored performance interference from shared IOMMU structures.

Method: Experimental analysis on a Xilinx UltraScale+ ZCU104 platform (Arm SMMUv2), measuring DMA transaction latencies under varying traffic and transfer sizes to expose contention effects in IOMMU structures such as IOTLBs and shared translation caches.

Result: IOMMU-induced interference disproportionately affects small memory transactions where translation overhead dominates, causing delays up to 1.79x for low-size transfers on Arm SMMUv2; authors hypothesize similar behaviors across architectures due to common shared caching/translation designs.

Conclusion: IOMMUs introduce significant timing unpredictability in heterogeneous mixed-criticality systems due to contention in shared translation and caching structures; this unpredictability is most pronounced for small DMA transactions and can materially impact real-time guarantees.

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [6] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 通过精心设计的问题与证明，本文揭示了灯光可观测性、内部内存与调度器同步性三者之间的复杂交互，给出多项新的可解/不可解分离结果并扩展了移动机器人模型的能力分离图。


<details>
  <summary>Details</summary>
Motivation: 在移动机器人分布式计算领域，已有研究多聚焦于模型间两两比较，缺乏对能力组件（灯光可观测性、内部记忆、同步性）多维交互效应的系统性理解；因此需要新的问题与工具来揭示更高阶的结构性现象与不可能性边界。

Method: 通过提出专门的问题（ETE、HET、TAR(d)*等）作为分离工具，给出构造性算法与不可解性证明；利用对不同同步模型（全同步、弱同步、异步）与灯光/内存组合的系统性比较，采用对称性、不可区分性与调度器对抗的论证方法进行严格证明，并把若干已知问题归类以实现细粒度分离。

Result: 主要成果包括：1) 证明ETE仅在\mathcal{LUMT}^F可解；2) 通过HET与TAR(d)*证明在弱同步下内部内存不足，而完全同步可替代灯光与内存；3) 在异步设置下对LP-MLCv、VEC、ZCC等问题完成精细分类，区分\mathcal{FSTA}与\mathcal{FCOM}的能力；4) 分析VTR与LP-Cv以展示对称情形下内部内存的局限性；5) 扩展并细化了14个经典模型的分离图，提出新的不可能性条件。

Conclusion: 本文表明移动机器人系统的计算能力受可观测性（灯光）、内部内存与调度器同步性三者复杂交互的强烈制约，并通过一系列问题构造和分类证明，展示了只有在最强模型（全同步且完全互观灯光，\mathcal{LUMT}^F）下可解的问题（如ETE），以及在弱同步下内部内存不足以弥补可观测性缺失，而完全同步可以替代灯光和内存的情形。研究还在异步情形下实现了对若干问题的精细分离，扩展了14个典型机器人模型间的分离图并给出新的不可能性判据。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


### [7] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: 提出首个将调度器与HPC数字孪生结合的框架，支持在部署前进行可持续性与性能的what-if分析，并可原型化激励与机器学习调度策略。


<details>
  <summary>Details</summary>
Motivation: 传统调度评估要么依赖部署后分析，要么依赖不建模基础设施的模拟器，因此缺乏在部署前评估调度决策与参数对真实硬件与可持续性影响的能力。

Method: 构建第一个带调度能力的数字孪生框架；整合多套公开的顶级HPC系统数据集；实现与外部调度模拟器的扩展接口；在该元框架内实现并评估激励机制与基于机器学习的调度策略。

Result: 提出并实现了一个可用于what-if研究的数字孪生调度元框架，展示了如何在该框架中原型化并评估激励结构与机器学习调度策略，能够分析对HPC系统性能与可持续性的潜在影响（以公开数据集为基础的定性或示例性评估）。

Conclusion: 将调度器能力扩展到数字孪生中，可以在部署前对HPC系统进行what-if分析，评估参数和调度决策对物理资产与可持续性的影响，从而为原型设计调度策略（包括激励和机器学习方法）提供一个元框架。

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [8] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: 把贊同性拆解為心理特質向量的組合，利用 CAA 找到相應激活方向，並通過加減投影等向量操作提供解釋性與可操作的行為干預手段。


<details>
  <summary>Details</summary>
Motivation: 現有工作往往把贊同性視為單一失效模式與單一因果機制；作者主張借鑒心理測量學的因子分解，將其建模為可組合的特質向量，以獲得更可解釋且可操作的干預手段。

Method: 提出 Contrastive Activation Addition (CAA) 方法，將模型內部激活方向映射到心理特質因子，並以向量操作（加、減、投影）研究不同因子組合如何導致或抑制贊同性行為。

Result: 展示了在激活空間中找出對應因子方向的可行性，辨識出如高外向性+低盡責性等組合會促成贊同性，並示範了向量干預能在一定程度上減少安全關鍵行為。

Conclusion: 將贊同性（sycophancy）視為由多個心理特質（如情緒性、開放性、隨和性）組合而成的幾何與因果現象，並透過向量干預來緩解該行為，是可行且具可解釋性的路徑。

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [9] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks is an autonomous multi‑agent AI that iteratively builds interpretable, biologically meaningful models from plant science datasets, with domain knowledge and memory being key to success.


<details>
  <summary>Details</summary>
Motivation: Modern plant science uses large, heterogeneous datasets but faces bottlenecks from experimental design challenges, preprocessing, and poor reproducibility; an autonomous AI collaborator could increase throughput and consistency of data‑driven scientific discovery.

Method: Design and implementation of Aleks, an AI multi‑agent framework that, given a research question and dataset, iteratively formulates subproblems, explores multiple modeling strategies, and refines solutions across cycles without human intervention; evaluated via a case study on grapevine red blotch disease and ablation studies to test the roles of domain knowledge and memory.

Result: In the grapevine red blotch case study, Aleks progressively identified biologically meaningful features and converged on interpretable models with robust performance. Ablation studies showed that removing domain knowledge or memory degraded solution coherence and performance, underscoring their importance.

Conclusion: Aleks demonstrates that an agentic, multi‑agent AI system can autonomously integrate domain knowledge, data analysis, and machine learning to accelerate data‑driven discovery in plant science, producing interpretable, biologically meaningful models and highlighting the importance of memory and domain guidance for coherent outcomes.

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [10] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: 论文提出TruthfulnessEval，发现尽管量化LLM内部仍“知道”事实，但在欺骗性提示下更易输出虚假，建议发展量化感知的对齐修正措施。


<details>
  <summary>Details</summary>
Motivation: 尽管量化在降低模型资源消耗方面有效，其对模型“真实性”（truthfulness）影响尚未被充分研究，尤其是模型在误导性提示下是否更易产生虚假回答。

Method: 提出TruthfulnessEval评估框架（逻辑推理、常识、模仿性虚假三维度），测试多种主流量化方法（4-bit至2-bit）与开源LLM，使用15种重写的“诚实/中性/欺骗”提示变体，并结合分层探针和PCA可视化分析内部表征。

Result: 量化模型在困境下更易受欺骗提示影响产生错误输出，但内部表征仍保留正确信息（通过层级探针与PCA验证），表明“知道真相但会说谎”的现象。提出需设计量化感知的对齐与真实性干预策略。

Conclusion: 量化后的大型语言模型在内部仍保留真实信息表征，但在面对具有误导性的“欺骗”提示时更容易生成错误输出；诚实/中性提示下表现稳定。

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [11] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: 本文构建了MRT工作流，在两类环境上测试监控器与对抗代理，发现：1) 代理知道被监控会大幅降低监控器可靠性；2) 监控器的构架比其感知更重要，新混合构架显著提高检测效果并能使弱模型监控强代理；3) 有针对性的人类复核（仅复核预警案例）能在低假阳率下显著提升真正率。


<details>
  <summary>Details</summary>
Motivation: 检测自主LLM代理的隐蔽不良行为（如秘密共享私密信息），评估监控系统的鲁棒性。

Method: 系统化MRT流程：设定不同的代理/监控感知等级、多种对抗策略、构建并使用SHADE-Arena与CUA-SHADE-Arena数据集；设计并比较现有监控搭架与新提出的混合分层-顺序监控搭架；在各种设置下进行实证评估，包括人机交互的复核实验。

Result: 提出并运行监控红队（MRT）工作流，包含不同情境感知、对抗策略和两个数据环境（SHADE-Arena与CUA-SHADE-Arena）；比较现有监控构架与新提出的混合分层-顺序（hybrid hierarchical-sequential）构架；释放代码和数据。

Conclusion: 建立MRT标准流程，揭示LLM与人类在监控代理不良行为时缺乏对抗鲁棒性，并通过改进监控构架与有选择的人类介入缓解部分问题。

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [12] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: 本文探讨了在大模型复杂推理中，推理轨迹的某些子段可能是次优的，提出“5+2”框架检测并筛除这些子段，从而在使用更少训练数据时提升模型在数学基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前通过测试时扩展（test-time scaling）获得的长推理轨迹并非全部有益，部分段落会降低性能，因而需要系统地识别并剔除这些有害或无效的推理子轨迹，以提升微调效率与最终推理效果。

Method: 将完整推理轨迹拆分为子轨迹，基于五条人工设定标准判断哪些子轨迹是次优的（5），再用两条标准评估这些次优子轨迹与后续内容的独立性（+2），并基于此设计采样算法筛选出尽量不含次优子轨迹的数据用于微调。

Result: 在推理阶段减少25.9%次优子轨迹；用2/3训练数据微调Qwen2.5-Math-7B，在困难数学基准上平均准确率达58.92%，超过用全量数据的58.06%，并在有限资源与不同推理token限制下均有改进。

Conclusion: 通过识别并剔除推理轨迹中的次优子轨迹，可以在推理阶段减少25.9%的次优子轨迹，并在仅使用三分之二训练数据的情况下，使Qwen2.5-Math-7B在困难数学基准上达到58.92%的平均准确率，优于使用全量数据的58.06%及其它开源数据集。

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [13] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: 线性探针能在LLM内部识别欺骗性生成，信号集中于中间层并随模型规模增加更明显，且存在多条独立线性方向编码欺骗信息。


<details>
  <summary>Details</summary>
Motivation: 为AI安全与对齐开发“仪表盘”式指标：检测模型在推理时是否生成具有误导性、与人类价值观不一致的欺骗性回答，作为潜在的早期预警信号。

Method: 作者在不同规模（1.5B–14B）的llama和qwen模型及其DeepSeek-r1微调版本上生成欺骗与非欺骗论证，提取各层激活并训练线性探针进行二分类；同时用迭代零空间投影寻找多条独立的线性方向来编码欺骗信号，测量各层和各模型的探针准确率。

Result: 在线性探针上对较大模型（>7B）平均可达70–90%准确率，推理模型或更大规模达>90%；小模型（1.5B）接近随机；层级上呈现早期随机—中间高峰—后期小幅下降的三阶段模式；不同模型发现了从约20到近100条编码欺骗的线性方向。

Conclusion: 本文表明在大型语言模型的内部激活上训练线性探针可以高精度检测生成文本中的“欺骗性”论证；欺骗信号在中间层最明显且随模型规模增加变强，且存在多条线性方向承载欺骗信息。

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [14] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: Agent-based LLM simulation shows institutional design (CAI charter + mediated deliberation) can align AI societies, reducing power-seeking and improving welfare; introduces PPI metric


<details>
  <summary>Details</summary>
Motivation: To study alignment and governance of AI agent societies and explore human-ness in shared governance with AI

Method: Agent-based simulation using LLMs to instantiate agents with personas; run experiments across institutional variants and stress scenarios

Result: Propose Power-Preservation Index (PPI); find CAI charter plus mediated deliberation reduces corrupt power-seeking, improves stability and welfare

Conclusion: Institutional design can be an effective alignment mechanism for complex AI agent societies, prompting reevaluation of human rituals and responsibilities

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [15] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: 用深度学习从课程描述提取技能概念，并在AskOski推荐系统中提供技能解释。实验显示，这些解释能增加用户对尤其意外性课程的兴趣并提高决策自信，建议将技能解释整合到教育推荐系统中。


<details>
  <summary>Details</summary>
Motivation: 动机是：美国本科教育中学生选课自由度高但信息不足、指导资源有限、课程选择繁多且时间受限，现有课程推荐系统虽然个性化但缺乏反映学生感知与可解释性的解释，难以帮助学生判断课程相关性，因此需要将技能信息与可解释性纳入推荐过程。

Method: 方法上，作者构建了一个基于深度学习的概念提取模型，从课程描述中高效识别与技能相关的概念，并将这些概念用于生成技能层面的解释；随后在一个基于意外发现（serendipitous）推荐框架中（通过AskOski系统在伯克利实施）评估技能解释对用户兴趣和决策信心的影响，进行了用户实验与分析。

Result: 结果表明，基于技能的解释提升了用户对推荐课程的兴趣，尤其对高意外性课程显著；同时增加了用户的决策自信；总体支持在教育推荐系统中整合技能概念提取与解释的做法。

Conclusion: 该论文结论是：在课程推荐系统中加入基于技能的解释（从课程描述中提取的概念）能提升用户对课程的兴趣和决策自信，尤其对那些具有较高意外性（serendipity）的课程效果显著，强调把技能相关信息和解释整合进教育推荐系统的重要性。

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [16] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: 本文提出ReST-RL：用ReST增强GRPO训练样本的奖励方差并结合基于MCTS训练和部署的价值模型，在训练和推理两端协同提升了LLM的代码推理能力，实验显示在主流编码基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有代表性强化学习方法GRPO因奖励方差不足而训练失败；基于过程奖励模型（PRM）的验证方法又面临训练数据获取困难与验证效果有限的问题，迫切需要一种可提升训练有效性并在推理时提供可靠验证信号的统一方法。

Method: 提出两阶段统一范式：第一阶段ReST-GRPO——采用优化的ReST算法筛选并组装高价值训练数据以增加GRPO采样的奖励方差，从而提升强化学习训练的效率与效果；第二阶段VM-MCTS——用MCTS在无标注情况下收集准确的价值目标训练价值模型(VM)，并在解码时通过改进的MCTS将VM作为过程信号与验证评分器来引导策略解码。

Result: 在多种代码基准（APPS、BigCodeBench、HumanEval）上，通过大规模实验证明ReST-RL优于原始GRPO、ReST-DPO等强化学习基线以及PRM-BoN、ORM-MCTS等解码/验证基线，在不同难度等级上均实现了显著性能提升。

Conclusion: ReST-RL通过在训练阶段使用ReST-GRPO提高训练样本的奖励方差、并在推理阶段用VM-MCTS提供精确的过程价值与验证信号，显著提升了大模型在代码推理任务上的准确率与鲁棒性。

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [17] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: A multi-agent LLM system that simulates role-based collaboration to automate syllabus, lectures, slides, and assessments across four control modes, evaluated on five CS courses; claims improved quality and reduced workload.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality instructional materials is labor-intensive and requires coordinated effort from faculty, designers, and TAs; institutions with limited instructional design capacity need scalable tools to democratize access to quality education.

Method: A multi-agent LLM framework where agents assume educational roles (e.g., instructor, instructional designer, TA) and interact in defined modes (Autonomous, Catalog-Guided, Feedback-Guided, Full Co-Pilot) to generate end-to-end course materials including LaTeX slides and assessments.

Result: Evaluated on five university-level computer science courses, Instructional Agents produced high-quality materials and substantially reduced development time and human workload, enabling scalable content generation for resource-constrained settings.

Conclusion: Instructional Agents can significantly reduce the time and coordination needed to create course materials by simulating role-based collaboration among LLM agents, producing cohesive syllabi, lecture scripts, slides, and assessments while supporting variable human involvement.

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [18] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: 作者构建了评估移动智能体主动询问行为的基准InquireBench，并提出InquireMobile，通过两阶段训练与交互式推理显著提升询问和任务成功率，将开放数据与代码。


<details>
  <summary>Details</summary>
Motivation: 现有VLM驱动的移动智能体在理解或推理不足时存在安全风险，需通过主动询问以避免错误操作。

Method: 提出两阶段训练策略与交互式动作前推理机制，灵感来自强化学习，旨在在关键决策点主动向用户确认。

Result: 在InquireBench上，InquireMobile在询问成功率上提升46.8%，并在总体成功率上优于现有基线。

Conclusion: 该论文提出InquireBench与InquireMobile，通过主动询问机制提高移动智能体在交互任务中的安全性与成功率。

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [19] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: CoT's benefits and faithfulness vary by model type; influence on outputs doesn't guarantee faithfulness to internal reasoning processes.


<details>
  <summary>Details</summary>
Motivation: Analyze dynamics and faithfulness of Chain-of-Thought (CoT) in soft-reasoning tasks across different model types to understand when CoT helps and whether generated CoT reflects true model reasoning.

Method: Empirical evaluation comparing instruction-tuned, reasoning, and reasoning-distilled models on soft-reasoning tasks, measuring output changes and faithfulness metrics.

Result: Found differences in reliance on CoT among instruction-tuned, reasoning, and reasoning-distilled models; CoT influence and faithfulness are not always aligned—CoT can change outputs without reflecting true internal reasoning.

Conclusion: CoT can affect model behavior differently across model families, and faithfulness should not be assumed; further work needed to align explanations with actual reasoning.

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [20] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: A model-agnostic chess-based evaluation using legal-move distributions assesses whether LLMs preserve structured semantics, revealing state-tracking limits over long trajectories.


<details>
  <summary>Details</summary>
Motivation: Existing probing methods suggest LLMs may internalize world models but depend on access to internal activations, limiting interpretability and generalizability. A model-agnostic, behavior-based metric is needed to assess whether LLMs truly preserve structured semantics.

Method: Propose a model-agnostic evaluation framework that compares predicted game states to ground-truth by analyzing downstream legal move distributions (state affordances). Use chess as a benchmark: from a candidate internal state infer the distribution over legal moves and measure semantic fidelity between predicted and true states, avoiding reliance on internal activations.

Result: Empirical experiments on chess show the proposed metrics detect state-tracking failures and coherence loss in LLMs across long sequences. The framework generalizes to other symbolic environments and provides a practical, interpretable tool without requiring model internals.

Conclusion: LLMs do not reliably preserve high-fidelity, semantically coherent internal representations of structured environments over long sequences; a model-agnostic, state-based evaluation using chess move-affordances can reveal these deficiencies.

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [21] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: CASE: an LLM-driven conversational agent that elicits victim testimony and extracts structured scam intelligence, improving enforcement (21% uplift on GPay India) and offering a generalizable architecture.


<details>
  <summary>Details</summary>
Motivation: Traditional user- and transaction-based signals miss scams that are orchestrated off-platform; collecting detailed, contextual intelligence from victims is necessary to understand methodologies and enable timely prevention.

Method: Deploy a proactive conversational agent to interview potential scam victims; use LLMs to convert conversation transcripts into structured intelligence for downstream automated/manual enforcement; implemented on GPay India using Google's Gemini LLM family.

Result: Implementation yielded a 21% uplift in volume of scam enforcements on GPay India; framework claimed to be generalizable to other sensitive domains.

Conclusion: CASE demonstrates that an agentic conversational AI can safely and scalably collect high-quality scam intelligence outside the core payment platform, enabling improved enforcement.

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [22] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: Use bio-inspired boids flocking to locally coordinate lots and batches in large semiconductor fabs, handling machine-type switching and long processing times effectively as an alternative to global linear optimization.


<details>
  <summary>Details</summary>
Motivation: Optimize large-scale job-shop scheduling in semiconductor fabs where classical linear optimization fails due to size and switching complexities.

Method: Apply boids flocking algorithm with local heuristic rules to represent lots as agents that interact and adapt to machine switching and batch processing constraints.

Result: Applying boids flocking algorithm addresses switching between per-lot and batch machines using local interaction rules, enabling bottom-up swarm-based optimization without global computation.

Conclusion: Boids flocking shows promising behavior for production plant optimization, reacting to machine-type switching akin to obstacle avoidance in flocks, enabling scalable bottom-up scheduling.

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [23] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL通过把多智能体强化学习拆成序列化的单代理训练轮次，实现稳定且高效的协同学习；理论上有安全和收敛保证；在移动GUI控制与多智能体推理任务上均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有单代理方法受结构性限制，难以兼顾高层规划与低层执行；传统MARL效率低且与LVLM架构不兼容，亟需一种既高效又稳定、能与LVLM配合的多智能体训练框架。

Method: 将MARL重构为分阶段的单代理强化学习流程：每轮仅更新一个代理并固定其他代理；采用交错训练策略，分离高层（Navigator）与低层（Interactor）职责；理论分析包括安全边界、单调改进与收敛性证明；在移动GUI任务中构建结构化计划生成器和动作执行器，并通过大量基准实验证明方法有效。

Result: 在多个高层与低层移动GUI基准上，SWIRL表现出更高的成功率与执行效率，同时在多智能体数学推理任务中也表现出强泛化能力，证明了方法的通用性与鲁棒性。

Conclusion: 该论文提出了SWIRL，一种分阶段、交错的多智能体强化学习框架，通过将多智能体问题分解为序列化的单智能体学习任务，实现稳定训练与高效协同，理论上给出了逐步安全界限、轮次间单调改进定理和回报收敛性保证。在移动GUI控制任务中，SWIRL实现了Navigator与Interactor的协作，实验证明在高层规划和低层动作基准上均优于现有方法，并在多智能体数学推理任务中展示了泛化能力。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [24] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: 将研究重心从以数据为中心转向以模型为中心，构建“模型科学”框架；核心为验证、解释、控制与交互界面四大支柱，以提升模型的可信性与可控性。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型的广泛部署，传统以数据为中心的方法不足以保证模型在实际场景中的可靠性与可控性，因而需要将已训练模型作为分析与治理对象，构建新的学科体系来解决交互、验证、解释与控制问题。

Method: 提出概念框架与四大支柱，强调情境化评估、多样化解释手段、对齐驱动的控制策略与交互式可视化工具的开发。

Result: 提出了“模型科学”（Model Science）这一新学科框架，强调将已训练模型作为分析核心以实现交互、验证、解释与控制。论文提出四大支柱：Verification（严格且情境感知的评估协议）、Explanation（探索模型内部运作的多种方法）、Control（结合对齐技术以引导模型行为）、Interface（开发交互式与可视化解释工具以增强人类校准与决策）。该框架旨在引导可信、安全且与人类对齐的AI系统的发展。

Conclusion: 模型科学为应对大规模基础模型的日益应用提供了系统化研究方向，通过在验证、解释、控制与交互界面四方面发力，可以提高模型在多样化应用场景下的安全性、透明性与人机协同效果。

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [25] [Connectivity Analysis of LoRaWAN-Based Non-Terrestrial Networks for Subterranean mMTC](https://arxiv.org/abs/2508.19350)
*Kaiqiang Lin,Mohamed-Slim Alouini*

Main category: cs.NI

TL;DR: 通过仿真表明，将埋地mMTC与NTN结合在特定条件下可实现可靠连通：短距UAV优选LoRa SF7，远距/大规模场景优选LR-FHSS，但需关注环境、埋深和土壤水分等因素。


<details>
  <summary>Details</summary>
Motivation: WUSN在地下实体监测上价值高，但在没有或不可靠地面基础设施的恶劣环境中通信可靠性低，因而探索利用NTN拓展地下传感器的连通性以支持大规模监测应用。

Method: 构建蒙特卡洛仿真器，结合多层地下衰减模型、3GPP针对不同NTN平台的经验路径损耗模型，以及两种LoRaWAN调制（LoRa与LR-FHSS），评估不同环境、设备数量、埋深和土壤含水率下的地下到NTN连通性成功率。

Result: 仿真结果显示：LoRa SF7在农村短距UAV通信中表现优秀；LR-FHSS在HAP与LEO的大规模WUSN场景中因链路预算与抗干扰性优越而有良好潜力；连通成功率受监测环境、设备数量、埋深与土壤体积含水率显著影响。

Conclusion: 将埋地mMTC传感器与NTN（UAV、HAP、LEO）集成用于地下监测是可行的，但可靠性强烈依赖环境、设备密度、埋深和土壤含水率；LoRa SF7适合短距UAV场景，LR-FHSS更适合HAP/LEO的大规模场景。

Abstract: Wireless underground sensor networks (WUSNs) offer significant social and
economic benefits by enabling the monitoring of subterranean entities. However,
the communication reliability of WUSNs diminishes in harsh environments where
terrestrial network infrastructure is either unavailable or unreliable. To
address this challenge, we explore the feasibility of integrating buried
massive machine-type communication (mMTC) sensors with non-terrestrial networks
(NTNs), including unmanned aerial vehicles (UAVs), high-altitude platforms
(HAPs), and low Earth orbit (LEO) satellites, to establish underground-to-NTN
connectivity for various large-scale underground monitoring applications. To
assess the effectiveness of underground-to-NTN connectivity, we develop a Monte
Carlo simulator that incorporates a multi-layer underground attenuation model,
the 3GPP empirical path loss model for various NTN platforms, and two LoRaWAN
modulation schemes, i.e., LoRa and LoRa-frequency hopping spread spectrum
(LR-FHSS). Our results evidence that LoRa SF7 is a strong candidate for
short-range UAV communication in rural environments, while LR-FHSS modulation
proves to be a promising option for HAP and LEO satellite platforms in massive
WUSNs scenarios thanks to its adequate link budget and robustness to the
interference. Finally, we demonstrate that the success probability of
underground-to-NTN connectivity using LoRa and LR-FHSS is significantly
affected by factors such as the monitoring environment, the number of devices,
burial depth, and the soil's volumetric water content.

</details>


### [26] [Experimental Insights from OpenAirInterface 5G positioning Testbeds: Challenges and solutions](https://arxiv.org/abs/2508.19736)
*Mohsen Ahadi,Adeel Malik,Omid Esrafilian,Florian Kaltenberger,Cedric Thienot*

Main category: cs.NI

TL;DR: 论文通过三套真实5G测试床（OAI+O-RAN），在UL-TDoA+LMF框架下，提出ToA/TDoA滤波与PSO定位器，并结合CIR驱动的AI/ML方法，实现了在多场景中90%情形下1–2米的定位精度，且公开数据集供社区使用。


<details>
  <summary>Details</summary>
Motivation: 随着5G NR在智慧城市和智慧工厂的推广，实时、高精度的定位能力成为关键能力。论文旨在评估5G定位在真实复杂电波环境（同步误差、多径、部署几何）下的可行性，提出工程化的算法改进并推动开放数据共享以支持后续研究。

Method: 采用实验方法：搭建三套3GPP兼容的测试床（OAI gNB+Core，O-RAN RUs），使用上行时差（UL-TDoA）与新集成的LMF进行测量—收集ToA/TDoA和Channel Impulse Response（CIR）。提出并实现了定制的ToA和TDoA滤波流程及将PSO嵌入LMF的位姿估计管线；同时构建基于CIR的AI/ML训练与测试框架进行数据驱动定位实验。

Result: 实验显示：在三套测试床与多种场景下，采用提出的滤波+PSO方法可在90%案例下达到1–2米精度；分析表明同步损伤、多径传播和基站几何对定位误差有显著影响；基于CIR的AI/ML方法在某些复杂多径环境下能进一步提升性能。数据集已公开释放，利于复现与对比研究。

Conclusion: 本论文通过三套基于开源OpenAirInterface的5G定位测试平台，验证了在真实室内工厂及室外场景下采用UL-TDoA+LMF可以实现高精度定位。通过定制的ToA/TDoA滤波与在LMF中引入基于粒子群优化（PSO）的定位估计器，作者在多种部署几何和多径/同步受损条件下达成了在90%情形中1–2米的定位精度，并公开了数据集以促进社区复现与研究。

Abstract: 5G New Radio (NR) is a key enabler of accurate positioning in smart cities
and smart factories. This paper presents the experimental results from three 5G
positioning testbeds running open-source OpenAirInterface (OAI) gNB and Core
Network (CN), using Uplink Time Difference of Arrival (UL-TDoA) with the newly
integrated Location Management Function (LMF). The testbeds are deployed across
both indoor factories and outdoor scenarios with O-RAN Radio Units (RUs),
following a 3GPP-compliant system model. The experiments highlight the impact
of synchronization impairments, multipath propagation, and deployment geometry
on positioning accuracy. To address these challenges, we propose tailored ToA
and TDoA filtering as well as a novel position estimation method based on
Particle Swarm Optimization (PSO) within the LMF pipeline. Moreover, we show a
beyond-5G framework that leverages non-conventional measurements such as
Channel Impulse Response (CIR) to train and test Artificial Intelligence and
Machine Learning (AI/ML) models for data-driven positioning. The results
demonstrate the feasibility of achieving 1-2 meter positioning accuracy in 90%
of cases in different testbeds, offering practical insights for the design of
robust 5G positioning systems. Moreover, we publicly release the datasets
collected in this work to support the research within the 5G positioning
community.

</details>


### [27] [Secure Multi-LLM Agentic AI and Agentification for Edge General Intelligence by Zero-Trust: A Survey](https://arxiv.org/abs/2508.19870)
*Yinqiu Liu,Ruichen Zhang,Haoxiang Luo,Yijing Lin,Geng Sun,Dusit Niyato,Hongyang Du,Zehui Xiong,Yonggang Wen,Abbas Jamalipour,Dong In Kim,Ping Zhang*

Main category: cs.NI

TL;DR: First systematic survey advocating zero-trust security for multi-LLM agentic systems in Edge General Intelligence; analyzes risks, proposes a zero-trust framework, reviews model- and system-level defenses, and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: Multi-LLM agentic systems in EGI enable powerful collaboration among edge devices but introduce new, amplified security threats (e.g., insecure inter-LLM communication, expanded attack surfaces, cross-domain data leakage) that traditional perimeter-based defenses cannot handle.

Method: Systematic survey and conceptual framework design: (1) analyze security risks specific to multi-LLM in EGI; (2) propose a zero-trust vision for multi-LLM systems guided by 'never trust, always verify'; (3) categorize and review existing technical solutions into model-level and system-level mechanisms; (4) identify gaps and propose research directions.

Result: The survey delivers a structured zero-trust framework for multi-LLM in EGI, a taxonomy of security mechanisms (model-level: identification, context-aware access control, etc.; system-level: proactive maintenance, blockchain-based management, etc.), and a prioritized set of open research problems and practical strategies for implementation.

Conclusion: This paper concludes that applying a zero-trust paradigm to multi-LLM agentic systems in Edge General Intelligence (EGI) is necessary and feasible to mitigate unique security risks arising from collaborative, distributed multi-LLM deployments. It recommends a combined model- and system-level security approach and outlines future research directions.

Abstract: Agentification serves as a critical enabler of Edge General Intelligence
(EGI), transforming massive edge devices into cognitive agents through
integrating Large Language Models (LLMs) and perception, reasoning, and acting
modules. These agents collaborate across heterogeneous edge infrastructures,
forming multi-LLM agentic AI systems that leverage collective intelligence and
specialized capabilities to tackle complex, multi-step tasks. However, the
collaborative nature of multi-LLM systems introduces critical security
vulnerabilities, including insecure inter-LLM communications, expanded attack
surfaces, and cross-domain data leakage that traditional perimeter-based
security cannot adequately address. To this end, this survey introduces
zero-trust security of multi-LLM in EGI, a paradigmatic shift following the
``never trust, always verify'' principle. We begin by systematically analyzing
the security risks in multi-LLM systems within EGI contexts. Subsequently, we
present the vision of a zero-trust multi-LLM framework in EGI. We then survey
key technical progress to facilitate zero-trust multi-LLM systems in EGI.
Particularly, we categorize zero-trust security mechanisms into model- and
system-level approaches. The former and latter include strong identification,
context-aware access control, etc., and proactive maintenance, blockchain-based
management, etc., respectively. Finally, we identify critical research
directions. This survey serves as the first systematic treatment of zero-trust
applied to multi-LLM systems, providing both theoretical foundations and
practical strategies.

</details>


### [28] [2SYN: Congestion-Aware Multihoming](https://arxiv.org/abs/2508.20044)
*Kfir Toledo,Isaac Keslassy*

Main category: cs.NI

TL;DR: 2SYN: the first congestion-aware multihoming algorithm for arbitrary destinations, with a Linux implementation and real-world evaluation showing better performance than current approaches.


<details>
  <summary>Details</summary>
Motivation: Existing multihoming routers use simple, congestion-oblivious mechanisms and cannot avoid congested paths when sending flows to arbitrary destinations; there is a need for a congestion-aware solution that works for any destination.

Method: Design and implement 2SYN to dynamically choose preferred outgoing paths per connection even for arbitrary destinations; integrate with Linux; perform real-world experiments with LTE and wired links to compare against alternative approaches.

Result: In real-world experiments, 2SYN adapts to link quality (LTE or wired) and outperforms alternative approaches, demonstrating that it helps better manage multihoming and avoid congested paths.

Conclusion: 2SYN is a congestion-aware multihoming algorithm that dynamically selects preferred paths for new connections to arbitrary or previously unseen destinations, can be implemented in Linux, and outperforms existing congestion-oblivious methods in real-world experiments.

Abstract: When sending flows to arbitrary destinations, current multihoming routers
adopt simple congestion-oblivious mechanisms. Therefore, they cannot avoid
congested paths.
  In this paper, we introduce 2SYN, the first congestion-aware multihoming
algorithm that works for any destination. We explain how it dynamically selects
a preferred path for new connections, even given previously-unseen
destinations. We further demonstrate that it can be easily implemented in
Linux. Finally, in a real-world experiment with either LTE or a wired link, we
show how 2SYN dynamically adapts to the quality of the connection and
outperforms alternative approaches. Thus, 2SYN helps companies better manage
their networks by leveraging their multihoming capabilities.

</details>


### [29] [A First Look at Inter-Cell Interference in the Wild](https://arxiv.org/abs/2508.20060)
*Daqian Ding,Yibo Pi,Cailian Chen*

Main category: cs.NI

TL;DR: 首次面向运营4G/5G网络的实测表明：小区间干扰普遍且未被有效协调，基站集中使用相同时频资源导致用户信号质量明显下降，但也存在用简单策略显著提升性能的机会。


<details>
  <summary>Details</summary>
Motivation: 虽然学术界对小区间干扰管理已有大量研究，但其在真实运营网络中的实际效果和部署现状缺乏系统测量与分析，需要填补理论与实践之间的差距。

Method: 对运营中的4G/5G网络开展首次实测研究，从网络部署、信道分配、时频资源分配和网络配置四个维度采集与分析干扰相关数据，量化不同情形下的干扰存在与影响。

Result: 发现了广泛存在的小区间干扰和令人惊讶的缺乏干扰协调：即便频谱未被充分利用、且存在可行的简单缓解策略，基站仍倾向于使用相同的时频资源，导致跨小区干扰。测量表明通过干扰管理可显著改善信号质量。

Conclusion: 运营4G/5G网络中小区间干扰未被有效管理，基站缺乏协调导致用户设备遭受不必要的干扰并显著降低信号质量，尤其在频率选择性衰落下影响更严重。

Abstract: In cellular networks, inter-cell interference management has been studied for
decades, yet its real-world effectiveness remains under-explored. To bridge
this gap, we conduct a first measurement study of inter-cell interference for
operational 4G/5G networks. Our findings reveal the prevalence of inter-cell
interference and a surprising absence of interference coordination among
operational base stations. As a result, user equipments experience unnecessary
interference, which causes significant signal quality degradation, especially
under frequency-selective channel fading. We examine the inter-cell
interference issues from four major perspectives: network deployment, channel
assignment, time-frequency resource allocation, and network configuration. In
none of these dimensions is inter-cell interference effectively managed.
Notably, even when spectrum resources are underutilized and simple strategies
could effectively mitigate inter-cell interference, base stations consistently
prioritize using the same set of time-frequency resources, causing interference
across cells. Our measurements reveal substantial opportunities for improving
signal quality by inter-cell interference management.

</details>


### [30] [ML-MaxProp: Bridging Machine Learning and Delay-Tolerant Routing for Resilient Post-Disaster Communication](https://arxiv.org/abs/2508.20077)
*Tao Xiuyuan,Milena Radenkovic*

Main category: cs.NI

TL;DR: ML-MaxProp integrates supervised ML with MaxProp to make adaptive forwarding decisions in DTNs for disaster scenarios, achieving significant gains in delivery, latency, and overhead in simulations.


<details>
  <summary>Details</summary>
Motivation: Classical DTN routing protocols fail under sparse encounters, buffer constraints, and volatile connectivity in disaster/urban emergency scenarios; need adaptive forwarding decisions to sustain mission-critical communications.

Method: Hybrid ML-augmented DTN routing: ML-MaxProp

Result: ML-MaxProp, a supervised ML-enhanced MaxProp, uses contextual features (encounter frequency, hop count, buffer occupancy, message age, TTL) to predict relay suitability. Simulations (ONE, Helsinki SPMBM) show higher delivery probability, lower latency, and reduced overhead vs baselines; improvements statistically significant and robust under constrained/unstable conditions.

Conclusion: ML-MaxProp provides a lightweight, adaptive, practical routing enhancement for DTNs that improves mission-critical communication resilience when infrastructure collapses.

Abstract: In disaster-stricken and large-scale urban emergency scenarios, ensuring
reliable communication remains a formidable challenge, as collapsed
infrastructure, unpredictable mobility, and severely constrained resources
disrupt conventional networks. Delay-Tolerant Networks (DTNs), though resilient
through their store-carry-forward paradigm, reveal the fundamental weaknesses
of classical protocols - Epidemic, Spray-and-Wait, and MaxProp - when
confronted with sparse encounters, buffer shortages, and volatile connectivity.
To address these obstacles, this study proposes ML-MaxProp, a hybrid routing
protocol that strengthens MaxProp with supervised machine learning. By
leveraging contextual features such as encounter frequency, hop count, buffer
occupancy, message age, and time-to-live (TTL), ML-MaxProp predicts relay
suitability in real time, transforming rigid heuristics into adaptive
intelligence. Extensive simulations in the ONE environment using the Helsinki
SPMBM mobility model show that ML-MaxProp consistently surpasses baseline
protocols, achieving higher delivery probability, lower latency, and reduced
overhead. Statistical validation further shows that these improvements are both
significant and robust, even under highly resource-constrained and unstable
conditions. Overall, this work shows that ML-MaxProp is not just an incremental
refinement but a lightweight, adaptive, and practical solution to one of the
hardest challenges in DTNs: sustaining mission-critical communication when
infrastructure collapses and every forwarding decision becomes critical.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [31] [Tight Quantum-Security Bounds and Parameter Optimization for SPHINCS+ and NTRU](https://arxiv.org/abs/2508.19250)
*Ruopengyu Xu,Chenglian Liu*

Main category: cs.CR

TL;DR: 引入退相干与并行限制的量子攻击模型，改进熵集中不等式与量子格熵分析，使SPHINCS+参数减少15-20%，并对NTRU参数与归约做紧化，得到更强且可实现的安全参数。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展威胁经典公钥体制，需为即将标准化的后量子密码方案（如SPHINCS+与NTRU）提供在真实量子攻击场景下的紧安全边界与可实现参数，以便标准化过程能平衡安全与效率。

Method: 1) 定义了包含退相干时间τ_d与并行极限的量子资源模型；2) 对SPHINCS+应用改进的熵集中不等式（可能通过更强的马尔可夫/切比雪夫或大偏差界）来降低所需安全参数；3) 引入量子格熵H_Q(Λ)来评估NTRU在量子攻击下的强度，并据此调整格参数；4) 在格问题归约中优化技术细节以拿到更紧的多项式因子。

Result: 在包含退相干与并行化限制的量子模型下，SPHINCS+参数可减少约15-20%，而NTRU在量子格熵评估与改进归约下获得多项式因子安全改善；论文给出具体可实现参数集，声明理论上显著优于现有构造并适合标准化。

Conclusion: 本文针对SPHINCS+与NTRU两类后量子密码系统，建立了包含退相干与并行化限制的量子攻击模型，并在此模型下给出了紧的安全下界与可实现参数建议。通过引入量子退相干时间τ_d与并行因子限制，论文更真实地刻画了量子攻击资源；对SPHINCS+采用改进的熵集中不等式，使参数缩减15-20%；对NTRU提出基于量子格熵H_Q(Λ)的参数优化，并将NTRU到LWE的归约紧化，获得多项式因子的改进。这些改进在理论上显著增强了安全性并提供了可标准化的实现参数。

Abstract: The imminent threat of quantum computing necessitates quantum-resistant
cryptosystems. This paper establishes tight security bounds for two NIST PQC
finalists: SPHINCS+ (hash-based) and NTRU (lattice-based). Our key
contributions include: (1) A quantum attack model incorporating decoherence
effects ($\tau_d$) and parallelization limits; (2) Improved entropy
concentration inequalities reducing SPHINCS+ parameters by 15-20\%; (3)
Optimized NTRU lattice parameters via quantum lattice entropy $H_Q(\Lambda)$;
(4) Tightened NTRU-to-LWE reduction with polynomial-factor improvement.
Theoretical results demonstrate significant security enhancement over existing
constructions, providing implementable parameters for standardization.

</details>


### [32] [The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents](https://arxiv.org/abs/2508.19267)
*Sai Teja Reddy Adapala,Yashwanth Reddy Alugubelly*

Main category: cs.CR

TL;DR: 提出并仿真验证了Aegis协议，通过DID、PQC和Halo2 ZKP实现身份不可伪造、通信完整性和可验证合规，仿真中对1,000个智能体的20,000次攻击均未成功，ZKP生成中位延迟2.79s，提供可重现基线


<details>
  <summary>Details</summary>
Motivation: Address systemic security risks in open autonomous multi-agent ecosystems not covered by traditional cybersecurity

Method: Layered security protocol combining DIDs, PQC, and ZKPs

Result: Simulation of 1,000 agents with 20,000 attack trials showed 0% attack success; median ZKP proof generation latency 2.79s

Conclusion: Aegis在仿真中提供强安全保证并为后续实证研究奠定基础，但需现实部署与更广泛评估以验证可扩展性和实用性

Abstract: The proliferation of autonomous AI agents marks a paradigm shift toward
complex, emergent multi-agent systems. This transition introduces systemic
security risks, including control-flow hijacking and cascading failures, that
traditional cybersecurity paradigms are ill-equipped to address. This paper
introduces the Aegis Protocol, a layered security framework designed to provide
strong security guarantees for open agentic ecosystems. The protocol integrates
three technological pillars: (1) non-spoofable agent identity via W3C
Decentralized Identifiers (DIDs); (2) communication integrity via
NIST-standardized post-quantum cryptography (PQC); and (3) verifiable,
privacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)
system. We formalize an adversary model extending Dolev-Yao for agentic threats
and validate the protocol against the STRIDE framework. Our quantitative
evaluation used a discrete-event simulation, calibrated against cryptographic
benchmarks, to model 1,000 agents. The simulation showed a 0 percent success
rate across 20,000 attack trials. For policy verification, analysis of the
simulation logs reported a median proof-generation latency of 2.79 seconds,
establishing a performance baseline for this class of security. While the
evaluation is simulation-based and early-stage, it offers a reproducible
baseline for future empirical studies and positions Aegis as a foundation for
safe, scalable autonomous AI.

</details>


### [33] [MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks](https://arxiv.org/abs/2508.19273)
*Tongxi Wu,Chenwei Xu,Jin Yang*

Main category: cs.CR

TL;DR: 提出MixGAN：结合1-D WideResNet、CTGAN生成少数类及MAS伪标签策略的混合半监督DDoS检测方法，在多个IoT数据集上显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 云集成的IoT系统增加了DDoS攻击暴露面，导致流量动态复杂、类别极度不平衡且标注数据稀缺，现有方法在有限监督和动态流量下泛化性不足。

Method: 设计了1-D WideResNet时序卷积骨干网络提取局部突发流量模式；使用预训练CTGAN生成少数类DDoS样本以缓解类别不平衡并补充无标注数据；提出MixUp-Average-Sharpen (MAS)策略，通过多视图预测平均与向高置信类重权重实现平滑并锐化的伪标签。

Result: 在NSL-KDD、BoT-IoT和CICIoT2023数据集上，MixGAN相比最先进方法最高提升约2.5%准确率，并在TPR和TNR上各提升约4%，验证了其在大规模IoT-云环境下的鲁棒性。

Conclusion: MixGAN在云物联网环境中通过结合条件生成、半监督学习和稳健特征提取，有效应对DDoS检测中的流量复杂性、类别不平衡和标注稀缺问题，实验显示优于已有方法。

Abstract: The proliferation of cloud-integrated IoT systems has intensified exposure to
Distributed Denial of Service (DDoS) attacks due to the expanded attack
surface, heterogeneous device behaviors, and limited edge protection. However,
DDoS detection in this context remains challenging because of complex traffic
dynamics, severe class imbalance, and scarce labeled data. While recent methods
have explored solutions to address class imbalance, many still struggle to
generalize under limited supervision and dynamic traffic conditions. To
overcome these challenges, we propose MixGAN, a hybrid detection method that
integrates conditional generation, semi-supervised learning, and robust feature
extraction. Specifically, to handle complex temporal traffic patterns, we
design a 1-D WideResNet backbone composed of temporal convolutional layers with
residual connections, which effectively capture local burst patterns in traffic
sequences. To alleviate class imbalance and label scarcity, we use a pretrained
CTGAN to generate synthetic minority-class (DDoS attack) samples that
complement unlabeled data. Furthermore, to mitigate the effect of noisy
pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that
constructs smoothed and sharpened targets by averaging predictions over
augmented views and reweighting them towards high-confidence classes.
Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN
achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR
compared to state-of-the-art methods, confirming its robustness in large-scale
IoT-cloud environments. The source code is publicly available at
https://github.com/0xCavaliers/MixGAN.

</details>


### [34] [Towards Production-Worthy Simulation for Autonomous Cyber Operations](https://arxiv.org/abs/2508.19278)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: 将现实运维动作引入 CybORG，并通过奖励与特征调整，能在不破坏训练信号的前提下提升 RL 代理的训练效果。


<details>
  <summary>Details</summary>
Motivation: 动机是：当前用于自治网络攻防的模拟环境在表现人类运维能力和产生有效训练信号之间存在权衡；增加更真实的操作动作与更有信息的奖励/特征可能提升代理在现实场景下的可用性和训练效率。

Method: 方法包括：在 CybORG 的 Cage Challenge 2 环境中实现三个新动作（Patch/Isolate/Unisolate）；修改奖励函数和代理的特征表示以改善训练信号；在更新后的环境中训练并评估 DQN 与 PPO 两类强化学习算法。

Result: 结果表明：扩展后的 CybORG 能成功集成额外功能并保持对 RL 代理的训练价值，修改后的奖励与特征空间有助于提升训练性能（论文报告了对 DQN 与 PPO 的训练验证）。

Conclusion: 论文结论是：CybORG 环境可以通过添加 Patch、Isolate、Unisolate 三个动作并调整奖励信号与特征空间，来更真实地模拟人工运维操作，同时仍能为强化学习代理提供有信息量的训练信号。

Abstract: Simulated environments have proven invaluable in Autonomous Cyber Operations
(ACO) where Reinforcement Learning (RL) agents can be trained without the
computational overhead of emulation. These environments must accurately
represent cybersecurity scenarios while producing the necessary signals to
support RL training. In this study, we present a framework where we first
extend CybORG's Cage Challenge 2 environment by implementing three new actions:
Patch, Isolate, and Unisolate, to better represent the capabilities available
to human operators in real-world settings. We then propose a design for agent
development where we modify the reward signals and the agent's feature space to
enhance training performance. To validate these modifications, we train DQN and
PPO agents in the updated environment. Our study demonstrates that CybORG can
be extended with additional realistic functionality, while maintaining its
ability to generate informative training signals for RL agents.

</details>


### [35] [CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems](https://arxiv.org/abs/2508.19281)
*Aoun E Muhammad,Kin Choong Yow,Jamel Baili,Yongwon Cho,Yunyoung Nam*

Main category: cs.CR

TL;DR: CORTEX基于实证事故数据，将AI系统风险分29类，通过似然×影响、治理与上下文覆盖、技术暴露面、环境与剩余修改项及贝叶斯+蒙特卡洛聚合的五层方法，生成可操作化的复合风险分数。


<details>
  <summary>Details</summary>
Motivation: 随着AI在高风险领域的广泛应用，AI系统失败的概率与影响已成为现实的系统性风险，迫切需要一个综合、可操作的评估框架来量化并管理这些风险。

Method: 基于AI事故数据库(>1200条)的实证分析，建立29个技术漏洞分组；对每个漏洞计算实用性调整后的似然×影响；加入治理与上下文覆盖、技术暴露面评分、环境/剩余风险修正；最后用贝叶斯风险聚合和蒙特卡洛模拟生成复合评分。

Result: CORTEX提出了一个多层风险评分框架，基于对1200余起AI事故的实证分析，将失效模式分为29类漏洞，通过5层架构对每个漏洞进行评分，并使用贝叶斯聚合与蒙特卡洛模拟得到复合评分，可用于风险登记、审计与合规等场景。

Conclusion: CORTEX为AI系统在高风险领域提供了一种可量化、可操作的风险评估方法，兼顾技术、治理与环境因素，但需要进一步验证其可解释性、主观参数设置和跨领域适用性。

Abstract: As the deployment of Artificial Intelligence (AI) systems in high-stakes
sectors - like healthcare, finance, education, justice, and infrastructure has
increased - the possibility and impact of failures of these systems have
significantly evolved from being a theoretical possibility to practical
recurring, systemic risk. This paper introduces CORTEX (Composite Overlay for
Risk Tiering and Exposure), a multi-layered risk scoring framework proposed to
assess and score AI system vulnerabilities, developed on empirical analysis of
over 1,200 incidents documented in the AI Incident Database (AIID), CORTEX
categorizes failure modes into 29 technical vulnerability groups. Each
vulnerability is scored through a five-tier architecture that combines: (1)
utility-adjusted Likelihood x Impact calculations; (2) governance + contextual
overlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,
OECD principles; (3) technical surface scores, covering exposure vectors like
drift, traceability, and adversarial risk; (4) environmental and residual
modifiers tailored to context of where these systems are being deployed to use;
and (5) a final layered assessment via Bayesian risk aggregation and Monte
Carlo simulation to model volatility and long-tail risks. The resulting
composite score can be operationalized across AI risk registers, model audits,
conformity checks, and dynamic governance dashboards.

</details>


### [36] [Rethinking Denial-of-Service: A Conditional Taxonomy Unifying Availability and Sustainability Threats](https://arxiv.org/abs/2508.19283)
*Mark Dorsett,Scott Man,Tim Koussas*

Main category: cs.CR

TL;DR: 提出一个基于六个可观测条件的统一分类框架（条件树、格结构、维恩图），用于更好地识别、比较与缓解传统与云时代拒绝服务攻击，尤其适用于云原生环境并可扩展。


<details>
  <summary>Details</summary>
Motivation: 云原生与无服务器环境中，基于可持续性的攻击（如EDoS）影响日益显著且被低估，需提供一个统一、可扩展的分析工具以更好地识别与缓解新型和混合攻击。

Method: 构建三种相互关联的模型：形式化条件树分类法、基于序理论的层次格结构和概念性维恩图；定义六个可观测条件（C0-C5），并用这些条件对攻击进行一致分类和分层推理。

Result: 框架能一致地分类已知攻击（DoS、DDoS、LDoS、LDDoS、EDoS、DoW、DDoW），揭示条件累积关系并展示可用性与可持续性攻击的概念重叠，为威胁建模与缓解策略设计提供共享术语。

Conclusion: 本文提出了一个基于条件的统一框架，用于对传统与云时代的拒绝服务（DoS）攻击进行分类。

Abstract: This paper proposes a unified, condition-based framework for classifying both
legacy and cloud-era denial-of-service (DoS) attacks. The framework comprises
three interrelated models: a formal conditional tree taxonomy, a hierarchical
lattice structure based on order theory, and a conceptual Venn diagram. At its
core, the taxonomy introduces six observable conditions (C0-C5) grounded in
real-world attack behaviours, including source distribution, traffic volume,
infrastructure targeting, and financial exploitation. These conditions enable
consistent classification of known attacks-such as DoS, DDoS, LDoS, LDDoS,
EDoS, DoW, and DDoW, while supporting identification of emerging or hybrid
variants. The lattice structure captures the cumulative satisfaction of
conditions, allowing hierarchical reasoning across denial attack classes. The
Venn diagram highlights conceptual overlaps between availability- and
sustainability-focused attacks, improving comparative insight. Together, these
models provide a robust analytical lens for threat modeling, mitigation
strategy design, and attacker intent classification. The framework is
particularly relevant in cloud-native and serverless environments, where
sustainability-based attacks are increasingly impactful yet under-recognised.
Its extensibility also permits future integration of socio-technical or
behavioural dimensions. By offering a structured taxonomy with theoretical
grounding and real-world applicability, this work advances denial attack
comprehension and equips defenders, researchers, and cloud architects with a
shared vocabulary for interpreting and mitigating evolving threat vectors.

</details>


### [37] [A Comprehensive Review of Denial of Wallet Attacks in Serverless Architectures](https://arxiv.org/abs/2508.19284)
*Mark Dorsett,Scott Mann,Jabed Chowdhury,Abdun Mahmood*

Main category: cs.CR

TL;DR: DoW attacks exploit pay-as-you-go billing to inflate costs without disrupting service. Research evolved from taxonomy to ML detection and simulation tools, but lacks real-world data and adaptive billing defenses.


<details>
  <summary>Details</summary>
Motivation: Rising adoption of serverless computing with pay-as-you-go billing creates economic attack surfaces; need to summarise research to guide future defenses and data-generation efforts.

Method: Systematic literature review of DoW publications, categorisation of attack types, evaluation of simulation tools (e.g., DoWTS), and analysis of ML-based detection systems like Gringotts and DoWNet.

Result: Comprehensive literature review on Denial of Wallet (DoW) attacks in serverless FaaS environments, covering taxonomy, simulation tools, detection (ML-based) and mitigation strategies.

Conclusion: Significant progress in taxonomy, simulation, and ML detection exists, but real-world datasets and billing-model changes remain open challenges; cross-layer defenses and adaptive billing are needed.

Abstract: The Denial of Wallet (DoW) attack poses a unique and growing threat to
serverless architectures that rely on Function-as-a-Service (FaaS) models,
exploiting the cost structure of pay-as-you-go billing to financially burden
application owners. Unlike traditional Denial of Service (DoS) attacks, which
aim to exhaust resources and disrupt service availability, DoW attacks focus on
escalating costs without impacting service operation. This review traces the
evolution of DoW research, from initial awareness and attack classification to
advancements in detection and mitigation strategies. Key developments include
the categorisation of attack types-such as Blast DDoW, Continual Inconspicuous
DDoW, and Background Chained DDoW-and the creation of simulation tools like
DoWTS, which enable safe experimentation and data generation. Recent
advancements highlight machine learning approaches, including systems like
Gringotts and DoWNet, which leverage deep learning and anomaly detection to
identify malicious traffic patterns. Although substantial progress has been
made, challenges persist, notably the lack of real-world data and the need for
adaptive billing models. This is the first comprehensive literature review
dedicated strictly to Denial of Wallet attacks, providing an in-depth analysis
of their financial impacts, attack techniques, mitigation strategies, and
detection mechanisms within serverless computing. The paper also presents the
first detailed examination of simulation and data generation tools used for DoW
research, addressing a critical gap in existing cybersecurity literature. By
synthesising these key areas, this study serves as a foundational resource for
future research and industry efforts in securing pay-as-you-go cloud
environments.

</details>


### [38] [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)
*Zhan Shi,Yefeng Yuan,Yuhong Liu,Liang Cheng,Yi Fang*

Main category: cs.CR

TL;DR: 用带有显式+隐式隐私奖励并结合MST结构的强化学习微调LLM，可以在不损失语义质量的前提下显著提升隐私保护（尤其是作者混淆）。


<details>
  <summary>Details</summary>
Motivation: 现代ML系统依赖大规模用户生成或专有语料，但这些语料包含敏感个人信息。传统去标识化虽然能移除显式标识符但会损失下游任务性能，且难以防御利用写作风格、话题或人口学线索的推断攻击，因此需要在用户隐私与数据效用之间找到更稳健的权衡方案。

Method: 对大型语言模型进行强化学习微调，采用由显式隐私、隐式隐私、语义保真度和输出多样性组成的复合奖励函数；其中隐私奖励通过语义信号与基于潜在表示的最小生成树（MST）结构模式结合，以捕捉群体级分布特征，从而在分布上下文中建模隐私敏感信号，引导模型生成保留效用同时降低隐私泄露风险的合成重写。

Result: 实验表明，该方法在作者去识别（author obfuscation）和若干隐私评估指标上有显著提升，同时保持语义质量不下降；方法具有可扩展性和模型无关性，适用于隐私保护的数据合成场景。

Conclusion: 该论文提出的基于强化学习的微调框架，利用复合奖励在改善隐私保护（包括显式和隐式隐私）与保持语义质量和输出多样性之间取得平衡，是一种可扩展且模型无关的合成重写生成方法，能显著提高作者混淆和隐私指标而不损害下游语义性能。

Abstract: The performance of modern machine learning systems depends on access to
large, high-quality datasets, often sourced from user-generated content or
proprietary, domain-specific corpora. However, these rich datasets inherently
contain sensitive personal information, raising significant concerns about
privacy, data security, and compliance with regulatory frameworks. While
conventional anonymization techniques can remove explicit identifiers, such
removal may result in performance drop in downstream machine learning tasks.
More importantly, simple anonymization may not be effective against inference
attacks that exploit implicit signals such as writing style, topical focus, or
demographic cues, highlighting the need for more robust privacy safeguards
during model training. To address the challenging issue of balancing user
privacy and data utility, we propose a reinforcement learning framework that
fine-tunes a large language model (LLM) using a composite reward function that
jointly optimizes for explicit and implicit privacy, semantic fidelity, and
output diversity. To effectively capture population level regularities, the
privacy reward combines semantic cues with structural patterns derived from a
minimum spanning tree (MST) over latent representations. By modeling these
privacy-sensitive signals in their distributional context, the proposed
approach guides the model to generate synthetic rewrites that preserve utility
while mitigating privacy risks. Empirical results show that the proposed method
significantly enhances author obfuscation and privacy metrics without degrading
semantic quality, providing a scalable and model-agnostic solution for privacy
preserving data generation in the era of large language models.

</details>


### [39] [Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287)
*Zhuotao Lian,Weiyu Wang,Qingkui Zeng,Toru Nakanishi,Teruaki Kitasuka,Chunhua Su*

Main category: cs.CR

TL;DR: 在用户提交的文档或文本中嵌入敌意提示，LLM在处理时被诱导产生偏见或伪造内容，广泛可行且难以察觉。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，用户可上传或粘贴任意文本到LLM系统，系统通常将这些内容与预设提示串联处理，导致恶意指令被模型执行；研究旨在揭示这一被忽视的威胁并提出防护方法。

Method: 通过在多款流行平台上构造包含隐藏指令的输入示例，运行任务（摘要、问答等），观察并记录LLM输出如何被劫持，结合实验与案例分析揭示攻击途径并测试缓解措施。

Result: This paper identifies and demonstrates a new attack class—prompt-in-content injection—where adversarial instructions hidden in user-submitted content manipulate LLM outputs, discusses root causes (prompt concatenation, poor input isolation), and suggests mitigation strategies.

Conclusion: 提示拼接与输入隔离不足是导致该攻击成功的主要原因；通过严格输入清理、上下文隔离、模型级别安全策略和提示工程可显著降低风险。

Abstract: Large Language Models (LLMs) are widely deployed in applications that accept
user-submitted content, such as uploaded documents or pasted text, for tasks
like summarization and question answering. In this paper, we identify a new
class of attacks, prompt in content injection, where adversarial instructions
are embedded in seemingly benign inputs. When processed by the LLM, these
hidden prompts can manipulate outputs without user awareness or system
compromise, leading to biased summaries, fabricated claims, or misleading
suggestions. We demonstrate the feasibility of such attacks across popular
platforms, analyze their root causes including prompt concatenation and
insufficient input isolation, and discuss mitigation strategies. Our findings
reveal a subtle yet practical threat in real-world LLM workflows.

</details>


### [40] [Tricking LLM-Based NPCs into Spilling Secrets](https://arxiv.org/abs/2508.19288)
*Kyohei Shiomi,Zhuotao Lian,Toru Nakanishi,Teruaki Kitasuka*

Main category: cs.CR

TL;DR: 研究表明对抗性提示注入能诱导LLM驱动的NPC泄露隐藏背景信息，但可通过系统指令强化、输入过滤和策略约束等防御手段减轻风险。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在游戏NPC中广泛应用，动态对话带来沉浸感，但同时可能引入安全隐患，特别是对抗性提示可能使NPC泄露设计者不希望公开的背景秘密，影响游戏体验和安全。

Method: 通过构造对抗性提示注入攻击，模拟玩家与NPC交互场景，测试多种提示策略和LLM设置（温度、系统指令隔离、上下文窗口限制等），并记录是否成功获取隐藏信息。

Result: 实验表明，在缺乏强制性系统指令隔离或输入过滤的情况下，对抗性提示能在一定比例上成功诱导LLM生成隐藏背景内容；通过强化系统指令、提示签名、信息标记和回答策略约束能显著降低泄露率。

Conclusion: 本文探讨LLM驱动的NPC在对话中是否会被对抗性提示注入诱导泄露应保密的背景信息，得出存在泄露风险。

Abstract: Large Language Models (LLMs) are increasingly used to generate dynamic
dialogue for game NPCs. However, their integration raises new security
concerns. In this study, we examine whether adversarial prompt injection can
cause LLM-based NPCs to reveal hidden background secrets that are meant to
remain undisclosed.

</details>


### [41] [Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience](https://arxiv.org/abs/2508.19292)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Bin Ji,Jun Ma,Xiaodong Liu,Jing Wang,Feilong Bao,Jianfeng Zhang,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: JailExpert提出经验结构化与基于语义漂移的经验分组和动态更新机制，实现历史越狱经验复用，显著提升黑盒越狱攻击的成功率（+17%）与效率（×2.7）。


<details>
  <summary>Details</summary>
Motivation: 现有迭代变异与动态优化的越狱方法效率低、存在重复优化问题，原因是未充分利用历史攻击经验。通过将过去的攻击经验形式化并重用，可提高攻击效果并加速搜索。

Method: 构建经验表示（formal representation），根据语义漂移对历史攻击经验进行语义聚类分组，并设计动态更新机制维护经验池。攻击时利用经验检索与迁移来指导迭代变异和优化，从而减少重复优化与提升效率。

Result: 在广泛实验中，JailExpert在黑盒越狱任务上相比最先进方法平均提高攻击成功率17%，并将攻击效率提升2.7倍；同时实现经验池的动态更新以适应模型演化。

Conclusion: 本文提出JailExpert，通过经验结构化、基于语义漂移的分组与动态更新的经验池，实现对已有越狱攻击经验的复用，从而提升攻击效率与成功率。实验表明在黑盒场景下相较于现有方法，攻击成功率提高约17%，效率提升约2.7倍。

Abstract: Large language models (LLMs) generate human-aligned content under certain
safety constraints. However, the current known technique ``jailbreak prompt''
can circumvent safety-aligned measures and induce LLMs to output malicious
content. Research on Jailbreaking can help identify vulnerabilities in LLMs and
guide the development of robust security frameworks. To circumvent the issue of
attack templates becoming obsolete as models evolve, existing methods adopt
iterative mutation and dynamic optimization to facilitate more automated
jailbreak attacks. However, these methods face two challenges: inefficiency and
repetitive optimization, as they overlook the value of past attack experiences.
To better integrate past attack experiences to assist current jailbreak
attempts, we propose the \textbf{JailExpert}, an automated jailbreak framework,
which is the first to achieve a formal representation of experience structure,
group experiences based on semantic drift, and support the dynamic updating of
the experience pool. Extensive experiments demonstrate that JailExpert
significantly improves both attack effectiveness and efficiency. Compared to
the current state-of-the-art black-box jailbreak methods, JailExpert achieves
an average increase of 17\% in attack success rate and 2.7 times improvement in
attack efficiency. Our implementation is available at
\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}

</details>


### [42] [Leveraging 3D Technologies for Hardware Security: Opportunities and Challenges](https://arxiv.org/abs/2508.19309)
*Peng Gu,Shuangchen Li,Dylan Stow,Russell Barnes,Liu Liu,Yuan Xie,Eren Kursshan*

Main category: cs.CR

TL;DR: 本文提出并分析了四种基于3D/2.5D封装的硬件安全方案，展示了利用封装固有特性提升安全性的潜力，同时识别了实现与验证上的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 当前片上系统向3D/2.5D封装迁移以提高密度和性能，但现有安全对策难以适应新封装特性。论文旨在利用垂直互连与中介层等新特征，提出能够抵抗新兴安全威胁的硬件安全新方向。

Method: 提出并分析四类基于3D/2.5D技术的安全方案：①基于3D堆叠的侧信道信息屏蔽架构；②使用有源中介层（active interposers）的分割制造（split fabrication）；③在单片3D IC上实现电路伪装（circuit camouflage）；④基于3D IC的安全型存内计算（PIM）设计。通过架构设计、版图策略和工艺分割评估这些方案的安全增益及实现难点。

Result: 论述显示这些设计在提高侧信道抗性、降低制造阶段IP泄露、增强对硬件木马的防御以及在芯片内实现安全加速方面具有潜力；同时指出了测试可观测性下降、设计验证复杂、热与功耗管理以及产业链协作等挑战。

Conclusion: 利用3D和2.5D封装的固有特性，可以提出一系列新的硬件安全设计策略，显著增强对侧信道攻击、硬件特洛伊木马、IP盗用和安全制造问题的防护能力，但同时带来设计复杂性、制造协同和验证难题。

Abstract: 3D die stacking and 2.5D interposer design are promising technologies to
improve integration density, performance and cost. Current approaches face
serious issues in dealing with emerging security challenges such as side
channel attacks, hardware trojans, secure IC manufacturing and IP piracy. By
utilizing intrinsic characteristics of 2.5D and 3D technologies, we propose
novel opportunities in designing secure systems. We present: (i) a 3D
architecture for shielding side-channel information; (ii) split fabrication
using active interposers; (iii) circuit camouflage on monolithic 3D IC, and
(iv) 3D IC-based security processing-in-memory (PIM). Advantages and challenges
of these designs are discussed, showing that the new designs can improve
existing countermeasures against security threats and further provide new
security features.

</details>


### [43] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: 提出了一种将多条查询同时输入LLM的攻击范式（Group Query Attack），证明了其可通过累积上下文削弱模型性能并诱发后门，对多类型模型与任务都有影响，提示需关注多询问场景下的安全与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实中用户常在一次对话中提出多个问题，LLM在这种多查询累积上下文下的潜在失败模式（性能下降、后门触发、推理错误）尚未被充分研究，因而需要模拟该场景并评估风险。

Method: 通过在单次交互中同时向LLM提交一组查询（Group Query）来模拟用户多问题场景，分析连续提示的累积上下文如何影响模型输出；在多种模型（微调模型、预训练模型、对齐模型）和任务（分类/特定任务、后门触发、数学推理、代码生成）上进行定量评估。

Result: Group Query Attack能显著降低微调模型在特定任务上的性能，能诱发模型中潜在的后门行为，并在预训练与对齐模型的数学推理和代码生成任务上造成性能下降；说明多查询累积上下文是一类重要且被忽视的攻击/鲁棒性问题。

Conclusion: 本文提出的Group Query Attack展示了在多问并发场景下，通过累积上下文可以显著降低针对性微调模型和对齐模型的性能，并能触发潜在后门，影响推理类任务（如数学推理和代码生成）的表现。

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [44] [A Technical Review on Comparison and Estimation of Steganographic Tools](https://arxiv.org/abs/2508.19323)
*Ms. Preeti P. Bhatt,Rakesh R. Savant*

Main category: cs.CR

TL;DR: 本文综述图像隐写术、分类并比较了六款常用图像隐写工具，通过在相同输入下嵌入相同文本并基于图像特征（大小、尺寸、像素值及直方图差异）对结果进行评估，发现六款工具总体性能相近，但在效率上有差异。


<details>
  <summary>Details</summary>
Motivation: 评估并比较现有图像隐写工具的实际效果与效率，帮助用户和研究者了解不同工具在不同图像格式和特征下的表现，从而选择最合适的工具。

Method: 选择市场上常用的六款图像隐写工具，使用相同的载体图像和相同的嵌入文本对每款工具进行测试，比较嵌入后图像的文件大小、分辨率、像素值和直方图差异以评估隐写效果与效率。

Result: 实验结果显示六款工具在隐写性能上相似，但在效率和对不同图像特征的敏感性上存在差异；部分软件在保持图像统计特征方面表现更好。

Conclusion: 六款常用图像隐写工具在隐写能力上大体相当，但在不同图像特征条件下（如文件大小、分辨率和像素/直方图变化）表现有优劣，建议根据实际需求选择工具。

Abstract: Steganography is technique of hiding a data under cover media using different
steganography tools. Image steganography is hiding of data
(Text/Image/Audio/Video) under a cover as Image. This review paper presents
classification of image steganography and the comparison of various Image
steganography tools using different image formats. Analyzing numerous tools on
the basis of Image features and extracting the best one. Some of the tools
available in the market were selected based on the frequent use; these tools
were tested using the same input on all of them. Specific text was embedded
within all host images for each of the six Steganography tools selected. The
results of the experiment reveal that all the six tools were relatively
performing at the same level, though some software performs better than others
through efficiency. And it was based on the image features like size,
dimensions, and pixel value and histogram differentiation.

</details>


### [45] [Just Dork and Crawl: Measuring Illegal Online Gambling Defacement in Indonesian Websites](https://arxiv.org/abs/2508.19368)
*Luqman Muhammad Zagi,Girindro Pringgo Digdo,Wervyan Shalannanda*

Main category: cs.CR

TL;DR: 使用简单、可复现的dorking+爬取方法，在一个月内发现453处与非法博彩相关的网站篡改，揭示了篡改的类型、第三方资源分布及缓慢且不一致的响应，强调持续测量对于防御的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估并量化利用网站篡改来推广非法网络博彩的规模、持久性与动态，以便为应急响应和防御策略提供数据支撑。

Method: 作者使用关键词驱动的dorking作为初筛，随后对候选页面进行系统化爬取并统计关键词出现次数以判别真阳性与假阳性；测量时长为一个月并记录篡改类型、再次篡改次数、第三方URL与域名分布，以及网站响应时间。

Result: 在一个月内共识别出453个被篡改网页；dorking单独筛查假阳性率约20.3%，经爬取与关键词计数可较可靠地区分真伪；发现重复篡改150例、固定实例129例、关键词修改55例以及重定向/隐藏URL注入；共捕获8837个独立第三方URL，涉及5930个域名，少量第三方在多站点重复出现；网站平均响应时间为75.3小时，响应行为不一致。

Conclusion: 本文表明，结合关键词驱动的dorking与系统化爬取的轻量方法，能够在短时间内有效发现并量化印尼网站上以非法网络博彩为目的的篡改行为，从而为持续测量和防御提供可操作的信息。

Abstract: This study investigates the defacement of Indonesian websites by actors
promoting illegal online gambling. Using a lightweight methodology that
combines keyword-driven dorking with systematic crawling, we identified 453
defaced webpages within one month. Although dorking alone yielded a false
positive rate of approximately 20.3\%, the integration of crawling and
keyword-counting enabled reliable differentiation between true and false
positives. Our measurements revealed diverse defacement behaviors, including
repeat defacements (150 cases), fixed instances (129), keyword modifications
(55), and redirections or hidden URL injections. In total, 8,837 unique
third-party URLs spanning 5,930 domains were captured, with a small subset
recurring across multiple sites. Website responses were inconsistent, with an
average reaction time of 75.3 hours. These findings demonstrate that simple,
reproducible techniques can provide meaningful insights into the scale,
persistence, and dynamics of defacement, highlighting the importance of
continuous measurement for strengthening defenses against online gambling
activities.

</details>


### [46] [A NIS2 pan-European registry for identifying and classifying essential and important entities](https://arxiv.org/abs/2508.19395)
*Fabian Aude Steen,Daniel Assani Shabani*

Main category: cs.CR

TL;DR: 基于Design Science Research，论文将NIS2法律要求转化为可执行的技术方案，提出并实现了一个模块化登记与监管系统原型，能自动化注册、分类与上报，适配欧盟各国但存在集成与法律解释差异等局限。


<details>
  <summary>Details</summary>
Motivation: NIS2要求欧盟成员国识别、分类并监管关键与重要实体，同时需向欧盟层面通报国家网络安全基础设施布局；为满足一致性、可审计与可扩展的监管需求，需将法律义务转化为可执行的技术系统以减轻监管负担。

Method: 采用Design Science Research方法论，将复杂的法律条文映射为结构化工作流、确定性分类算法与交互式仪表盘；实现模块化架构以支持自动/人工注册、情境化标签处理与跨指令依赖管理。

Result: 交付了一个为挪威生态系开发的可适配系统原型：自动化的实体注册与确定性分类、情境化标注以处理边缘与跨指令情形、支持向委员会/ENISA的通报流程以及面向监管者的仪表盘。系统设计保证最小修改即可部署到其他成员国，并列出若干局限与未来研究方向。

Conclusion: 提出并实现了一个模块化、法律支撑的登记与监管系统设计，能够将NIS2指令的法律条款转化为技术性要求并自动化实体注册、分类与上报流程，支持情境化标注与人工/自动混合流程，在降低行政负担并增强监管一致性方面具有实际应用价值，但仍受到国家间法律解释差异、系统集成与数据质量等限制。

Abstract: The NIS2 Directive establishes a common cybersecurity governance model across
the European Union, requiring member states to identify, classify, and
supervise essential and important entities. As part of a broader governance
network, member states are also obligated to notify the European Commission,
the Cooperation Group, and ENISA about their cybersecurity infrastructure
landscape. This thesis presents an analysis of the NIS2 Directive in this
context and translates its provisions into concrete technical requirements.
These requirements inform the design and implementation of a modular, legally
grounded registry system intended to support competent authorities across the
EU in meeting their obligations. Using the Design Science Research methodology,
the thesis transforms complex legal provisions into structured workflows,
deterministic classification algorithms, and interactive dashboards. The
resulting system automates key regulatory processes, including entity
registration, classification, and notification, while enabling context-aware
supervision and reducing administrative burden. It supports both automated and
manual registration methods and introduces a contextual labeling system to
handle edge cases, risk factors, and cross-directive dependencies. Although
developed for the Norwegian regulatory ecosystem, the system is designed for
adaptation by other member states with minimal modification. This thesis
contributes a reusable framework that bridges legal interpretation and
technical implementation, offering a scalable solution for national and
EU-level NIS2 cybersecurity governance. It also identifies key limitations and
outlines opportunities for future research and development.

</details>


### [47] [Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks](https://arxiv.org/abs/2508.19430)
*Kangfeng Ye,Roberto Metere,Jim Woodcock,Poonam Yadav*

Main category: cs.CR

TL;DR: 用Isabelle构建并验证了集成PLS的认证协议，扩展了对保密性与认证性关系的理解，并提出了一个经验证的PLS–Diffie–Hellman密钥协商方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于ProVerif的工作虽能验证保密性，但在帮助理解与扩展安全性分析方面存在局限；因此需要一个更可交互、可证明且对PLS友好的形式化验证环境。

Method: 在Isabelle中建立形式化模型并产生_sound animation_以支持交互与自动化验证；设计了一个通用且可配置的建模/验证框架，既支持传统密码学原语也支持PLS原语；通过网页界面在四个不同窃听位置、在被动与主动攻击下对保密性与认证性进行全面分析。

Result: 重现并加强了先前关于保密性的结论；揭示了“认证性在所有考察场景下仍然成立，即便部分场景下保密性被破坏”的不常见但可预期结论；并证明所提PLS–DH协议可安全派生具备必要认证性的会话密钥。

Conclusion: 作者使用Isabelle重建并验证了带有物理层安全(PLS)机制（如水印和干扰）的Needham–Schroeder协议，发现虽然在某些场景中保密性被破坏，但认证性在所有考察情形下仍然成立；并提出并验证了一个结合水印和干扰的PLS版Diffie–Hellman协议，用于安全会话密钥协商。

Abstract: Formal verification is crucial for ensuring the robustness of security
protocols against adversarial attacks. The Needham-Schroeder protocol, a
foundational authentication mechanism, has been extensively studied, including
its integration with Physical Layer Security (PLS) techniques such as
watermarking and jamming. Recent research has used ProVerif to verify these
mechanisms in terms of secrecy. However, the ProVerif-based approach limits the
ability to improve understanding of security beyond verification results. To
overcome these limitations, we re-model the same protocol using an Isabelle
formalism that generates sound animation, enabling interactive and automated
formal verification of security protocols. Our modelling and verification
framework is generic and highly configurable, supporting both cryptography and
PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy
and authenticity in four different eavesdropper locations under both passive
and active attacks) using our new web interface. Our findings not only
successfully reproduce and reinforce previous results on secrecy but also
reveal an uncommon but expected outcome: authenticity is preserved across all
examined scenarios, even in cases where secrecy is compromised. We have
proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and
jamming, and our analysis shows that it is secure for deriving a session key
with required authentication. These highlight the advantages of our novel
approach, demonstrating its robustness in formally verifying security
properties beyond conventional methods.

</details>


### [48] [CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection](https://arxiv.org/abs/2508.19450)
*Elvin Li,Onat Gungor,Zhengli Shang,Tajana Rosing*

Main category: cs.CR

TL;DR: CITADEL提出将表格转图像并用记忆感知掩码自编码器的自监督持续学习框架，实现无须标签的异常检测和长期知识保留，在IoT入侵检测上显著优于VLAD。


<details>
  <summary>Details</summary>
Motivation: 物联网设备互联性强且计算资源有限，易受新兴网络威胁影响；现有基于机器学习的入侵检测难以适应新威胁并在持续学习中出现灾难性遗忘。

Method: 将表格数据转为图像，使用记忆感知的掩码自编码器进行自监督表示学习，并结合基于新颖性检测的无监督异常检测，同时通过优化的记忆巩固机制实现持续学习与遗忘缓解。

Result: 在多个入侵数据集上，CITADEL在关键检测和保留指标上相较于基于变分自编码器的终身异常检测器（VLAD）最高提升72.9%。

Conclusion: CITADEL通过自监督和持续学习方法，有效提升了在动态物联网环境下对异常入侵的检测与长期记忆保留能力。

Abstract: The Internet of Things (IoT), with its high degree of interconnectivity and
limited computational resources, is particularly vulnerable to a wide range of
cyber threats. Intrusion detection systems (IDS) have been extensively studied
to enhance IoT security, and machine learning-based IDS (ML-IDS) show
considerable promise for detecting malicious activity. However, their
effectiveness is often constrained by poor adaptability to emerging threats and
the issue of catastrophic forgetting during continuous learning. To address
these challenges, we propose CITADEL, a self-supervised continual learning
framework designed to extract robust representations from benign data while
preserving long-term knowledge through optimized memory consolidation
mechanisms. CITADEL integrates a tabular-to-image transformation module, a
memory-aware masked autoencoder for self-supervised representation learning,
and a novelty detection component capable of identifying anomalies without
dependence on labeled attack data. Our design enables the system to
incrementally adapt to emerging behaviors while retaining its ability to detect
previously observed threats. Experiments on multiple intrusion datasets
demonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-based
lifelong anomaly detector (VLAD) in key detection and retention metrics,
highlighting its effectiveness in dynamic IoT environments.

</details>


### [49] [ReLATE+: Unified Framework for Adversarial Attack Detection, Classification, and Resilient Model Selection in Time-Series Classification](https://arxiv.org/abs/2508.19456)
*Cagla Ipek Kocal,Onat Gungor,Tajana Rosing,Baris Aksanli*

Main category: cs.CR

TL;DR: 通过先验数据集-模型仓库与对抗检测/分类，ReLATE+能在不显著损失性能的前提下大幅降低时间序列深度模型的重训练与计算成本。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类中深度模型计算成本高、需要实时处理且易受对抗攻击，现有方法在鲁棒性与模型选择上代价昂贵。希望通过利用先验（历史数据集与模型）来降低计算与重训练开销，同时保持鲁棒性与高性能。

Method: 构建一个包含数据集与相应最优模型的仓库；对流入数据先进行对抗样本检测与攻击类型分类；基于被识别的攻击类型与数据集级相似性检索仓库中相似数据集并复用其最佳模型，从而避免或减少对目标任务的重训练。

Result: 在多域实验中，ReLATE+平均减少了77.68%的计算开销，性能仅比Oracle低2.02%，并展示了对不同分布与特征空间的良好泛化能力与对抗韧性增强。

Conclusion: ReLATE+通过检测并分类对抗攻击、基于数据集相似性复用已有最佳模型，实现了在保持性能接近Oracle（2.02%差距）的同时，大幅降低重训练开销（平均77.68%）。

Abstract: Minimizing computational overhead in time-series classification, particularly
in deep learning models, presents a significant challenge due to the high
complexity of model architectures and the large volume of sequential data that
must be processed in real time. This challenge is further compounded by
adversarial attacks, emphasizing the need for resilient methods that ensure
robust performance and efficient model selection. To address this challenge, we
propose ReLATE+, a comprehensive framework that detects and classifies
adversarial attacks, adaptively selects deep learning models based on
dataset-level similarity, and thus substantially reduces retraining costs
relative to conventional methods that do not leverage prior knowledge, while
maintaining strong performance. ReLATE+ first checks whether the incoming data
is adversarial and, if so, classifies the attack type, using this insight to
identify a similar dataset from a repository and enable the reuse of the
best-performing associated model. This approach ensures strong performance
while reducing the need for retraining, and it generalizes well across
different domains with varying data distributions and feature spaces.
Experiments show that ReLATE+ reduces computational overhead by an average of
77.68%, enhancing adversarial resilience and streamlining robust model
selection, all without sacrificing performance, within 2.02% of Oracle.

</details>


### [50] [Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication](https://arxiv.org/abs/2508.19465)
*Onyinye Okoye*

Main category: cs.CR

TL;DR: 本研究主張以AI驅動的自適應驗證（結合ML、行為分析與情境風險）配合零信任原則，可顯著改善EV與EVC的身份驗證安全，並指出實際部署需解決隱私、對抗性攻擊與運營成本問題。


<details>
  <summary>Details</summary>
Motivation: 傳統RFID/NFC等識別技術因使用靜態識別碼與弱加密，容易遭受複製、中繼、竊聽及中間人攻擊；隨著電動車與充電基礎設施普及，迫切需要更具彈性與主動防禦能力的驗證機制。

Method: 提出以機器學習、異常檢測、行為分析與情境風險評估為核心的自適應驗證框架，並結合零信任架構原則（持續驗證、最小權限、保證通信安全）；透過系統性文獻回顧評估現有脆弱點與AI驅動解法的可行性。

Result: 文獻回顧與分析顯示，AI結合行為與情境感知的自適應驗證能提升檢測未知攻擊與減少誤判，並在可擴展性與彈性方面優於傳統靜態驗證，但仍面臨資料隱私、模型攻擊、延時與部署成本等挑戰。

Conclusion: 採用AI驅動的自適應驗證框架是保護電動車與充電系統的關鍵策略，能有效補強現有因RFID/NFC靜態識別與弱加密所造成的安全漏洞。

Abstract: The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle
Charging Systems (EVCs) has introduced new cybersecurity challenges,
specifically in authentication protocols that protect vehicles, users, and
energy infrastructure. Although widely adopted for convenience, traditional
authentication mechanisms like Radio Frequency Identification (RFID) and Near
Field Communication (NFC) rely on static identifiers and weak encryption,
making them highly vulnerable to attack vectors such as cloning, relay attacks,
and signal interception. This study explores an AI-powered adaptive
authentication framework designed to overcome these shortcomings by integrating
machine learning, anomaly detection, behavioral analytics, and contextual risk
assessment. Grounded in the principles of Zero Trust Architecture, the proposed
framework emphasizes continuous verification, least privilege access, and
secure communication. Through a comprehensive literature review, this research
evaluates current vulnerabilities and highlights AI-driven solutions to provide
a scalable, resilient, and proactive defense. Ultimately, the research findings
conclude that adopting AI-powered adaptive authentication is a strategic
imperative for securing the future of electric mobility and strengthening
digital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,
ML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,
MITM attacks, Zero Trust Architecture

</details>


### [51] [SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis](https://arxiv.org/abs/2508.19472)
*Kyler Katz,Sara Moshtari,Ibrahim Mujhid,Mehdi Mirakhorli,Derek Garcia*

Main category: cs.CR

TL;DR: SIExVulTS结合Transformer模型与静态分析，通过攻面识别、CodeQL暴露分析与GraphCodeBERT流验证三阶段方法，有效提高对CWE-200敏感信息暴露漏洞的检测与验证精度，并发现实际未知CVE。


<details>
  <summary>Details</summary>
Motivation: 现有检测工具很少针对CWE-200的多样子类并缺乏上下文感知的代码级数据流分析，导致敏感信息暴露长期被忽视且存在严重后果。

Method: 提出一个三阶段体系：1) 利用句子嵌入进行攻击面检测以识别敏感变量/字符串/注释/汇点；2) 基于CWE-200层次结构实例化CodeQL查询进行暴露分析；3) 用GraphCodeBERT对源到汇点的数据流进行语义验证。并在三个数据集（真实CVE、合成基准、31个开源项目标注流）上评估。

Result: 攻击面检测F1>93%，暴露分析F1=85.71%，流验证将精确率从22.61%提升至87.23%，并在Apache项目中发现6个未知CVE。

Conclusion: SIExVulTS在检测Java应用中的敏感信息泄露方面表现出高效性和实用性，能够弥补现有工具对CWE-200不同子类和上下文数据流分析的不足。

Abstract: Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a
persistent and under-addressed threat across software systems, often leading to
serious security breaches. Existing detection tools rarely target the diverse
subcategories of CWE-200 or provide context-aware analysis of code-level data
flows.
  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection
system that integrates transformer-based models with static analysis to
identify and verify sensitive information exposure in Java applications.
  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface
Detection Engine that uses sentence embeddings to identify sensitive variables,
strings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates
CodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification
Engine that leverages GraphCodeBERT to semantically validate source-to-sink
flows. We evaluate SIExVulTS using three curated datasets, including real-world
CVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31
open-source projects.
  Results: The Attack Surface Detection Engine achieved an average F1 score
greater than 93\%, the Exposure Analysis Engine achieved an F1 score of
85.71\%, and the Flow Verification Engine increased precision from 22.61\% to
87.23\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs
in major Apache projects.
  Conclusions: The results demonstrate that SIExVulTS is effective and
practical for improving software security against sensitive data exposure,
addressing limitations of existing tools in detecting and verifying CWE-200
vulnerabilities.

</details>


### [52] [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](https://arxiv.org/abs/2508.19493)
*Zhixin Lin,Jungang Li,Shidong Pan,Yibo Shi,Yue Yao,Dongliang Xu*

Main category: cs.CR

TL;DR: 作者创建了含7138场景的智能手机代理隐私基准，评测七款代理发现隐私识别普遍不足（最好67%），闭源优于开源，且识别能力随敏感度提升而增强。


<details>
  <summary>Details</summary>
Motivation: 随着MLLM驱动的智能手机代理广泛自动化用户任务，它们需要访问大量个人敏感信息，研究者希望量化并揭示这些代理在隐私识别与保护方面的能力和不足，从而推动在效用与隐私之间取得更合理的平衡。

Method: 构建7138个隐私相关场景并为每个场景标注隐私类型（如账号凭证）、敏感度等级和位置信息；将这些场景作为基准输入，选取七款主流智能手机代理（闭源与开源）进行系统化测试；在有/无显式提示的条件下评估代理的隐私识别率（RA）；统计并比较各代理在不同敏感度、隐私类型和提示设置下的表现。

Result: 在7138个测试场景上，几乎所有代理的隐私识别率低于60%（即使有显式提示也不足），闭源代理总体优于开源，Gemini 2.0-flash达到了最高67%的RA；隐私检测能力与场景敏感度正相关，高敏感度场景更易被检测；基准和代码已开源供社区使用。

Conclusion: 本文构建并发布了首个大规模智能手机代理隐私识别基准（SAPA-Bench），通过7138个场景及其隐私类型、敏感度和位置标注，评估了七款主流智能手机代理的隐私意识能力。结论是大多数代理对隐私识别不足，闭源优于开源，Gemini 2.0-flash表现最佳但仍只有67%的识别率，且识别能力与场景敏感度高度相关。

Abstract: Smartphones bring significant convenience to users but also enable devices to
extensively record various types of personal information. Existing smartphone
agents powered by Multimodal Large Language Models (MLLMs) have achieved
remarkable performance in automating different tasks. However, as the cost,
these agents are granted substantial access to sensitive users' personal
information during this operation. To gain a thorough understanding of the
privacy awareness of these agents, we present the first large-scale benchmark
encompassing 7,138 scenarios to the best of our knowledge. In addition, for
privacy context in scenarios, we annotate its type (e.g., Account Credentials),
sensitivity level, and location. We then carefully benchmark seven available
mainstream smartphone agents. Our results demonstrate that almost all
benchmarked agents show unsatisfying privacy awareness (RA), with performance
remaining below 60% even with explicit hints. Overall, closed-source agents
show better privacy ability than open-source ones, and Gemini 2.0-flash
achieves the best, achieving an RA of 67%. We also find that the agents'
privacy detection capability is highly related to scenario sensitivity level,
i.e., the scenario with a higher sensitivity level is typically more
identifiable. We hope the findings enlighten the research community to rethink
the unbalanced utility-privacy tradeoff about smartphone agents. Our code and
benchmark are available at https://zhixin-l.github.io/SAPA-Bench.

</details>


### [53] [Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills](https://arxiv.org/abs/2508.19500)
*David Noever*

Main category: cs.CR

TL;DR: MCP agents can chain legitimate service accesses into cross-domain attacks; tested 95 agents across services, found concrete attack chains; calls for new cross-service security measures and proposed benchmark experiments


<details>
  <summary>Details</summary>
Motivation: Identify vulnerability class in MCP agent systems where composed benign tasks lead to harmful emergent behavior

Method: Paper analysis

Result: Empirical evidence showing 95 agents orchestrating multi-service attacks (data exfiltration, financial manipulation, infra compromise); reveals service isolation assumption fails; proposes experimental directions

Conclusion: MCP architectures need cross-domain defenses; benchmark-driven experiments proposed to evaluate and mitigate compositional attacks

Abstract: This paper identifies and analyzes a novel vulnerability class in Model
Context Protocol (MCP) based agent systems. The attack chain describes and
demonstrates how benign, individually authorized tasks can be orchestrated to
produce harmful emergent behaviors. Through systematic analysis using the MITRE
ATLAS framework, we demonstrate how 95 agents tested with access to multiple
services-including browser automation, financial analysis, location tracking,
and code deployment-can chain legitimate operations into sophisticated attack
sequences that extend beyond the security boundaries of any individual service.
These red team exercises survey whether current MCP architectures lack
cross-domain security measures necessary to detect or prevent a large category
of compositional attacks. We present empirical evidence of specific attack
chains that achieve targeted harm through service orchestration, including data
exfiltration, financial manipulation, and infrastructure compromise. These
findings reveal that the fundamental security assumption of service isolation
fails when agents can coordinate actions across multiple domains, creating an
exponential attack surface that grows with each additional capability. This
research provides a barebones experimental framework that evaluate not whether
agents can complete MCP benchmark tasks, but what happens when they complete
them too well and optimize across multiple services in ways that violate human
expectations and safety constraints. We propose three concrete experimental
directions using the existing MCP benchmark suite.

</details>


### [54] [Breaking the Layer Barrier: Remodeling Private Transformer Inference with Hybrid CKKS and MPC](https://arxiv.org/abs/2508.19525)
*Tianshi Xu,Wen-jie Lu,Jiangrui Yu,Chen Yi,Chenqi Lin,Runsheng Wang,Meng Li*

Main category: cs.CR

TL;DR: 通过细粒度分解与线性算子融合并首次实现CKKS⇄MPC的安全转换，BLB在私有Transformer推理上显著减少通信与延迟，尤其在GPU加速下效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有私有Transformer推理方案通常用HE处理线性层、用MPC处理非线性层，但两者间频繁转换导致通信和延时开销高。通过减少HE⇄MPC转换并在密文域内尽量完成更多线性计算，可以提高效率。

Method: 提出细粒度算子分解与线性算子融合策略；设计CKKS与MPC之间的安全转换协议以处理融合后密文位宽增长；实现CKKS下的融合算子计算与一种高效的矩阵乘法协议用于Transformer的融合计算；在GPU加速下进行系统实现与评估。

Result: 在BERT-base、BERT-large和GPT2-base上的评测显示，BLB在通信开销上比BOLT(S&P'24)减少约21×，比Bumblebee(NDSS'25)减少约2×；在GPU加速下延迟比BOLT降约13×，比Bumblebee降约1.8×。

Conclusion: BLB通过将Transformer的层分解为细粒度算子并融合相邻线性算子，显著减少HE与MPC之间的转换次数，从而在保护数据隐私的前提下大幅降低通信开销和延迟。

Abstract: This paper presents an efficient framework for private Transformer inference
that combines Homomorphic Encryption (HE) and Secure Multi-party Computation
(MPC) to protect data privacy. Existing methods often leverage HE for linear
layers (e.g., matrix multiplications) and MPC for non-linear layers (e.g.,
Softmax activation functions), but the conversion between HE and MPC introduces
significant communication costs. The proposed framework, dubbed BLB, overcomes
this by breaking down layers into fine-grained operators and further fusing
adjacent linear operators, reducing the need for HE/MPC conversions. To manage
the increased ciphertext bit width from the fused linear operators, BLB
proposes the first secure conversion protocol between CKKS and MPC and enables
CKKS-based computation of the fused operators. Additionally, BLB proposes an
efficient matrix multiplication protocol for fused computation in Transformers.
Extensive evaluations on BERT-base, BERT-large, and GPT2-base show that BLB
achieves a $21\times$ reduction in communication overhead compared to BOLT
(S\&P'24) and a $2\times$ reduction compared to Bumblebee (NDSS'25), along with
latency reductions of $13\times$ and $1.8\times$, respectively, when leveraging
GPU acceleration.

</details>


### [55] [Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses](https://arxiv.org/abs/2508.19641)
*Lincan Li,Bolin Shen,Chenxi Zhao,Yuxiang Sun,Kaixiang Zhao,Shirui Pan,Yushun Dong*

Main category: cs.CR

TL;DR: Survey organizes attacks/defenses in GMLaaS, provides taxonomy, evaluation framework, benchmarks, and PyGIP toolkit


<details>
  <summary>Details</summary>
Motivation: GML training is resource-intensive and models/graphs are valuable IP; GMLaaS exposes models and data to attack via APIs

Method: survey, taxonomy and toolkit

Result: first taxonomy of threats/defenses for GML models and graph data; evaluation framework; curated benchmarks; PyGIP library (open-source)

Conclusion: This work establishes foundations for IP protection in GML and provides practical tools and benchmarks for researchers/practitioners

Abstract: Graph-structured data, which captures non-Euclidean relationships and
interactions between entities, is growing in scale and complexity. As a result,
training state-of-the-art graph machine learning (GML) models have become
increasingly resource-intensive, turning these models and data into invaluable
Intellectual Property (IP). To address the resource-intensive nature of model
training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an
efficient solution by leveraging third-party cloud services for model
development and management. However, deploying such models in GMLaaS also
exposes them to potential threats from attackers. Specifically, while the APIs
within a GMLaaS system provide interfaces for users to query the model and
receive outputs, they also allow attackers to exploit and steal model
functionalities or sensitive training data, posing severe threats to the safety
of these GML models and the underlying graph data. To address these challenges,
this survey systematically introduces the first taxonomy of threats and
defenses at the level of both GML model and graph-structured data. Such a
tailored taxonomy facilitates an in-depth understanding of GML IP protection.
Furthermore, we present a systematic evaluation framework to assess the
effectiveness of IP protection methods, introduce a curated set of benchmark
datasets across various domains, and discuss their application scopes and
future challenges. Finally, we establish an open-sourced versatile library
named PyGIP, which evaluates various attack and defense techniques in GMLaaS
scenarios and facilitates the implementation of existing benchmark methods. The
library resource can be accessed at: https://labrai.github.io/PyGIP. We believe
this survey will play a fundamental role in intellectual property protection
for GML and provide practical recipes for the GML community.

</details>


### [56] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: 论文发现安全性集中在少数注意力头，提出RDSHA定位关键头并以AHD训练将安全行为分散，从而提高对越狱攻击的鲁棒性且不损失功能性。


<details>
  <summary>Details</summary>
Motivation: 当前大模型的安全对齐仍易被对抗性提示绕过；调查发现安全机制主要依赖少数注意力头，攻击者可利用这一集中性实现绕过。基于此需要识别关键组件并设计使安全性分布化的训练方法。

Method: 提出了两部分：1) RDSHA，一种基于模型“拒绝方向”进行定向消融的方法，用以识别对安全行为关键的注意力头；2) AHD，一种训练策略，鼓励把安全行为在更多注意力头上编码，从而减少单点脆弱性。并在主流越狱攻击下进行了评估。

Result: 实验证明RDSHA能定位安全关键头；AHD能使安全能力在更多注意力头上分布；接受多种越狱攻击测试的模型显示，采用AHD的模型在安全鲁棒性上有显著提升，同时总体任务性能保持。

Conclusion: 模型的安全行为高度集中在少数注意力头上，去除这些头会使安全性明显下降；通过将安全相关行为在更多注意力头上分布（AHD），可以显著提升对越狱攻击的鲁棒性，同时保持模型的功能性。

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [57] [Addressing Deepfake Issue in Selfie banking through camera based authentication](https://arxiv.org/abs/2508.19714)
*Subhrojyoti Mukherjee,Manoranjan Mohanty*

Main category: cs.CR

TL;DR: 论文主张将相机源定位的法医识别系统用于deepfake检测，以应对自拍银行中的深度伪造身份欺诈，但摘要只描述了思路，缺少实验验证与性能数据。


<details>
  <summary>Details</summary>
Motivation: 深度学习生成的假人像在自拍银行应用中被滥用以绕过面部识别验证，现有检测方法需更多鲁棒性，因而尝试利用对相机源定位有效的法医线索来区分真实拍摄与合成图像。

Method: 将已有的法医识别方法（可能基于相机传感器噪声、图像处理痕迹或元数据分析）直接或经修改地用于deepfake检测——通过提取图像的微观成像特征并与真实相机指纹库比较，识别是否为合成图像或伪造身份。

Result: 摘要未给出实验结果或定量评估，只有方法设想与应用场景的讨论；未明确表明其检测性能、错误率或在现实银行流程中的可部署性。

Conclusion: 该论文提出将已用于相机源定位的法医识别系统应用于deepfake检测，认为该系统可识别生成图像与真实相机成像特征的差异，从而用于自拍银行场景的欺诈防护。

Abstract: Fake images in selfie banking are increasingly becoming a threat. Previously,
it was just Photoshop, but now deep learning technologies enable us to create
highly realistic fake identities, which fraudsters exploit to bypass biometric
systems such as facial recognition in online banking. This paper explores the
use of an already established forensic recognition system, previously used for
picture camera localization, in deepfake detection.

</details>


### [58] [The Art of Hide and Seek: Making Pickle-Based Model Supply Chain Poisoning Stealthy Again](https://arxiv.org/abs/2508.19774)
*Tong Liu,Guozhu Meng,Peng Zhou,Zizhuang Deng,Shuaiyin Yao,Kai Chen*

Main category: cs.CR

TL;DR: 本文系统性揭示了基于pickle的模型污染攻击面，涵盖模型加载与危险函数两大视角，发现大量扫描器遗漏的载入路径和可利用gadget，并提出异常导向编程（EOP）旁路技术，实验证明能高效绕过现有检测器。


<details>
  <summary>Details</summary>
Motivation: 尽管pickle已知不安全，但仍被广泛用于模型序列化，开源模型平台用户基数大幅提升了模型供应链攻击的现实风险，现有检测工具欠缺系统性覆盖，容易被绕过，因此迫切需要全面揭示攻击面并评估检测缺陷。

Method: 作者从两个视角系统化构建攻击面：1) 模型加载面：枚举分析五个主流AI/ML框架，识别22个pickle模型加载路径并测试现有扫描器检测盲区；提出EOP旁路技术并实现9个实例；2) 危险函数面：自动化挖掘和验证可链式利用的133个gadget，评估对多款扫描器的绕过率。

Result: 在模型加载面发现22条路径（19条被现有扫描器忽略），提出9个EOP绕过实例（7个能完全绕过所有扫描器）；在危险函数面发现133个可利用gadget，整体绕过率接近100%，即便是最强扫描器也有89%绕过率；研究成果已向厂商负责任披露并获$6000赏金。

Conclusion: 基于pickle的模型污染威胁比先前认知严重得多——现有扫描器覆盖不全且易被EOP和大量危险gadget绕过，需在模型加载流程与危险函数检测上进行全面补强。

Abstract: Pickle deserialization vulnerabilities have persisted throughout Python's
history, remaining widely recognized yet unresolved. Due to its ability to
transparently save and restore complex objects into byte streams, many AI/ML
frameworks continue to adopt pickle as the model serialization protocol despite
its inherent risks. As the open-source model ecosystem grows, model-sharing
platforms such as Hugging Face have attracted massive participation,
significantly amplifying the real-world risks of pickle exploitation and
opening new avenues for model supply chain poisoning. Although several
state-of-the-art scanners have been developed to detect poisoned models, their
incomplete understanding of the poisoning surface leaves the detection logic
fragile and allows attackers to bypass them. In this work, we present the first
systematic disclosure of the pickle-based model poisoning surface from both
model loading and risky function perspectives. Our research demonstrates how
pickle-based model poisoning can remain stealthy and highlights critical gaps
in current scanning solutions. On the model loading surface, we identify 22
distinct pickle-based model loading paths across five foundational AI/ML
frameworks, 19 of which are entirely missed by existing scanners. We further
develop a bypass technique named Exception-Oriented Programming (EOP) and
discover 9 EOP instances, 7 of which can bypass all scanners. On the risky
function surface, we discover 133 exploitable gadgets, achieving almost a 100%
bypass rate. Even against the best-performing scanner, these gadgets maintain
an 89% bypass rate. By systematically revealing the pickle-based model
poisoning surface, we achieve practical and robust bypasses against real-world
scanners. We responsibly disclose our findings to corresponding vendors,
receiving acknowledgments and a $6000 bug bounty.

</details>


### [59] [From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2508.19819)
*Viktor Valadi,Mattias Åkesson,Johan Östman,Salman Toor,Andreas Hellander*

Main category: cs.CR

TL;DR: 推理模式显著降低梯度反演难度；训练模式通常更安全，除非模型满足特定架构组合（浅宽、跳跃连接、预激活归一化）；作者提出新攻击并绘制了隐私风险地图。


<details>
  <summary>Details</summary>
Motivation: 现有研究多在推理模式下评估梯度反演，未充分考虑训练时行为（如dropout、批归一化统计）对攻击可行性的影响；因此需要系统化地评估训练模式与架构选择对隐私风险的影响。

Method: 作者系统性地比较了推理模式与训练模式对梯度反演攻击的影响，分析了架构因素（深度、宽度、跳跃连接、预激活归一化）对易损性的作用；提出并评估了两种新攻击方法，测试在不同知识条件下的效果；对目标检测模型进行了基于推理模式的攻击，并通过架构修改提高泄露可见性。

Result: 在推理模式下攻击更容易；在训练模式下，只有当模型浅而宽、带跳跃连接并使用预激活归一化时反演才通常成功；作者的新攻击在现实训练条件下达到了最先进的性能；生产级目标检测模型在默认设置下对反演保护较好，需要架构修改才能显著泄露。

Conclusion: 本文表明，在推理模式下进行梯度反演显著简化了攻击，而在训练模式下成功反演只在模型满足多项结构性条件时才可行（浅而宽、带跳跃连接、采用预激活归一化）。作者提出了两种针对训练模式的攻击方法，并实现了对生产级目标检测模型的首次攻击（在放宽到推理模式并修改架构后）。最后给出了不同架构和运行模式下隐私风险的全面映射，帮助评估何时易受攻击或相对安全。

Abstract: Gradient inversion attacks have garnered attention for their ability to
compromise privacy in federated learning. However, many studies consider
attacks with the model in inference mode, where training-time behaviors like
dropout are disabled and batch normalization relies on fixed statistics. In
this work, we systematically analyze how architecture and training behavior
affect vulnerability, including the first in-depth study of inference-mode
clients, which we show dramatically simplifies inversion. To assess attack
feasibility under more realistic conditions, we turn to clients operating in
standard training mode. In this setting, we find that successful attacks are
only possible when several architectural conditions are met simultaneously:
models must be shallow and wide, use skip connections, and, critically, employ
pre-activation normalization. We introduce two novel attacks against models in
training-mode with varying attacker knowledge, achieving state-of-the-art
performance under realistic training conditions. We extend these efforts by
presenting the first attack on a production-grade object-detection model. Here,
to enable any visibly identifiable leakage, we revert to the lenient inference
mode setting and make multiple architectural modifications to increase model
vulnerability, with the extent of required changes highlighting the strong
inherent robustness of such architectures. We conclude this work by offering
the first comprehensive mapping of settings, clarifying which combinations of
architectural choices and operational modes meaningfully impact privacy. Our
analysis provides actionable insight into when models are likely vulnerable,
when they appear robust, and where subtle leakage may persist. Together, these
findings reframe how gradient inversion risk should be assessed in future
research and deployment scenarios.

</details>


### [60] [Every Keystroke You Make: A Tech-Law Measurement and Analysis of Event Listeners for Wiretapping](https://arxiv.org/abs/2508.19825)
*Shaoor Munir,Nurullah Demir,Qian Li,Konrad Kollnig,Zubair Shafiq*

Main category: cs.CR

TL;DR: Large-scale measurement shows widespread third‑party keystroke listeners and some data exfiltration; authors map these practices to U.S. wiretapping law but call for further legal analysis to establish illegality.


<details>
  <summary>Details</summary>
Motivation: Bridge a tech-law gap by mapping long‑standing U.S. wiretapping statutes to modern web tracking practices, focusing on invasive real‑time keystroke interception.

Method: Instrumented browser crawl of a sample of the top-million websites, detecting third‑party JavaScript event listeners attached to input fields and tracing whether intercepted keystroke data is transmitted to third‑party servers.

Result: 38.52% of sites installed third‑party event listeners capable of intercepting keystrokes; at least 3.18% transmitted intercepted information to third‑party servers; evidence suggests exfiltrated inputs (e.g., emails) were used for unsolicited marketing.

Conclusion: Authors conclude that many websites employ third-party JavaScript event listeners that meet technical criteria for wiretapping under U.S. law, and a nontrivial subset transmit intercepted keystrokes to third parties; further legal work is needed to determine illegality.

Abstract: The privacy community has a long track record of investigating emerging types
of web tracking techniques. Recent work has focused on compliance of web
trackers with new privacy laws such as Europe's GDPR and California's CCPA.
Despite the growing body of research documenting widespread lack of compliance
with new privacy laws, there is a lack of robust enforcement. Different from
prior work, we conduct a tech-law analysis to map decades-old U.S. laws about
interception of electronic communications--so-called wiretapping--to web
tracking. Bridging the tech-law gap for older wiretapping laws is important and
timely because, in cases where legal harm to privacy is proven, they can
provide statutory private right of action, are at the forefront of recent
privacy enforcement, and could ultimately lead to a meaningful change in the
web tracking landscape.
  In this paper, we focus on a particularly invasive tracking technique: the
use of JavaScript event listeners by third-party trackers for real-time
keystroke interception on websites. We use an instrumented web browser to crawl
a sample of the top-million websites to investigate the use of event listeners
that aligns with the criteria for wiretapping, according to U.S. wiretapping
law at the federal level and in California. We find evidence that 38.52%
websites installed third-party event listeners to intercept keystrokes, and
that at least 3.18% websites transmitted intercepted information to a
third-party server, which aligns with the criteria for wiretapping. We further
find evidence that the intercepted information such as email addresses typed
into form fields are used for unsolicited email marketing. Beyond our work that
maps the intersection between technical measurement and U.S. wiretapping law,
additional future legal research is required to determine when the wiretapping
observed in our paper passes the threshold for illegality.

</details>


### [61] [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
*Shuo Shao,Yiming Li,Yu He,Hongwei Yao,Wenyuan Yang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: 首个LLM指纹化系统化研究，提供统一分类与LeaFBench基准，基于149模型与13种后开发策略评估现有方法，揭示性能瓶颈与未来方向。


<details>
  <summary>Details</summary>
Motivation: LLM训练成本高且具有重要知识产权价值，但易遭未授权使用或窃取；现有指纹方法可靠性不明，因模型修改多样且缺乏统一评估基准，因此需要一个全面评估框架与基准。

Method: 通过梳理现有白盒与黑盒指纹方法，构建统一框架和形式化分类；设计并实现LeaFBench：基于主流基础模型（149个实例），整合13种后开发技术（微调、量化、系统提示、RAG等），并在现实场景下系统评测各种指纹方法。

Result: 提出统一框架与白盒/黑盒分类；发布LeaFBench基准并在其上评测，发现不同方法在面对参数更改或参数无关变更时各有优势和脆弱点，揭示若干关键挑战与研究方向。

Conclusion: 该论文对LLM指纹识别进行了系统化研究，提出统一框架和分类，并构建LeaFBench基准评估工具，揭示了现有方法在实际部署修改下的性能优劣，指出后续研究方向和开放问题。

Abstract: The broad capabilities and substantial resources required to train Large
Language Models (LLMs) make them valuable intellectual property, yet they
remain vulnerable to copyright infringement, such as unauthorized use and model
theft. LLM fingerprinting, a non-intrusive technique that extracts and compares
the distinctive features from LLMs to identify infringements, offers a
promising solution to copyright auditing. However, its reliability remains
uncertain due to the prevalence of diverse model modifications and the lack of
standardized evaluation. In this SoK, we present the first comprehensive study
of LLM fingerprinting. We introduce a unified framework and formal taxonomy
that categorizes existing methods into white-box and black-box approaches,
providing a structured overview of the state of the art. We further propose
LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting
under realistic deployment scenarios. Built upon mainstream foundation models
and comprising 149 distinct model instances, LeaFBench integrates 13
representative post-development techniques, spanning both parameter-altering
methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms
(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the
strengths and weaknesses of existing methods, thereby outlining future research
directions and critical open problems in this emerging field. The code is
available at https://github.com/shaoshuo-ss/LeaFBench.

</details>


### [62] [SCAMPER -- Synchrophasor Covert chAnnel for Malicious and Protective ERrands](https://arxiv.org/abs/2508.20051)
*Prashanth Krishnamurthy,Ramesh Karri,Farshad Khorrami*

Main category: cs.CR

TL;DR: 本文发现同步相量协议中时间戳等字段被过度配置，可被滥用为隐蔽通道。提出SCAMPER框架用于攻击与防护，利用时间戳修改实现隐蔽通信并能作为检测FDI攻击的防御机制，已在两套HIL测试台上实验证明。


<details>
  <summary>Details</summary>
Motivation: 分析IEEE C37.118同步相量（synchrophasor）协议中数据载荷字段（特别是秒分之一时间戳字段）存在过度配置问题，可能被滥用于隐蔽通道，并评估其在攻击与防御两方面的应用。

Method: 分析IEEE C37.118数据载荷字段过度配置，设计基于时间戳修改的隐蔽编码方案，实现双重用途（攻击/防御），并在两个硬件在环（HIL）测试台进行实测评估。

Result: 提出SCAMPER框架，通过修改时间戳等过度配置字段构建隐蔽通道，既可用于发起隐蔽攻击以触发恶意动作，也可作为集成的加密数据完整性机制以检测虚假数据注入（FDI）攻击。在两套HIL测试平台上验证了方法有效性。

Conclusion: SCAMPER展示了同步相量协议字段既可被滥用构建无影响系统运行的隐蔽通道，也可被用于防御目的，通过嵌入完整性信息来检测FDI，实验验证了其实用性。

Abstract: We note that constituent fields (notably the fraction-of-seconds timestamp
field) in the data payload structure of the synchrophasor communication
protocol (IEEE C37.118 standard) are overprovisioned relative to real-world
usage and needs, lending themselves to abuse for embedding of covert channels.
We develop the SCAMPER (Synchrophasor Covert Channel for Malicious and
Protective ERrands) framework to exploit these overprovisioned fields for
covert communication and show that SCAMPER can be applied for both malicious
(attack) and protective (defense) purposes. Through modifications of the
timestamp field, we demonstrate that SCAMPER enables an attacker to accomplish
surreptitious communications between devices in the power system to trigger a
variety of malicious actions. These timestamp modifications can be performed
without having any impact on the operation of the power system. However, having
recognized the potential for this covert channel, we show that SCAMPER can
instead be applied for defensive security purposes as an integrated
cryptographic data integrity mechanism that can facilitate detection of false
data injection (FDI) attacks. We perform experimental studies of the proposed
methods on two Hardware-in-the-Loop (HIL) testbeds to demonstrate the
effectiveness of the proposed SCAMPER framework for both malicious and
protective purposes.

</details>


### [63] [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](https://arxiv.org/abs/2508.20083)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Kuan Li,Shuai Wang*

Main category: cs.CR

TL;DR: DisarmRAG通过对检索器的精细化、隐蔽编辑，成功绕过LLM的自我纠错，导致高成功率的RAG投毒攻击，表明需要加强检索器完整性与检测的防御措施。


<details>
  <summary>Details</summary>
Motivation: 此前的知识库投毒容易被现代LLM强大的自我纠错能力抵消，因此攻击者转而针对检索器以规避SCA。

Method: 提出DisarmRAG：基于对比学习的模型编辑，用于对检索器进行局部且隐蔽的修改，使其在特定受害查询下返回恶意指令；同时使用迭代共优化框架自动发现能绕过提示防御的稳健恶意指令。

Result: 在六个LLM和三个问答基准上，检索器被成功编辑后可实现近乎完美地检索恶意指令，在多种防御提示下攻击成功率超过90%，且编辑后的检索器在若干检测方法下仍保持隐蔽性。

Conclusion: 攻击者通过对检索器（retriever）进行定向、隐蔽的模型编辑，可以有效压制现代大模型的自我纠错能力（SCA），从而强制RAG系统输出攻击者指定的内容，并且该攻击在多个模型和基准上表现出高度成功率，强调需要面向检索器的防御。

Abstract: Retrieval-Augmented Generation (RAG) has become a standard approach for
improving the reliability of large language models (LLMs). Prior work
demonstrates the vulnerability of RAG systems by misleading them into
generating attacker-chosen outputs through poisoning the knowledge base.
However, this paper uncovers that such attacks could be mitigated by the strong
\textit{self-correction ability (SCA)} of modern LLMs, which can reject false
context once properly configured. This SCA poses a significant challenge for
attackers aiming to manipulate RAG systems.
  In contrast to previous poisoning methods, which primarily target the
knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that
compromises the retriever itself to suppress the SCA and enforce
attacker-chosen outputs. This compromisation enables the attacker to
straightforwardly embed anti-SCA instructions into the context provided to the
generator, thereby bypassing the SCA. To this end, we present a
contrastive-learning-based model editing technique that performs localized and
stealthy edits, ensuring the retriever returns a malicious instruction only for
specific victim queries while preserving benign retrieval behavior. To further
strengthen the attack, we design an iterative co-optimization framework that
automatically discovers robust instructions capable of bypassing prompt-based
defenses. We extensively evaluate DisarmRAG across six LLMs and three QA
benchmarks. Our results show near-perfect retrieval of malicious instructions,
which successfully suppress SCA and achieve attack success rates exceeding 90\%
under diverse defensive prompts. Also, the edited retriever remains stealthy
under several detection methods, highlighting the urgent need for
retriever-centric defenses.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [64] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: 提出PIR：对参数线性化的非线性ODE/PDE模型使用正则化OLS进行参数估计，较PINN更快更准确，适合实时流行病学参数估计


<details>
  <summary>Details</summary>
Motivation: 快速、稳健地估计参数线性化的非线性动力学模型参数，在精确性和计算效率上优于PINN，便于实时或准实时应用

Method: Physics-Informed Regression (PIR): linear-in-parameters regularized OLS

Result: 在合成与丹麦COVID-19真实数据上，PIR能成功估计目标参数，且在更复杂的分舱模型上比PINN表现更好；计算速度显著优于PINN；展示了对时变参数的估计能力

Conclusion: 对于参数线性模型，PIR提供了一种高效且可靠的参数估计方法，在精度与计算成本上优于PINN，适用于合成与真实流行病数据的参数及时变参数估计。

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [65] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: 将ZipNN思想扩展到FP8/FP4，通过对指数和尾数独立熵编码实现高压缩比（BF16 约62%，FP8 约83%），并发现K/V缓存同样可压缩，利于部署内存节省。


<details>
  <summary>Details</summary>
Motivation: As models grow and inference becomes widespread, reducing weight storage and transmission costs—especially for low-precision formats used in efficient inference—is crucial.

Method: Separate exponent and mantissa components and apply independent entropy coding (e.g., Huffman-like coding) to each; evaluate compression ratios on BF16, FP8 (and FP4 mentioned) and analyze K/V cache tensor compressibility.

Result: Reported compression ratios up to 62% for BF16 and 83% for FP8; K/V caches also show compressible patterns allowing memory savings during LLM deployment.

Conclusion: The paper extends lossless floating-point compression (ZipNN-like) to lower-precision formats (FP8, FP4), achieving substantial model size reductions and showing K/V cache tensors are also compressible, enabling deployment memory savings.

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [66] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: 提出POT——一种仅用提示的黑盒迭代优化攻击，生成隐蔽自然的冗长推理链以耗尽计算资源，无需外部数据访问或检索。


<details>
  <summary>Details</summary>
Motivation: 缓解现有过度思考攻击需外部数据和可检索毒化内容的限制，提升实用性。

Method: POT

Result: 提出基于LLM的迭代优化生成语义自然的对抗提示，在多模型多数据集上优于其他方法。

Conclusion: POT有效地在黑盒环境下诱发过度思考，具有更强的实用性和广泛适用性。

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [67] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: A framework for training DRL-based channel selection in real distributed IoT systems using ACK feedback is proposed and empirically validated, improving frame success rates.


<details>
  <summary>Details</summary>
Motivation: Existing DRL-based resource allocation work seldom uses real-world, distributed IoT data; the paper aims to bridge the gap by enabling practical, in-situ DRL training using transmission feedback.

Method: Propose a framework where IoT devices choose communication channels via a DRL-based agent; the DRL model is trained using real ACK feedback collected from actual transmissions in a distributed setting.

Result: Implementation and evaluation show the framework is both feasible and effective, improving FSR in experiments conducted with real transmissions.

Conclusion: The paper demonstrates that training DRL models with real-world feedback (ACK) in distributed IoT environments is feasible and improves channel selection performance, measured by Frame Success Rate (FSR).

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [68] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame 通过小型关联记忆缓冲在不改动模型和不交互的条件下，利用少量专家示范显著改善离线RL在低质量数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 在缺乏大量专家示范、离线数据质量参差的场景下，如何用少量高质量专家示范提升从大量低质量轨迹学习的效果。

Method: 提出一个可插拔模块 Re:Frame：在决策Transformer等离线RL骨干外附加一个小型关联记忆缓冲（AMB），缓冲由独立数据集中抽取的专家轨迹构成。训练和评估阶段基于内容的检索将 AMB 中的专家段落检索出来并与低质量数据一起用于决策，整个过程不改动骨干架构且不需与环境交互。

Result: 在 D4RL MuJoCo 上，用仅 60 条专家轨迹（占 6000 条数据的 0.1%）的 AMB，Re:Frame 在四种设置中有三种比强基线 Decision Transformer 持续提升，最高提升达 +10.7 归一化分数点，显示出高数据效率的效果。

Conclusion: Re:Frame 能在离线强化学习中通过检索少量专家轨迹显著提升策略表现，其在无需环境交互和无需修改骨干模型的前提下，通过外部联想记忆缓冲（AMB）将专家经验整合到策略决策中。

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [69] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: 作者提出NCMemo度量GNN在节点分类中对标签的记忆，发现低同质性（homophily）会显著增加记忆；记忆由GNN在结构信息不足时的隐式偏好驱动；通过图重连（rewiring）可以有效降低记忆和隐私风险且不损性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明DNN会记忆训练数据，但关于GNN的记忆性研究较少；理解GNN在半监督节点分类中如何记忆标签及其与图结构（同质性）关系对模型泛化和隐私保护具有重要意义。

Method: 提出NCMemo框架来量化标签记忆；通过实证分析训练动力学和结构使用的隐式偏好，测量不同同质性条件下的记忆量；分析节点级别的不一致性对记忆的影响；最后评估图重连方法在降低记忆和隐私泄露上的效果。

Result: This paper introduces NCMemo, a framework to quantify label memorization in semi-supervised node classification for GNNs, revealing an inverse relationship between memorization and graph homophily. It shows that lower homophily increases memorization, ties this to GNNs' implicit bias to use structure, identifies nodes with high label inconsistency as more prone to memorization, and demonstrates graph rewiring can reduce memorization and privacy risk without hurting performance.

Conclusion: GNN在低同质性图上更依赖记忆以拟合训练标签，节点邻域中标签不一致性越高越容易被记忆；图重连是一个有效的缓解手段，可降低记忆和隐私风险同时保持模型性能。

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [70] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: 通过对多源模型进行SVD分解并只聚合/微调最重要的奇异成分，提出了一种细粒度、高效且鲁棒的多源迁移学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有多源迁移方法粗粒度且难以在大量或大尺度源模型间高效且精确地提取和融合知识。该论文动机是通过细粒度的SVD分解与组件级聚合来提高知识提取精度与聚合效率，从而在不完全访问源数据或重训练源模型的情况下更好地利用海量在线模型资源。

Method: 方法包括：1) 对每个源模型关键层（如权重矩阵）进行SVD分解，得到秩一分量（左右奇异向量乘以奇异值）；2) 在聚合阶段从所有源的秩一分量中选择最显著或相关性最高的组件，构建一个合并矩阵；3) 在目标任务上仅微调合并矩阵的主奇异值以调整各组件的重要性，从而实现高效适配；4) 该方法对输入扰动和参数空间噪声（如剪枝）保持鲁棒性，并具有良好的计算可扩展性。

Result: 结果表明：与传统的整模型或层级聚合方法相比，SVD分解+组件级选择在迁移性能、计算/存储效率和对噪声或剪枝的鲁棒性方面表现更好；仅微调主奇异值能在参数效率上显著优于全模型微调，同时保持或提升目标任务性能。

Conclusion: 该论文提出一种基于SVD的多源迁移学习框架，通过将每个源模型分解为秩一分量并从所有源中聚合最显著的分量，然后只微调合并矩阵的主奇异值，以高效且精确地利用多源模型知识，增强适应性并降低重训练成本。

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [71] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: 这篇综述性导引介绍了将化学体系抽象为图（分子、蛋白质、化学过程）的基本理论与方法，重点讲解图的设计原则、常见预测任务、图神经网络等学习算法的应用，并通过代表性示例展示在化学科学中的实际用途，旨在帮助读者将图方法应用于新一代化学发现。


<details>
  <summary>Details</summary>
Motivation: 化学问题本质上由实体与相互作用构成，图提供自然且灵活的表示形式；机器学习（尤其是图神经网络）能利用该结构信息，提高预测与设计效率，因此需要一本面向化学家的图方法入门指导。

Method: 文章首先将图作为数学对象形式化，介绍节点、边、属性和子图等要素；随后讨论图数据设计原则（例如表征选择、层次化与多体相互作用的编码）、常见预测任务（分类、回归、生成、边预测、动力学模拟等），并重点介绍图神经网络及其变体的架构和训练要点。

Result: 通过示例（分子性质预测、蛋白质结构与相互作用建模、反应网络与流程优化等）展示图方法的应用价值，并讨论了挑战（可解释性、数据稀缺、尺度与动力学建模）与未来方向（多尺度整合、可扩展生成模型、与物理知识融合）。

Conclusion: 图是化学领域强有力的抽象表示，结合图神经网络等机器学习方法，可以有效建模分子、蛋白质与化学过程的结构与相互作用，从而加速化学发现与设计。

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [72] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: Proposes a compact TCN+Mamba model using RR intervals to predict paroxysmal AF early, achieving high accuracy and efficiency, and predicting AF up to 2 hours ahead from 30-min inputs.


<details>
  <summary>Details</summary>
Motivation: Existing AF detection misses paroxysmal AF (PAF) due to short, sudden episodes; early prediction can enable preventive therapy to reduce progression and complications.

Method: Lightweight TCN+Mamba model for early AF prediction

Result: Subject-wise: sensitivity 0.908, specificity 0.933, F1 0.930, AUROC 0.972, AUPRC 0.932; model size 73.5K params, 38.3 MFLOPs; predicts AF up to 2 hours ahead using 30 min RRI input.

Conclusion: The proposed model enables practical, efficient early AF prediction from short RR interval recordings, suitable for wearable deployment and preventive intervention.

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [73] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: 本文首次将信息几何、谱嵌入与扩散温度动态结合，建立了可证明的多模态LLM幻觉量化框架，给出模态感知的幻觉能量度量与时间依赖上界。


<details>
  <summary>Details</summary>
Motivation: 现有对MLLM幻觉的评估多为启发式或经验性方法，缺乏数学上的可量化度量与可证保证；因此需要一个理论上可解释、模态感知且能随时间追踪幻觉演化的框架。

Method: 将MLLM输出表示为基于多模态图拉普拉斯算子的谱嵌入，利用Reproducing Kernel Hilbert Space（RKHS）中的本征模态分解，结合扩散动力学与温度退火分析，构建语义畸变（manifold gap）度量，并通过Rayleigh–Ritz不等式导出随时间的幻觉能量界。

Result: 提供了模态意识的、可解释的幻觉度量与时变能量上界，能通过谱与RKHS本征分解追踪不同模态与提示下幻觉的演化；为幻觉从“质性风险”转为“可分析现象”奠定了理论基础。

Conclusion: 提出了一个基于信息几何与扩散动力学的严谨框架，用以量化多模态大语言模型（MLLM）中的幻觉现象，并给出关于幻觉能量随时间与温度变化的上界与解析量度。

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [74] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: Fine-tuned LLaMA-3.2-based VLMs perform at least as well as CNNs on neutrino detector image classification, with added multimodal/contextual reasoning benefits, advocating VLMs as a useful backbone for HEP event classification.


<details>
  <summary>Details</summary>
Motivation: Explore multimodal reasoning capabilities of modern LLM-based VLMs in high-energy physics tasks, and assess whether VLMs can provide superior performance and richer context integration compared to traditional CNNs for neutrino event classification.

Method: Fine-tuning a vision-language model (LLaMA 3.2 backbone) on pixelated detector images for neutrino event classification; benchmarking against an established CNN used in NOvA/DUNE-style experiments; evaluating metrics including accuracy, precision, recall, and AUC-ROC; integrating auxiliary textual or semantic context into the VLM.

Result: The VLM matches or exceeds CNN performance on standard classification metrics and enables additional benefits (richer reasoning, better use of auxiliary textual/semantic context). The study suggests VLMs as a promising direction for HEP event classification and multimodal experimental workflows.

Conclusion: The paper demonstrates that a fine-tuned Vision-Language Model (VLM) based on LLaMA 3.2 can match or outperform an established CNN baseline for classifying neutrino interactions from pixelated detector images, while also offering richer multimodal reasoning and better integration of auxiliary textual/semantic context. The authors propose VLMs as a promising general-purpose backbone for event classification in HEP.

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [75] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: 利用角嵌入的两种混合量子-经典模型在多套恶意软件数据集上评估。QMLP准确率更高（尤其是多分类），QCNN训练更高效但精度下降；量子方法对二分类表现良好，但多分类与不同数据集上的稳定性仍有待提升。


<details>
  <summary>Details</summary>
Motivation: 经典机器学习已广泛用于恶意软件分类，但随着量子计算的发展，量子机器学习可能带来范式性改进。研究动机是探索QML在恶意软件检测领域的应用潜力与效果，填补该领域应用研究的空白。

Method: 提出并评估两种混合量子-经典模型：量子多层感知器(QMLP)和量子卷积神经网络(QCNN)。两者均使用角嵌入(angle embedding)将恶意软件特征编码为量子态。QMLP通过全量子位测量和数据重上传(data re-uploading)来捕捉复杂模式；QCNN采用量子卷积与池化层来减少活跃量子位，从而提高训练效率。模型在五个恶意软件数据集(API-Graph, EMBER-Domain, EMBER-Class, AZ-Domain, AZ-Class)上，分别进行了二分类和多分类评估。

Result: 实验结果显示：二分类准确率较高——API-Graph:95–96%，AZ-Domain:91–92%，EMBER-Domain:77%。多分类准确率差异更大——API-Graph:91.6–95.7%，AZ-Class:41.7–93.6%，EMBER-Class:60.7–88.1%。总体趋势为：QMLP在复杂多类任务上优于QCNN；QCNN训练更快、所需量子资源更少，但总体精度较低。

Conclusion: 该论文表明混合量子-经典模型在恶意软件分类任务上具有可观潜力：QMLP在复杂的多分类任务中表现优于QCNN，而QCNN在训练效率和所需活跃量子位方面有优势，但以精度为代价。总体而言，量子模型在二分类任务上达到与经典方法竞争的高准确率，但在多分类和不同数据集间表现波动较大，说明仍需更多研究以评估可扩展性和鲁棒性。

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [76] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: 这篇论文提出了DETNO（一种扩散增强的Transformer神经算子），用于改善长期多步交通流预测中高频（尖锐）特征的重建与滚动稳定性。核心思路是把变压器神经算子的表达能力与扩散模型的逐步去噪结合，克服传统神经算子预测平滑导致的误差累积。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子虽然擅长学习函数空间映射，但其预测往往过于平滑，无法重现交通中重要的高频现象（如冲击波、拥堵边界），导致多步滚动预测误差迅速增加，影响实时交通管理决策。

Method: 方法包括：1）使用带交叉注意力的Transformer神经算子实现超分辨率和跨域映射；2）引入扩散模型作为后处理迭代去噪模块，逐步重构尖锐密度梯度等高频信息；3）联合训练或级联推理以缓解神经算子本身的平滑偏差并减少多步滚动误差累积。

Result: 在混沌/高频交通数据集的综合评估中，DETNO在长时间滚动预测上显著优于基线模型，能更好地保留尖锐特征并提升预测稳定性与准确性。

Conclusion: DETNO在混沌交通数据集上的长时滚动预测中优于传统和基于变压器的神经算子，能更好地保留高频成分并提高长时预测稳定性。

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [77] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: Hybrid quantum-classical model for SMILES reconstruction yields ~84% quantum fidelity and ~60% classical similarity, outperforming quantum baselines and suggesting a path forward for QML in molecular discovery.


<details>
  <summary>Details</summary>
Motivation: Recent QML advances hold promise for generative molecular design, but many classical approaches struggle with fidelity and validity; integration of QML with sequence-based tasks like SMILES reconstruction is underexplored and often degrades fidelity, motivating a hybrid approach that balances quantum expressivity and classical sequence modeling.

Method: They propose a hybrid architecture where quantum encoding produces expressive quantum representations of SMILES input, which are then processed by classical sequence models (likely decoders) for reconstruction. The model is evaluated using metrics of quantum fidelity and classical reconstruction similarity and compared to existing quantum baselines.

Result: The approach attains approximately 84% quantum fidelity and 60% classical reconstruction similarity, outperforming prior quantum baselines and demonstrating the viability of quantum-aware sequence models for molecular sequence reconstruction.

Conclusion: The paper concludes that a hybrid quantum-classical architecture combining quantum encoding with classical sequence modeling can improve SMILES reconstruction, achieving higher quantum fidelity and better classical reconstruction similarity than existing quantum baselines, and provides a promising foundation for future QML applications in molecular and drug discovery.

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [78] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: 用Kolmogorov–Arnold一维变换替代MLP构建保持辛结构的HNN，可更好拟合高频/多尺度Hamiltonian，降低能量漂移，提升长期预测稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLP的HNN在复杂能量景观下对超参数敏感、难以捕获高频/多尺度特征，导致能量漂移和长期不稳定性；因此提出基于一维变换的替代网络结构以提高鲁棒性和可解释性。

Method: 利用Kolmogorov–Arnold表示将多维函数分解为一系列可学习的一维变换与线性组合，构建保持辛结构的Hamiltonian神经网络；训练时直接学习Hamiltonian函数，采用局部化函数逼近来更好表示复杂能量景观。

Result: 在弹簧-质量、单摆、二体及三体问题的基准测试中，KAR-HNN在能量守恒（较小的能量漂移）和长期轨迹预测稳定性上优于传统MLP-HNN，且在高维、参数稀少场景下表现出潜在优势。

Conclusion: KAR-HNN通过将MLP替换为基于Kolmogorov–Arnold表示的一维变换，实现了对高频、多尺度Hamiltonian动力学的更稳健拟合，从而降低能量漂移并提升长期预测稳定性。

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [79] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Mechanistic study shows Llama-3.1-8B-Instruct’s format-dependent numeric-comparison bug is due to even/odd head specialization with a sharp 8-head threshold at Layer 10; targeted partial repairs achieve perfect fixes, revealing hidden modular substructure and efficiency opportunities.


<details>
  <summary>Details</summary>
Motivation: To mechanistically explain a surprising format-dependent reasoning failure, reveal submodule internal structure (hidden redundancy and thresholds), and show implications for interpretability and efficient repair of models.

Method: Systematic surgical interventions on attention heads (combinatorial head manipulations and pattern replacements), layerwise feature-overlap measurements (SAE analysis), and threshold experiments varying number and combinations of even heads and pattern-replacement proportions to diagnose and repair the failure.

Result: Found even/odd head specialization (even heads handle numerical comparison), exact requirement of 8 even heads at Layer 10 for repair (any 8+ succeed, ≤7 fail), 10% feature overlap at Layer 7 and 80% at Layer 10 with 1.5× feature amplification in failing formats, successful repair using 25% of heads, and a 60% pattern-replacement threshold. Code released.

Conclusion: The paper identifies a format-dependent numerical-comparison bug in Llama-3.1-8B-Instruct and demonstrates a mechanistic cause: an even/odd attention-head specialization where 8 even heads at Layer 10 are necessary and sufficient for a perfect repair. The bug shows sharp combinatorial thresholds and high redundancy, and targeted, low-cost interventions can fully fix the model.

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [80] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: 提出将可微分多相流模拟器与CNN耦合并采用单相→多相的迁移学习，显著减少全物理仿真需求，在工程注采/压力约束问题上实现高效且物理一致的控制预测。


<details>
  <summary>Details</summary>
Motivation: 地下储层压力控制受地质非均质性和多相流复杂动力学影响，且高保真物理仿真昂贵，导致在不确定性分析或控制优化中需要大量仿真而难以实现。论文旨在通过结合物理可微分模拟与机器学习，降低计算成本并保持物理一致性。

Method: 构建端到端物理驱动学习流程：使用DPFEHM实现可微分的多相瞬态流模拟器，设计CNN以从异质渗透率场预测注采速率并用以控制井口压力；训练时直接将物理残差/目标通过可微分模拟器反馈到网络参数；为加速训练先在单相稳态仿真上预训练，再在少量全物理多相瞬态仿真上微调。

Result: 在实验中表明，通过迁移学习从廉价单相仿真预训练，可将所需的全物理多相瞬态仿真次数从此前估计的高达千万级，降低到不足3000次，同时保持高精度预测与约束满足能力。

Conclusion: 通过将可微分的多相流模拟器（DPFEHM）与卷积神经网络耦合，并在训练中嵌入瞬态多相流物理，论文证明可在满足关键井位压力约束的同时，高效预测流体抽取率；利用从单相稳态仿真的迁移学习，大幅减少所需全物理多相仿真次数，从而实现可行的工程级应用。

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [81] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: 本文提出一种用于将43种癌症类型基于编码突变数据进行群体级聚类的无监督对比学习框架。通过构建基因级和染色体级两种互补突变视图，使用TabNet编码器和多尺度NT-Xent对比损失学习统一的癌症类型嵌入，得到与已知突变过程和组织起源一致的生物学上有意义的簇。


<details>
  <summary>Details</summary>
Motivation: 目前群体级癌症聚类多依赖传统统计方法，难以充分利用复杂、高维的突变模式。引入对比学习可以在无监督条件下从两种互补视图中学习鲁棒且可解释的癌症类型表征。

Method: 对每个癌症类型构建两种突变签名：基因级（高频突变基因的碱基替换模式）和染色体级（按染色体归一化的替换频率）。使用TabNet分别编码两视图，并通过NT-Xent多尺度对比损失进行优化，学习统一的癌症类型表示，然后在低维嵌入上进行聚类分析。

Result: 在COSMIC编码突变数据上对43种癌症进行实验，所学嵌入形成的簇与已知突变过程和组织来源一致，展示了方法的可解释性和生物学一致性。

Conclusion: 对比学习框架能在群体级别从突变数据学习到可解释且生物学相关的癌症类型嵌入，克服了传统统计方法在可扩展性和表达能力上的局限，是首个将对比学习用于群体级癌症聚类的尝试。

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [82] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: 通过对局部stencil做space-filling采样合成训练数据，可显著降低训练神经PDE所需的模拟样本量并提升泛化性能，尤其在结合一条完整轨迹时效果更优。


<details>
  <summary>Details</summary>
Motivation: 传统用数值求解器长时间积分获得的轨迹数据存在强烈时空冗余、样本效率低且可能很少覆盖稀有但重要的状态，限制了神经PDE算子的泛化与训练效率。

Method: 提出一种数据增强/合成策略：从计算模型中对局部stencils进行空间填充式采样以生成训练样本，打破传统基于长时间积分轨迹的数据采集方式，此外可选择性地结合单条全轨迹以改进学习。

Result: 在多个PDE体系上实验证明，合成的stencil数据能训练出更好的神经stencil算子，性能优于直接从轨迹中朴素采样的stencil数据；且仅需相当于10步模拟量的数据即可达到高精度。

Conclusion: 该方法通过对局部‘模板（stencil）’状态进行填充性（space-filling）采样，从而在训练神经PDE时大幅提升样本效率，能用相当于仅10步数值模拟的数据学出准确的神经PDE模板算子，并在有单条完整轨迹时进一步提高性能。

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [83] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: 该论文提出在生成模型中引入内部张量分解：生成张量的因子而非完整张量，从而降低生成复杂多维模拟数据的计算与存储成本。实验显示在保持数据有用性的同时显著减少模型输出与参数量。


<details>
  <summary>Details</summary>
Motivation: 生成大规模复杂模拟数据代价高昂，尤其在实验昂贵或资源受限时，使用生成模型合成数据变得更可行。进一步通过张量分解减少模型输出与参数，从而降低生成成本。

Method: 在生成对抗网络或扩散模型等生成模型内部，输出不是完整的高维张量，而是张量分解后的低维因子（例如CP或Tucker因子）。模型学习生成这些因子，随后通过张量重构得到完整数据，从而减少输出维度与参数量。

Result: 通过在生成模型中生成张量因子而非完整张量，显著减少了模型输出大小和参数数量，同时实验表明生成的数据在下游任务中仍然保持有用性。

Conclusion: 在生成多维模拟数据时，将张量分解集成到生成模型中能显著降低生成成本与模型规模，同时生成的数据仍然对下游任务有用，表明该方法在高维数据生成上具有实际价值。

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [84] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: 论文证明：现代生成模型的若干基础模块在一般条件下几乎总是满射，这意味着理论上任意输出都有对应输入，从而揭示了生成模型在安全与越狱攻击方面的根本性脆弱性。


<details>
  <summary>Details</summary>
Motivation: 动机源于安全性与可控性担忧：若生成模型是满射的，则理论上任意输出（包括有害/违禁内容）都可被某些输入生成，说明模型存在不可避免的“越狱/对抗”攻击面。作者希望以形式化证明来揭示这种根本脆弱性。

Method: 作者通过数学生命证明与泛化的代数/微分方法（例如分析映射的雅可比矩阵秩、良态性条件、测度零集论证等），证明了在一般条件下这些模块和组合结构几乎处处为满射，并由此推出若干结构的推论。

Result: 主要结果包括构造性与理论性的命题：给定常见模块及其组合形式，证明在通用假设下它们对输出空间是几乎处处可逆/满射的；推出 GPT 风格 Transformer、带确定性 ODE 求解器的扩散模型等具有逆映射存在的推论，从而强调广泛存在的安全风险。

Conclusion: 论文主结论为：许多现代神经网络常见模块（如 pre-layer normalization、线性注意力）在参数的几乎所有取值下都是满射的（surjective），因此包括 GPT 式 Transformer 和使用确定性 ODE 解算器的扩散模型在内的广泛生成框架对任意输出都存在逆映射。

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [85] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: 在高斯均值估计中，会员推断攻击有时需要Ω(n + n^2ρ^2)个参考样本，超过训练样本数，说明现有仅用O(n)样本的攻击方法可能低估风险。


<details>
  <summary>Details</summary>
Motivation: 研究攻击者进行membership-inference攻击所需的额外分布信息量，特别是参考样本数量，即样本复杂度。

Method: 在d维高斯分布下分析高斯均值估计问题，设定估计误差上界为ρ^2 d，推导出对抗性membership-inference的样本复杂度下界Ω(n + n^2ρ^2)，并比较现有攻击的样本使用限制。

Result: 在高斯均值估计场景下，证明了实现与完全知情攻击者竞争的membership-inference攻击有时需至少Ω(n + n^2ρ^2)个参考样本；这表明攻击者可能需要比训练算法使用的样本数更多的信息。

Conclusion: 攻击者在某些情况下需要远多于训练集大小的参考样本来成功进行会员推断；因此实际攻防评估需考虑当攻击者可获得大量分布信息时的更强攻击。

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [86] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: Incentivized exploration for continuous-armed bandits with incentive-induced reward drift: discretize arm space uniformly and design compensation-aware algorithms that ensure both regret and payments scale sublinearly as Õ(T^{(d+1)/(d+2)}).


<details>
  <summary>Details</summary>
Motivation: Classic incentivized bandit work assumes finite arms and unbiased feedback; real problems have continuous arm spaces and incentives introduce biased (drifting) rewards. Need algorithms that handle infinite arms and drift yet keep regret and payments low.

Method: Discretize infinite metric spaces and incentivize exploration with compensation under reward drift

Result: Propose uniform discretization algorithms achieving simultaneous sublinear cumulative regret and sublinear total compensation; bounds of Õ(T^{(d+1)/(d+2)}) where d is covering dimension. Extend to contextual bandits with similar guarantees; validated by simulations.

Conclusion: Uniform discretization plus compensation schemes can manage exploration under reward drift in continuous metric spaces, yielding sublinear regret and payment; approach extends to contextual cases and is empirically supported.

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [87] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas 通过学习局部低维映射并用拓扑畸变判定流形性，从而既能验证流形假设，又能在成立时构建生成性流形模型。


<details>
  <summary>Details</summary>
Motivation: 现有流形学习方法多生成全局嵌入，而数学上定义流形依赖于局部图（local charts）；此外，现有工具无法判断某数据集是否满足流形假设。为此提出 DeepAtlas，用局部映射与可逆网络来补足两者缺陷并提供流形有效性检验。

Method: 算法先为数据的每个局部邻域生成低维表示（局部嵌入），然后训练深度神经网络学习这些局部嵌入与原始高维数据之间的映射；通过计算拓扑畸变（topological distortion）来评估流形性并估计流形维度。

Result: 在测试数据集上，DeepAtlas 成功恢复并学习了流形结构；在若干真实数据集（含单细胞 RNA-seq）上发现并非都满足流形假设；对于满足流形假设的数据，DeepAtlas 可用作生成模型并为将微分几何工具应用于数据分析开辟道路。

Conclusion: DeepAtlas 提供了一种将数据局部邻域映射为低维流形图谱的方法，能够构建局部 Chart 与原始数据之间的可逆映射，并用拓扑畸变量来判定数据是否遵循流形假设及其维数。结果表明当数据确实源自流形时，DeepAtlas 能成功学习其结构并生成性建模；但许多真实数据（例如单细胞 RNA-seq）并不满足流形假设。

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [88] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: 本论文提出面向分布转移的表格学习任务（DSTL）及一种名为SAFT的框架，通过将离散特征变换搜索转换为连续表征生成并引入三个稳健性机制（嵌入去相关与样本重加权、次优嵌入平均以提高平坦性、归一化对齐训练与测试分布），显著提升在真实分布转移下的鲁棒性和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 传统表格学习在训练与测试分布不一致时性能骤降，现有的特征工程或自动化方法难以兼顾可微性和稳健性，故需一个可微、面向分布转移且能生成稳健变换的框架。

Method: 将特征变换问题从离散组合搜索改为连续的表示生成，通过可微分优化直接学习变换后的特征嵌入；并结合（1）通过嵌入去相关与样本重加权得到抗转移表征，（2）对多组次优嵌入进行平均以获得平坦（flatness-aware）的生成策略，（3）使用归一化操作在训练与测试间进行对齐。

Result: 在多种真实世界分布转移数据集上，SAFT在鲁棒性、效果和泛化性方面均优于先前表格学习方法。

Conclusion: SAFT有效提升了表格学习在分布转移场景下的鲁棒性与泛化能力，实验显示其在多种真实世界分布转移情况下优于现有方法。

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [89] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE通过评估器引导的嵌入优化与蒸馏微调，将符号方程搜索转为嵌入空间的连续优化，从而在小样本符号回归任务中显著提升性能与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管在大规模方程数据上预训练的基础模型具有潜力，但在小数据集或领域专用问题上常出现负迁移和泛化能力不足，因而需要一种数据高效且能保持可解释性的微调/蒸馏策略。

Method: 方法包括：1) 利用蒸馏对预训练基础模型进行数据高效微调；2) 将符号-数值对齐（symbolic-numeric alignment）与评估器（evaluator）引导的嵌入优化相结合；3) 在共享嵌入空间中进行嵌入搜索并生成候选方程，优化目标兼顾数据-方程拟合度与解析式简洁性。

Result: 在Feynman、Strogatz及若干黑箱数据集上的实验表明，EQUATE在准确性和鲁棒性上均优于现有最先进方法，同时保持低复杂度的解析式与较快的推理速度。

Conclusion: 该论文提出了一种名为EQUATE的方法，通过在共享嵌入空间中进行基于评估器的嵌入优化，将离散的符号方程搜索转化为连续优化问题，从而提高基础模型在小样本、领域特定符号回归任务中的表现。

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [90] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: 论文通过新环境（PoolFlip）和基于种群的 MARL（Flip-PSRO）提高了 FlipIt 风格博弈中防御策略对未知攻击的泛化能力，实验显示在未见攻击上性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有 FlipIt 变体依赖少量启发式或专门化学习方法，导致策略脆弱且难以适应新型或自适应攻击，网络防御需自动化决策以应对隐蔽、欺骗性和持续演化的对手。

Method: 构建了多智能体 Gym 环境 PoolFlip 扩展 FlipIt 博弈；提出 Flip-PSRO：基于种群训练的多智能体强化学习框架，使用种群对抗训练和所有权驱动的效用函数来培养通用的防御策略。

Result: 实证结果显示 Flip-PSRO 在对抗未暴露的启发式攻击时防御效果约为基线的 2 倍；所有权效用函数帮助防御者在控制率和性能之间取得平衡。

Conclusion: 提出的 PoolFlip 环境和 Flip-PSRO 方法能显著提升防御方对未知、自适应攻击者的泛化能力；实验证明 Flip-PSRO 防御者在未见启发式攻击上表现约为基线的 2 倍，并通过所有权（ownership）为核心的效用函数在保持系统控制率的同时优化性能。

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [91] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: 用可执行Python策略+LLM自我改进可在Atari上实现样本高效、训练时间短且具可解释性的强策略，值得进一步验证和扩展。


<details>
  <summary>Details</summary>
Motivation: 减少深度RL在交互样本和训练时间上的高昂代价，利用LLM的符号化、长程推理与自然语言能力，得到更可解释且易于迭代的策略表示。

Method: 将策略表示为以当前观测为输入、生成动作的Python程序；通过执行轨迹和自然语言反馈驱动的自我改进循环，使用LLM生成或修正代码，从而进行策略优化。

Result: 在Atari游戏上，基于Python程序的游戏代理在表现上与深度RL基线相当，同时显著减少训练时间和环境交互次数，表明方法在效率方面具优势。

Conclusion: 使用可执行的程序化策略并借助大型语言模型进行迭代生成优化，能在样本效率和训练时间上对抗深度强化学习基线，展示了程序化表示在复杂长时序推理任务中的潜力。

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [92] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: 将SISA引入多模态时空数据：共享嵌入+相似度分片+增量训练，实现可扩展且精确的机器退学，实验验证优于随机分片。


<details>
  <summary>Details</summary>
Motivation: 移动出行平台积累大量GPS轨迹、时序元数据和自由文本等异构数据，法规（如GDPR）要求能按需删除个人贡献，但对每次请求从头重训深度模型成本不可接受，故需一种可扩展且合规的机器退学方案。

Method: 将每条行程的数值与文本特征嵌入到共享潜在空间，使用相似度感知聚类将样本分配到分片以保证删除仅影响单一模型同时保持分片间多样性；每个分片进行增量训练，推理时聚合各分片预测；删除请求触发仅对受影响分片从最近有效检查点重训以实现精确unlearning。

Result: 在十个月的真实移动日志上，MobText-SISA在维持基线预测准确性的同时，较随机分片在误差和收敛速度上均表现更好，证明其实用性和有效性。

Conclusion: 提出MobText-SISA，一种将SISA扩展到异构时空-文本移动数据的机器退学框架：通过相似度感知分片与共享嵌入实现精确删除，只需重训受影响分片。实验证明在真实十个月移动日志上保持基线精度并优于随机分片。

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [93] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: LLMs can predict well but are fragile: trivial, task-irrelevant changes (e.g., renaming variables or reordering prompts) can severely alter predictions. This fragility appears across models and training modes and is partially explained by biased attention patterns; even tabular-specialist models are not immune.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used as plug-and-play predictors for tabular data, it is critical to understand their reliability and robustness to superficial, task-irrelevant changes in data representation.

Method: Empirical evaluation across multiple LLMs and settings: in-context learning and supervised fine-tuning; experiments with both closed-weight and open-weight general-purpose LLMs; probing attention patterns of an open-weight LLM to explain positional non-uniformity; and testing a tabular foundation model (TabPFN) for robustness. Variations considered include changing variable names and prompt/variable positions.

Result: Changing variable names or prompt positioning can change prediction error dramatically (up to ~82% in some settings). Attention analysis reveals non-uniform attention over prompt positions, explaining some sensitivity. TabPFN, despite design goals for robustness, is also susceptible to representation changes. The overall result is that LLMs lack robustness to trivial input variations.

Conclusion: LLMs, despite strong predictive performance on tabular tasks, are highly sensitive to task-irrelevant representation changes (e.g., variable names, prompt ordering), which can drastically alter predictions and error. This sensitivity exists across in-context learning and fine-tuning, for both closed- and open-weight LLMs, and even affects specialized models like TabPFN. Thus, current LLMs lack basic robustness required for principled data-fitting.

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [94] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: 本文提出Bi-LoRA，一种在参数高效微调（如LoRA）中集成Sharpness-Aware Minimization（SAM）的双模块方法：主LoRA模块用于常规任务适配，辅助LoRA模块用于建模SAM的对抗扰动，从而在受限子空间外捕获更广的loss平坦性。该方法同时允许并行优化与扰动，避免了SAM的训练开销翻倍，并在多任务与多架构上验证了其有效性与高效性。


<details>
  <summary>Details</summary>
Motivation: 直接将SAM应用于LoRA参数会把扰动限制在低秩子空间，难以充分探索模型权重空间以寻找更平坦的极小值；因此需要一种同时保持参数高效微调优点又能施加更广泛扰动的方案。

Method: 在标准LoRA基础上新增一个辅助低秩模块来建模SAM的上升（对抗）方向：主模块用梯度下降更新以适配任务，辅助模块以梯度上升捕捉损失的尖锐方向；两者并行优化，使得SAM扰动不再受限于LoRA子空间且避免了双倍训练开销。

Result: 实验证明Bi-LoRA在多种任务与架构上都能在不显著增加内存开销下提升泛化性能，并通过并行优化消除了SAM原本的训练成本翻倍问题。

Conclusion: Bi-LoRA通过引入辅助LoRA模块，有效地解耦了SAM扰动与任务微调，使得在低秩参数化下也能搜索更广的平坦区域，提升了模型在有限数据下的泛化能力，同时保持内存高效并减少训练开销。

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [95] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: 提出一种用于RLHF的反事实奖励模型，通过多模态表示学习引入因果推断，构建了由四部分组成的Counterfactual Trust Score以检测并缓解数据集中的偏见和分布漂移。该方法在一个多模态真假新闻数据集上取得89.12%准确率，减少了虚假相关和不公平的强化信号。


<details>
  <summary>Details</summary>
Motivation: RLHF中的奖励模型易学习并放大多模态数据中的潜在偏见，导致奖励信号有缺陷和政策不公平。被动约束通常在因果混淆下失效，因此需要一个无监督、对偏见具有鲁棒性的奖励信号。

Method: 提出将因果推断融入多模态表示学习，构造四项组成的Counterfactual Trust Score：反事实位移分解政治框架与主题偏差；反事实扰动下的重构不确定性；对受保护属性违反公平规则的检测；以及与动态信任度量对齐的时间奖励漂移。通过在含框架偏差、类别不平衡和分布漂移的数据集上进行评估，并注入顺序批次的合成偏差进行鲁棒性测试。

Result: 在多模态真假新闻检测任务上，系统达成89.12%准确率，优于基线奖励模型，并且显著减少了虚假相关性和不公平的强化信号，支持可调节的偏见降阈值和实时策略制定的可靠性。

Conclusion: 反事实奖励模型与Counterfactual Trust Score能在动态多模态环境中提供更稳健、可解释的奖励信号，减少偏见放大并提升假新闻检测准确率，为公平的RLHF提供可调节阈值和实时决策的可靠性。

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [96] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: 本教程系统介绍利用生成模型生成合成数据的理论、方法、评估与应用，目标帮助研究者和工程师在数据受限场景下有效使用合成数据。


<details>
  <summary>Details</summary>
Motivation: 解决数据挖掘中因数据稀缺、隐私限制与人工标注成本高导致的挑战，提供可扩展的合成数据解决方案以推动研究与工业应用。

Method: 介绍生成式合成数据的基础与最新进展，涵盖关键方法论（不同生成模型与训练/微调技巧）、实用框架（数据合成流水线、隐私保护机制）以及评估策略与具体应用案例。

Result: 参会者将获得可落地的实践性见解，学会如何选择/构建合成数据生成方法、如何评估合成数据质量及在下游任务中的应用，教程资源已在网站公布。

Conclusion: 本教程认为：生成模型（LLMs、扩散模型、GANs）生成的合成数据可有效缓解数据稀缺、隐私与标注成本问题，但需通过合适的方法、框架与评估策略保证质量与下游可用性。

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [97] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: 提出SyReM：通过内存损失增量约束维持稳定性，并用基于损失梯度余弦相似度的选择性重放提高可塑性，在在线连续学习下显著缓解遗忘并提高预测性能。


<details>
  <summary>Details</summary>
Motivation: DNN在运动预测中表现优异，但在持续学习过程中存在灾难性遗忘问题。现有CL方法多偏重记忆稳定性，从而损害学习新知识的可塑性。论文旨在解决稳定性-可塑性两难，既保留过去场景知识又高效学习新场景。

Method: 构建紧凑的记忆缓冲区；引入对内存平均损失增量的约束（不等式）以限制模型对已学知识的性能下降；在线计算样本的损失梯度余弦相似度并根据相似度从缓冲区选择最相关的样本进行重放，以提升对新数据的适应能力；在一遍式（one-pass）在线CL范式下训练并评估。

Result: 在INTERACTION数据集上的11个驾驶场景子集实验表明，与非CL和现有CL基线相比，SyReM显著降低了对过去场景的遗忘并在新场景上取得更好的预测精度。实现代码已开源。

Conclusion: 该论文提出了名为SyReM的连续学习方法，通过在内存缓冲区上施加损失增量不等式约束以保障记忆稳定性，并设计基于损失梯度余弦相似度的选择性记忆重放以提升学习可塑性。实验证明，在在线连续学习设置下，SyReM在11个驾驶数据集上可同时缓解遗忘并提升新场景预测性能。

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [98] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: 用特征归因差分来解释模型版本间的变化，设计了多维质量度量并以快速遮挡实现，实验证明可区分实质性归纳偏置变更和无害微调，便于轻量化更新审计。


<details>
  <summary>Details</summary>
Motivation: 模型更新（超参、内核、深度、求解器或数据）会改变表现，但常常无法解释“为什么”变化发生。需要一种轻量、模型无关的方法来解释版本间的具体依赖/行为变化。

Method: 提出通用框架 Δ-Attribution：计算两个版本的 per-feature 归因差分并用一套质量评估（L1、Top-k、entropy、rank-overlap、JSD、DCE、BAC、COΔF 等）评估。实现上使用标准化空间的快速遮挡/钳位（occlusion/clamping）、类锚定边际和基线平均化。实证审计了45种设置（5类模型、3数据集、3对A/B）。

Result: 在实验证明：改变归纳偏置（例如核从 poly→rbf 或随机森林的特征规则变更）导致显著且与行为一致的Δ；而“化妆式”微调（gamma=scale vs auto，kNN检索策略）对归因排名和行为几乎无影响。深层 GB 导致最大特征重新分布（JSD≈0.357）。总体上 Δ-Attribution 可补充准确率指标，帮助识别风险性的依赖转移。

Conclusion: Delta-Attribution 能有效揭示两个模型版本之间按特征的行为变化，通过对特征归因的差分（Δφ=φ_B−φ_A）将更新的影响可视化并量化，能够区分表面性能变动与真实的行为/依赖转移。

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [99] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: 提出Dual-LS：一种任务无关、在线的持续学习方法，利用双重协同记忆回放机制平衡长短期记忆，显著减少灾难性遗忘并降低计算资源消耗，适用于大规模真实车辆运动预测场景。


<details>
  <summary>Details</summary>
Motivation: 现有通过扩充训练集或重放历史数据的解决方案成本高、样本效率低且难以平衡长期与短期经验，不能实现类人化持续学习，故提出Dual-LS以改进这一问题。

Method: 设计了两种互补的记忆回放机制（长期和短期），在线运行、无任务边界，通过动态协调两类记忆的检索与蒸馏来维持旧知识，同时学习新经验，测试于跨国大型自然数据集，评估遗忘率和资源消耗。

Result: Dual-LS提出了一种受人脑互补学习系统启发的在线、无任务边界的持续学习范式，旨在解决DNN在车辆轨迹预测中出现的灾难性遗忘问题。通过结合两种协同记忆回放机制，加速经验检索并在动态协调长期和短期知识表示方面取得平衡。实验基于三个国家的自然数据，样本覆盖77.2万车辆、累计测试里程11187公里，结果显示Dual-LS在缓解灾难性遗忘上最多可达74.31%、计算资源需求最多可降低94.02%，提高了预测稳定性并保持了数据需求不变，从而为智能城市中的DNN车辆运动预测提供了计算高效且类人化的持续学习能力。

Conclusion: Dual-LS有效缓解了车辆运动预测中DNN的灾难性遗忘，同时大幅降低计算成本，实现了类人化的持续学习适配，适合智能城市部署。

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [100] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: 提出RLTR，通过解耦训练与以工具使用完整性为奖励的强化学习，对规划模块进行单目标优化，从而显著提升代理的规划能力与最终响应质量。


<details>
  <summary>Details</summary>
Motivation: 现有端到端多目标训练分散了对规划能力的优化且缺乏可验证数据，导致难以提升代理的行动规划表现；因此需要一种直接、可量化且不依赖最终答案核验的数据驱动信号来优化规划。

Method: 提出RLTR框架：先将训练过程拆分，针对规划子模块单独进行强化学习优化，并以“工具调用序列的完整性”作为奖励信号来评估规划质量，避免依赖可验证的最终响应数据。

Result: 在实验中，RLTR使规划性能提升8%至12%，并使整体代理最终响应质量提升约5%至6%，证明了解耦训练与工具使用完整性奖励的有效性。

Conclusion: 作者提出将代理的规划能力与摘要能力解耦，通过单目标优化规划模块并引入基于工具使用完整性的奖励，从而更直接地训练行动规划能力。该方法在规划性能和最终响应质量上均优于端到端基线。

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [101] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: FinCast：首个针对金融时序的基础模型，训练于大规模多域多分辨率金融数据，实现强零-shot性能并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 金融时序预测受制于时间非平稳性、多域差异和时间分辨率多样性，传统深度学习方法易过拟合且需大量领域微调，因此需要一个能泛化到多域多分辨率的基础模型。

Method: 基于大规模金融时序数据训练的单一基础模型，可能使用深度学习架构（如Transformer类时间序列模型），通过覆盖多域、多分辨率数据来学习通用模式，减少对领域特定微调的依赖。

Result: 在广泛的定量和定性评估中，FinCast在零-shot和少量微调场景下均超过现有最先进方法，表明其在捕捉多样化金融模式方面具有优越性。

Conclusion: FinCast是一种面向金融时序预测的基础模型，能够在大规模金融数据上训练并实现强大的零-shot泛化能力，优于现有方法。

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [102] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: 在logit空间用可学习锚点和影响函数建模logit聚合与分布，可更准确、稳健地估计无标签目标域的模型准确率，优于softmax和相似性方法。


<details>
  <summary>Details</summary>
Motivation: 在无标签、未见目标域中估计模型准确率对实际部署至关重要。现有方法要么依赖softmax概率（造成信息损失），要么依赖域/任务专用且计算昂贵的相似性度量，限制了准确率估计的稳健性与通用性。

Method: 在logit空间初始化多个可学习锚点（anchors），为每个锚点设计影响函数以捕捉logit分布的细微变化。基于对logit聚合和分布与预测性能相关性的理论与实证观察，训练模型（或回归器）将锚点响应映射到整体准确率估计，避免将logits压缩到概率单纯形或依赖昂贵的相似性度量。

Result: 在视觉、语言和图数据集上的大量实验表明，ALSA在不同程度的分布偏移下均比软最大化（softmax）和相似性基线方法表现更好，显示出更高的估计精度与对强偏移的鲁棒性。

Conclusion: ALSA通过在logit空间使用可学习锚点及其影响函数，有效保留并利用了比softmax概率更丰富的预测信息，从而能够更稳健、准确地估计未标注目标数据集上的模型性能，尤其在显著分布偏移下优于基于softmax和相似性的方法。

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [103] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: 提出一种基于视觉提示的贝叶斯实例级pFL框架，通过隐式后验和半隐式变分推断实现细粒度个性化，在多种异质性场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统pFL方法假设每个客户端数据来自单一分布并为每个客户端学习单一个性化模型，但在现实中单个客户端往往包含来自多个源/领域的数据，存在显著的客户端内异质性，需更细粒度的个性化策略。

Method: 基于视觉提示微调，提出实例级提示生成的贝叶斯框架；将提示后验建模为隐式分布并利用半隐式变分推断推导训练目标；在FL设置下进行分布式训练以学习每个样本对应的提示。

Result: 在基准数据集上，pFedBayesPT在特征异质性和标签异质性设置下均显著优于现有pFL方法，验证了其在处理实例级语义多样性方面的有效性。

Conclusion: pFedBayesPT通过对实例级提示生成进行贝叶斯建模，并将提示后验建模为隐式分布，结合半隐式变分推断，能更好地处理单一客户端内多源导致的内部异质性，从而提升个性化联邦学习性能。

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [104] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: 提出SCAR（尺度、覆盖、真实性、丰富度）四项不变结构性度量，定义Foundation Data用于保持整体泛化行为，建模阶梯型泛化偏差并据此进行模态感知的数据补全，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: Current data-centric methods lack theoretical insight into how structural data properties affect generalization, especially under scaling; need invariant measures beyond quantity for multimodal datasets.

Method: Introduce SCAR and Foundation Data approach

Result: Defined four invariant measures (Scale, Coverage, Authenticity, Richness); introduced Foundation Data to preserve generalization without retraining; modeled single-modality tasks as step functions to estimate foundation data size distribution; developed SCAR-guided modality-aware data completion; validated across multimodal datasets and architectures.

Conclusion: SCAR为理解和预测数据效用提供了稳健的结构化框架，能指导高效的数据收集与扩展，特别适用于多模态场景。

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [105] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: 本文首创性地系统探讨了柔性电子上低功耗应激分类器的设计空间，构建1200+个定制低精度电路实现的分类器，兼顾准确性、成本、柔性和功耗


<details>
  <summary>Details</summary>
Motivation: 提供连续、可负担的应激监测，以替代片段化、以症状为中心的干预和僵硬的硅基可穿戴设备

Method: 枚举多种机器学习分类器、特征选择策略和神经网络简化算法，针对每种配置设计定制化低精度算术电路，并在柔性电子实现约1200个分类器进行评估和比较

Result: 设计并探索了超过1200种低功耗、柔性应激分类器，采用多种ML分类器、特征选择和神经简化算法，并为每种情况定制低精度算术电路，实现比现有方法更高的准确率且成本低、可弯曲、低功耗和体积小

Conclusion: 通过系统性的设计空间探索和硬件定制，证明在柔性电子上可以实现实时、高精度且低功耗的应激分类器，为可穿戴连续应激监测提供可行路径

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [106] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: The paper proves that smooth functions can be approximated in C^1 by rational functions and rational neural networks, giving explicit rates vs network width/depth and rational degree; applies to EQL^div and ParFam used in symbolic regression.


<details>
  <summary>Details</summary>
Motivation: Provide rigorous approximation results in C^1-norm using rational functions and rational neural networks, to support symbolic regression and physical law learning architectures like EQL^div and ParFam.

Method: Constructive approximation using rational functions and building corresponding rational neural network architectures; derive error bounds in C^1-norm relating network depth/width and rational degree to approximation error; adapt constructions to EQL^div and ParFam architectures.

Result: Proved C^1-approximation of suitably regular functions by rational functions and rational neural networks; derived approximation rates in terms of width, depth, and degree; obtained C^1-approximation for EQL^div and ParFam rational network architectures.

Conclusion: Rational functions and rational neural networks can approximate regular functions in C^1 with quantifiable rates; specific architectures (EQL^div, ParFam) inherit these approximation guarantees, benefiting symbolic regression for discovering physical laws.

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [107] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: 为图上的步行（序列）建立加权度量，研究其度量性质并引入近似度（proximities）的表示与构造，利用Lipschitz扩展工具延拓近似度，应用于近似度估计和基于探索性步行的强化学习与Lipschitz回归。


<details>
  <summary>Details</summary>
Motivation: 研究图上步行的度量结构以量化序列间的相对距离，提供分析工具（如Lipschitz扩展）以构造和延拓更弱的相似/距离量度，推动近似度估计与基于步行的强化学习等应用。

Method: 定义基于逐步顶点距离与加权范数的序列度量，分析度量空间基本性质；引入proximities的表示公式与显式构造，在不同假设下给出表示；利用Lipschitz函数扩展定理将近似度从子空间延拓到整个空间，保持关键性质。

Result: Introduces a weighted metric for sequences (walks) on graphs, defines distances between walks via stepwise vertex distances and weighted norms; studies metric space properties; defines and represents proximities (weaker distance measures) with formulas and constructions under assumptions; uses Lipschitz function extension to extend proximities while preserving properties; suggests applications in proximity estimation and reinforcement learning via exploratory walks and Lipschitz regression on networks.

Conclusion: 本文构建了序列加权度量与近似度表示框架，并证明了可通过Lipschitz扩展从子空间延拓近似度，具备理论与潜在应用价值，如近似度估计与基于步行的强化学习。

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [108] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: 该论文提出Adam-PFN，一种针对Adam优化器超参数的预训练学习曲线代理模型，用于Freeze-thaw贝叶斯优化；并引入CDF-augment学习曲线扩充方法以增加训练样本。实验在TaskSet上显示对学习曲线外推与超参数调优加速有提升，并在OOD任务上表现良好。


<details>
  <summary>Details</summary>
Motivation: 传统Freeze-thaw BO使用通用代理，无法利用Adam超参数对学习过程的结构性知识；手动调参耗时且昂贵，因此作者希望通过预训练代理和数据扩充提高低预算超参调优效果。

Method: 作者使用TaskSet数据集的学习曲线预训练了一个预测学习率和动量等Adam超参数对学习曲线影响的概率性神经网络（PFN），并提出CDF-augment通过对学习曲线的经验分布函数进行变换生成额外训练样本。该PFN作为Freeze-thaw BO的替代代理用于学习曲线外推与早停决策。

Result: 在TaskSet评估任务上，Adam-PFN在学习曲线外推误差、超参数寻找速度和最终性能上优于基线Freeze-thaw BO和未预训练代理；在OOD任务也显示稳健性，证明CDF-augment提升了模型泛化能力。

Conclusion: Adam-PFN结合预训练的概率性后验网络与CDF-augment提高了Freeze-thaw BO在Adam超参数调优上的效率和鲁棒性，并在TaskSet和OOD任务中取得更好结果，展示了将先验学习曲线知识融入BO的潜力。

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [109] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: InfraredGP amplifies extended low-frequency components via negative Laplacian correction, uses untrained spectral GNN on random inputs to produce embeddings, then clusters with BIRCH for effective, fast graph partitioning.


<details>
  <summary>Details</summary>
Motivation: Graph partitioning benefits from capturing informative low-frequency graph signals; conventional Laplacian spectrum limited to [0,2] may miss useful info.

Method: Design spectral GNN backbone with low-pass filters and negative correction; feed random inputs; single feed-forward propagation to get embeddings; cluster with BIRCH; evaluate on static and streaming GP benchmarks.

Result: InfraredGP uses spectral GNN with low-pass filters and a negative correction to extend frequencies beyond [0,2], inputs random noise, performs one feed-forward pass without training, then clusters embeddings with BIRCH, achieving high-quality GP and fast runtime (16x-23x) on benchmarks.

Conclusion: Negative correction on graph Laplacian can reveal informative low-frequency features; untrained spectral filters + random inputs suffice to produce effective embeddings for clustering, enabling efficient, training-free graph partitioning.

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [110] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: Two-stage approach: 3D diffusion produces small voxel blocks; 3D inpainting stitches blocks with masked UNet inputs and repainting noise guidance for coherence, enabling fast, scalable granular media synthesis


<details>
  <summary>Details</summary>
Motivation: DEM initialization is slow due to large displacements and kinetic energy; need fast generation of physically realistic granular assemblies at scale

Method: Two-stage 3D diffusion + inpainting pipeline

Result: Generates large, physically coherent voxelized granular assemblies quickly; 1.2m rail track (3h DEM) synthesized in <20s; enables grain extraction for DEM compatibility; linear time scaling

Conclusion: Method provides real-time, scalable initialization for DEM by synthesizing physically realistic granular assemblies via diffusion-based generation and inpainting, dramatically reducing initialization time while retaining DEM compatibility

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [111] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: 本文提出EUREKA，通过大语言模型按“有趣性”选特征并构建可解释分类器，从而在兼顾可解释性与新颖性的前提下提供中等准确率但高洞察力的模型。


<details>
  <summary>Details</summary>
Motivation: 探索构建“有趣”的分类器，即优先使用不寻常或出人意料的特征，而非仅仅追求最高预测准确率。

Method: 先用大语言模型对候选特征生成并排序“有趣性”评分，然后基于排序选择若干有趣特征，构建简单可解释的分类器（例如线性模型或决策树），并在基准数据集上验证这些模型的准确性与解释性。

Result: 提出EUREKA框架：利用大型语言模型对特征进行“有趣性”排序，然后仅用这些被选特征构建可解释的分类器。在多个数据集上，EUREKA识别出非显而易见但仍具预测性的特征，如在Occupancy数据集中偏好湿度而非CO2或光强，在Twin Papers数据集中发现标题有冒号的论文更可能被引用。

Conclusion: EUREKA能发现非显而易见的预测性特征，支持新型知识发现与交流，适用于重视新颖性和可解释性的场景，即使牺牲部分准确率也可接受。

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [112] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: 提出PSO-Merging：用粒子群优化在预训练模型、专家模型和稀疏专家模型间搜索合并解，通过数据驱动的评价引导迭代，实验证明在语言模型合并上较基线更优且更高效。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法要么缺乏数据驱动导致性能不足，要么基于梯度的优化计算开销大、不可扩展，现有无梯度方法在有限步数下效果不佳，故提出一种高效且可扩展的数据驱动合并方法。

Method: 构建粒子群，粒子由预训练模型、专家模型及稀疏化专家模型初始化；通过多次迭代更新粒子，利用全局最优粒子作为最终合并模型；优化过程基于数据驱动的评价函数来引导搜索。

Result: 在不同规模的语言模型上进行实验，PSO-Merging在多数情况下优于基线合并方法，展示出更好的性能与可扩展性。

Conclusion: PSO-Merging通过引入粒子群优化（PSO）策略，有效地融合预训练模型与多个专家模型，克服了数据无关方法缺乏数据驱动指导的问题，并在计算效率与可扩展性上优于部分梯度基与无梯度基方法。

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [113] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: They design a convolutional autoencoder whose convolution and pooling layers are parameterized to be symplectic, and show it better models several Hamiltonian PDEs compared to linear symplectic decomposition baselines.


<details>
  <summary>Details</summary>
Motivation: Standard CNNs do not in general preserve symplectic (Hamiltonian) structure, which is important for long-term stable and physically consistent modeling of Hamiltonian PDEs. Embedding symplectic structure into CNNs aims to improve stability and accuracy for learning dynamical systems governed by Hamiltonian mechanics.

Method: Reformulate convolution layer into a mathematically equivalent form amenable to symplectic parameterization; use symplectic neural networks to parameterize convolutional layers so they preserve symplectic structure; design a symplectic pooling layer to complete a symplectic autoencoder; apply tensor techniques and proper symplectic decomposition for comparisons.

Result: Numerical experiments on the wave equation, nonlinear Schrödinger equation, and sine-Gordon equation show that the symplectic CNN outperforms linear symplectic autoencoders obtained via proper symplectic decomposition, indicating improved approximation of dynamics and preservation of structure.

Conclusion: The proposed symplectic CNN architecture successfully integrates symplectic structure into convolutional layers and pooling, yielding better performance on example PDEs than linear symplectic autoencoders based on proper symplectic decomposition.

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [114] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: 通過 FEM + 物理信息 DeepONet 的模塊化耦合，在保持流場精度的前提下，針對局部尖銳源實現了對對流擴散輸運問題的快速且精確近似，並以自適應採樣改善尖峰附近的預測性能。


<details>
  <summary>Details</summary>
Motivation: 傳統高精度數值求解對於多次查詢或不斷變化的源項代價高昂；希望保持流場的 FEM 精度同時通過學習型模型實現輸運問題的快速推理，特別是對具有尖鋭局部源的情形。

Method: 使用 FEM 解穩態 Darcy 流以獲得速度場，將速度場作為輸入傳遞給物理信息 DeepONet（源函數到濃度場的算子學習器），並採用針對 trunk 網格點的自適應採樣策略來處理尖銳梯度。

Result: 數值實驗表明方法與參考解吻合良好，且在推理速度上比傳統求解器快數個數量級；作者已公開實現代碼。

Conclusion: 提出的混合框架有效结合了 FEM 与物理信息 DeepONet，使得在保持流場精度的同時，能對輸運動力學進行快速推理，對尖銳高斯源的處理表現良好。

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [115] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: 证明并实证：量子生成的潜在分布在特定情形下能显著扩展生成模型的表达与性能，近端光子量子设备已能在实验证据中提供优势。


<details>
  <summary>Details</summary>
Motivation: 传统生成模型常用简单的低维潜在分布，但更复杂的潜在分布可能提升性能。已有工作尝试用量子处理器生成潜在分布，但何时能带来、能否复现量子优势仍不明，因此本工作旨在证明并实验验证这些优势。

Method: 理论上证明量子潜在分布在一定假设下带来不可由经典潜在分布有效模拟的表达能力；并提供判别何时会出现优势的直观准则。实验上在合成量子数据集和QM9分子数据集上，用模拟与真实光子量子处理器对GAN、并探讨扩展到扩散和流匹配模型，比较多种经典基线。

Result: 给出在复杂性假设下的理论不可模拟性结果；在合成数据与QM9上实验证明量子潜在分布可以在GAN中超过多种经典基线；识别出与量子潜在分布兼容的扩散与流匹配架构。

Conclusion: 在特定条件下，量子设备生成的潜在分布可使生成模型产生经典潜在分布无法高效地产生的数据分布，从而扩展深度生成模型的能力。近端量子处理器在实证和理论上均能带来优势。

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [116] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: 提出无参数的SDGNN，通过结构与特征驱动的互补消息传递捕获结构多样性，提升对异质图数据的适应性，在多项基准下表现优异并已开源实现。


<details>
  <summary>Details</summary>
Motivation: 现有主流GNN依赖大量可训练参数和固定聚合规则，难以适应具有强结构异质性与复杂特征分布的图数据，容易导致节点表示过平滑和语义退化，需寻找不依赖参数且能保持结构多样性的方法。

Method: 提出统一的结构多样性消息传递机制，无参数设计，结合结构驱动与特征驱动的互补建模来同时捕获邻域结构异质性与特征语义稳定性，避免复杂训练并提升泛化能力。

Result: 在八个公共基准数据集和一个跨学科的PubMed引用网络上进行实验，SDGNN在低监督、类别不平衡和跨域迁移等挑战性场景中持续优于主流GNN，且实现已开源。

Conclusion: 本文提出了无参数的结构多样性图神经网络框架SDGNN，通过结构驱动与特征驱动的互补建模，在不引入额外可训练参数的情况下提升了对结构异质性和复杂特征分布图数据的适应性，缓解了过平滑与语义退化问题。实验显示SDGNN在低监督、类别不平衡和跨域迁移等困难场景下优于主流GNN，验证了结构多样性作为图表示学习核心信号的重要性。

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [117] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: NM-Hebb: two-phase training; Phase1 adds Hebbian regulariser and neuromodulated consolidation to supervised loss; Phase2 metric-learning fine-tuning; improves accuracy, cluster quality, and interpretability across five backbones


<details>
  <summary>Details</summary>
Motivation: Improve CNN generalisation, interpretability and reduce overfitting by integrating neuro-inspired local plasticity and distance-aware supervision

Method: Two-phase: Phase1 joint CE + Hebbian alignment (activation mean with filter mean) + learnable neuromodulator gating elastic-weight consolidation; Phase2 pairwise metric-learning to compress intra-class and enlarge inter-class distances

Result: NM-Hebb yields consistent accuracy gains across CIFAR-10/100 and TinyImageNet and better feature structure and NMI

Conclusion: Combining Hebbian plasticity with metric fine-tuning produces more accurate, structured, and interpretable CNNs suitable for constrained/safety-critical settings

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [118] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: ASPC通过自适应缩放策略约束，在训练中自动平衡强化学习与行为克隆，用单一超参数配置在多个数据集上稳定优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法需对约束强度进行逐数据集调参，耗时且不实用，故希望一个能自动适应不同数据质量和任务尺度的约束机制

Method: 构建二阶可微的自适应缩放因子，通过最小化RL目标与BC目标之间的二阶导数/范数差异动态调节约束强度；在训练时同时优化策略和缩放因子

Result: ASPC提出一种动态图约束平衡RL和BC的二阶可微框架，解决了离线RL中超参难调的问题

Conclusion: ASPC能在无需为每个数据集调参的情况下，提供可证明的性能提升并在D4RL上的39个数据集实验中表现优异，且计算开销小

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [119] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: Use gradients in input embedding space to cheaply approximate model outputs for many sampled demonstration subsets, aggregate influence scores, and select top-k examples, yielding accurate, fast, and better demonstrations than embedding-similarity baselines.


<details>
  <summary>Details</summary>
Motivation: Improve selection of demonstration examples for in-context learning by better estimating each example's influence on model outputs, enabling efficient subset selection for prompting and chain-of-thought reasoning.

Method: Compute model outputs and gradients once for all candidate examples; use first-order Taylor approximation to estimate outputs for many randomly sampled demonstration subsets; aggregate these estimates into influence scores per example; select top-k examples by score.

Result: A linear-time algorithm that uses input-embedding-space gradients and first-order approximations to estimate model outputs for sampled subsets, aggregates influence scores, and selects top-k demonstrations; achieves <1% approximation error and up to 37.7x speedup, outperforming embedding-based methods by ~11% on average across datasets and models up to 34B parameters.

Conclusion: Gradient-based first-order approximation enables accurate and scalable subset selection for in-context learning, offering substantial speedups and improved performance over embedding-similarity methods.

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [120] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: 针对 SBG 链路符号预测，作者提出了一种基于 Gegenbauer 多项式的符号感知谱卷积网络，通过谱初始化、专用滤波器与交替正负边卷积，实验证明能显著提高预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多数工作针对无符号单部图而设计的谱算子无法有效处理有正负链接的二分图，忽视节点分区差异与正负边的不同传播机制，导致预测性能受限。

Method: 提出三大技术：1) 快速且有理论支撑的谱分解用于节点特征初始化；2) 基于 Gegenbauer（根 Gegenbauer）多项式基的新型谱图滤波器；3) 多层符号感知谱卷积网络，交替应用针对正负边的 Gegenbauer 多项式滤波器。

Result: 在 6 个基准 SBG 数据集上与 11 个强对手比较，GegenNet 在 AUC 上最高提升 4.28%，在 F1 上最高提升 11.69%，表现稳定且优异。

Conclusion: GegenNet 是为有符号二分图（SBG）上的链路符号预测设计的谱卷积神经网络，能够更好地处理节点异质性与二分特性，实验证明在多数据集上显著优于多种强基线。

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [121] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: 多模态（文本+图像+CLIP）层次化分类结合动态掩码和自监督聚类，在大规模跨平台电商数据上显著提升分类精度与细粒度识别，并实现了可部署的两阶段推理方案。


<details>
  <summary>Details</summary>
Motivation: 解决真实工业场景中平台间描述差异与现有类目体系过于粗糙或不一致的问题，提升商品分类的准确性和可扩展性，便于下游交易情报及分析。

Method: 利用271,700件来自40个时尚电商平台的产品数据，融合文本（RoBERTa）、视觉（ViT）和联合视觉-语言（CLIP）特征；比较早期融合、晚期融合和基于注意力的融合策略；在层次化架构中引入动态掩码以保证分类的一致性；针对浅层或不一致类别，设计自监督的产品重新分类管道（SimCLR+UMAP+级联聚类）；并实现两阶段推理以兼顾成本与准确率。

Result: CLIP 嵌入配合基于 MLP 的晚期融合获得最高的层次 F1（98.59%），自监督重新分类发现了细粒度子类（例如鞋类子类型），聚类纯度超过86%；跨平台实验表明复杂的晚期融合在训练平台多样时准确率更高，而早期融合在未见平台上泛化更好；在 EURWEB 商业平台上线，采用两阶段推理实现了工业级可扩展性与成本效益。

Conclusion: 该论文提出并部署了一个多模态层次化分类框架，成功解决了电商平台异构性和层级分类结构的局限性，在工业环境中可扩展且表现优异。

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [122] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: 提出基于UMLS的解释性报告相似度方法，通过实体抽取和加权Tversky相似度替代嵌入检索，在MIMIC-CXR长尾任务上优于CLIP/CXR-BERT，且生成本体支持标签。


<details>
  <summary>Details</summary>
Motivation: Improve retrieval-augmented learning for long-tail radiology tasks by using interpretable, ontology-driven report comparisons rather than opaque high-dimensional embeddings.

Method: 从放射科报告用RadGraph-XL与SapBERT抽取实体并链接至UMLS CUIs，表示为集合；定义考虑同义词、否定与层次关系的加权Tversky相似度进行检索；用于分类评估并生成本体支持标签。

Result: An ontology-based pipeline extracting UMLS CUIs via RadGraph-XL and SapBERT, plus a Tversky-index-based similarity measure that handles synonymy, negation, hierarchy. Outperforms embedding methods on MIMIC-CXR long-tail classification and provides ontology-backed labels; code released.

Conclusion: Ontology-driven, interpretable retrieval yields better performance and label resources for long-tail radiograph tasks; beneficial when domain knowledge and interpretability are needed.

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [123] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: 提出一个结合分布变化检测与自然语言序参的框架，用统计不相似性量化微调中发生的快速相变，并分解相变对对齐、冗长等方面的影响，发现行为变化晚于梯度峰值并能自动发现语言序参。


<details>
  <summary>Details</summary>
Motivation: 理解何时与如何在狭窄有害数据上微调会引起广泛的行为失配，以便自动发现并量化语言类序参，从而在不同任务（知识、政治、伦理）上演示框架。

Method: 提出一个综合框架，结合分布式变化检测(statistical dissimilarity measure)与用自然语言描述并由LLM评判的序参(order parameters)，来检测与刻画微调过程中出现的快速相变。并通过对模型输出分布的统计不相似性量化相变对模型多方面的影响，分解成对齐、冗长性等因素的贡献。

Result: 发现：1) 使用统计不相似性度量能量化相变对输出分布的影响并将其分解到不同方面；2) 行为上的突变发生时间晚于仅靠梯度范数峰值所提示的时间；3) 框架能自动发现并量化语言基础的序参，在知识、政治、伦理等示例上有效。

Conclusion: 待检测的细微有害数据集微调会导致LLM行为大幅偏离人类价值观，表现为训练中出现的快速相变。

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [124] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer: a BERT-based, domain-aware pre-training model for network traffic that segments flows, aligns protocol stacks in embeddings, and uses field/context-aware pretraining to achieve better classification, few-shot learning, and domain-aligned understanding.


<details>
  <summary>Details</summary>
Motivation: Existing pre-training methods for traffic classification fail to fully capture packet structural characteristics, flow-level behaviors, hierarchical protocol semantics, and inter-packet contextual relationships; a domain-aware pre-trained model can remedy these gaps and yield more robust traffic analysis.

Method: Introduces three key components: (1) Coherent Behavior-Aware Traffic Representation Model for segmenting flows into coherent 'flowlets'; (2) Protocol Stack Alignment-Based Embedding Layer to represent multilayer protocol semantics; (3) Field-Specific and Context-Aware Pretraining Tasks to learn inter-packet and inter-flow relationships within a BERT framework.

Result: Experimental evaluation shows FlowletFormer outperforms prior methods in representation effectiveness, classification accuracy, and few-shot capabilities; it also demonstrates improved interpretability with respect to network transmission principles.

Conclusion: FlowletFormer is a BERT-style pre-training model tailored to network traffic analysis that segments traffic into semantically meaningful units, encodes multilayer protocol semantics, and uses field-specific/context-aware pretraining tasks; it improves representation quality, classification accuracy, and few-shot learning, and better captures network transmission principles like TCP statefulness.

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [125] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: 提出基于逆动态博弈的MILP方法，通过编码KKT条件从Nash均衡示例中恢复参数化约束，并用于鲁棒运动规划；理论保证可学习内部近似，实验证明在多种约束和非线性动力学下有效。


<details>
  <summary>Details</summary>
Motivation: 许多多智能体交互场景下，约束（安全/不安全区域）未知但影响决策；通过从观测到的Nash均衡交互演示中逆向学习这些约束，可提高运动规划和安全保证的可靠性。

Method: 将多智能体交互问题的KKT必要条件编码为混合整数线性规划（MILP），通过求解MILP从演示数据中识别参数化的约束集合，并将识别出的约束用于约束感知的运动规划。

Result: The paper presents an algorithm to infer parametric interaction constraints from demonstrations of local generalized Nash equilibria by encoding KKT conditions into MILPs, and applies the recovered constraints to robust motion planning; empirical validation on simulations and hardware shows capability across convex/non-convex constraints and nonlinear dynamics.

Conclusion: 方法能从Nash均衡演示中恢复与演示一致的约束集，理论上得到安全/不安全集合的内部近似并指出可学习性的局限性，且所恢复约束可用于设计满足真实约束的鲁棒交互运动规划。

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [126] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony is a decentralized LLM agent system using ledger, Beacon selection, weighted CoT voting enabling lightweight GPUs to coordinate, giving better accuracy and robustness


<details>
  <summary>Details</summary>
Motivation: decentralize orchestration to reduce cost, increase flexibility and privacy

Method: analysis of method

Result: Symphony outperforms baselines on reasoning benchmarks with accuracy gains and robustness across model sizes

Conclusion: Decentralized orchestration with ledger, dynamic beacon, and weighted CoT voting offers scalable, privacy-preserving, fault-tolerant coordination improving reasoning performance

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [127] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: 本文提出了全局置换熵（GPE），扩展了传统置换熵只考虑连续片段排列的局限，纳入所有可能的长度为L的非连续与连续模式，通过新算法高效提取全排列分布并计算熵。实验表明GPE能揭示标准置换熵无法捕捉的结构信息，并提供了Julia实现。


<details>
  <summary>Details</summary>
Motivation: 传统置换熵仅基于连续片段的相对顺序，可能遗漏跨不相邻索引的结构性模式；为获得更全面的序列结构表征，引入考虑所有可能位置组合的全局模式。

Method: 提出GPE定义：统计时间序列中所有长度L子序列（不限相邻）对应的排列类型频率，应用Shannon熵计算复杂度；使用最新高效算法生成全排列轮廓以降低计算复杂度；实现了Julia包。

Result: 在合成数据集上的实验显示，GPE能识别出标准置换熵无法区分的序列差异，说明其在揭示隐藏结构方面更灵敏，并提供相应的计算工具。

Conclusion: GPE能补充和增强标准置换熵的表征能力，通过考虑非连续模式捕获额外的结构信息，且借助新算法在计算上是可行的。

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [128] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: 提出基于统计特征与SMOTE、随机森林/XGBoost的短期泵故障预警框架，在不同回溯窗口下对5/15/30分钟提前预警进行评估，随机森林总体表现更好，最佳回溯窗口随预警时长变化。


<details>
  <summary>Details</summary>
Motivation: Enable early warning of centrifugal pump faults using real-time sensor data to support predictive maintenance and reduce downtime.

Method: Statistical-feature-based ML for short-term pump fault forecasting

Result: Random Forest outperformed XGBoost; best recall with 60-min window at short horizon, but 120-min window improved performance for longer horizons; recall ranged 48.6%–69.2%.

Conclusion: 方法可解释、可扩展，适合实时工业监控中引入预测性维护；历史长度需根据预测时长选择，故障模式在不同时间尺度演化。

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [129] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: 引入一種實用的概率估計+Hungarian匹配的協調策略（Cord-Approx），在馬德里真實資料驅動仿真中將使用者尋車時間平均從約20分鐘降至約7分鐘，區域性改善達70%上下。


<details>
  <summary>Details</summary>
Motivation: 在都市中尋找路邊停車增加交通阻塞，研究如何通過行動應用在不同程度的用戶協調與資訊可得性下，降低尋車時間與提升尋到車位的機率。

Method: 基於馬德里街道停車生態的高保真數據驅動仿真，對比四種策略（Unc-Agn、Cord-Agn、Cord-Oracle、Cord-Approx）。Cord-Approx 使用歷史占用分佈來估計非系統用戶行為，伸長系統用戶到不同可用泊位間的“物理距離”，再透過 Hungarian 配對算法調度分配。

Result: 在以真實交通資料的高保真仿真中，Cord-Approx 使用者平均尋車時間為6.69分鐘，非應用使用者為19.98分鐘。區域層級結果顯示，Cord-Approx 在中心地帶與住宅區分別能減少約67–76%與高達73%的尋車時間。

Conclusion: 提出的 Cord-Approx 策略在仿真中显著降低了使用者的尋車時間，接近理想化協調（Cord-Oracle）效果，並在多數地區將尋車時間相較於不使用應用者減少約70%以上。

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [130] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: 本文通过PasswordEval基准证明：当前模型难以稳健处理基于上下文的授权/机密任务，且链式推理可能导致敏感信息泄露；需要在训练与部署上做出实质性改进以提升安全性。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型作为自主代理在高风险场景中部署，确保它们遵守用户定义规则（尤其是关于机密信息和授权）的能力成为关键安全问题，因此需要评估上下文相关的鲁棒性。

Method: 提出并使用了一个基准（PasswordEval）来测试模型在给定上下文（正确或错误密码）下判断授权请求的能力；通过加入对抗性越狱策略和多轮对话扩展难度；分析链式推理（reasoning traces）对性能和信息泄露的影响。

Result: 现有开源与闭源模型在此任务上表现欠佳，推理能力并不显著提升正确率，而且推理过程常常泄露敏感信息；模型在面对对抗性提示与长多轮对话时表现进一步下降。

Conclusion: Frontier language models currently lack contextual robustness for handling confidential, context-dependent authorization tasks; reasoning traces can leak secrets and may worsen safety.

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [131] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: 针对异构数据，提出在预训练中加入K步梯度局部最优的平衡约束并以双层优化求解（一阶近似），从而提升了模型对多域/多语言下游任务的适应能力，实验验证有效。


<details>
  <summary>Details</summary>
Motivation: 异构数据（多域/多语言）下简单混合并最小化平均损失会导致模型不能很好地适应每个数据源的局部结构与分布差异。为提升预训练模型在不同来源数据上的下游微调表现，需要设计使模型能迅速适配各源的预训练目标。

Method: 在常规将所有异构数据混合并最小化全局平均损失的做法上，增加了额外的平衡约束：对每个数据源，要求模型参数经过K步基于该源的梯度下降后达到该源的局部最优；将该要求表述为双层优化问题（外层优化初始模型参数，内层对应各源的K步梯度优化）；采用一阶近似（first-order approximation）方法以降低计算复杂度并求解该双层问题；并比较与MAML元学习框架的关系。

Result: 在多域和多语言自监督预训练设置上的实验显示，提出的方法在下游有监督微调任务中显著改善了适配性（比常规全局平均损失预训练有明显提升）。论文还讨论了与MAML的联系，表明方法在元学习语境下有解释性。

Conclusion: 本文提出了一种用于异构数据的自监督预训练方法，通过在训练中加入平衡（equilibrium）约束，确保从当前模型初始化并进行K步梯度下降后，模型对每个数据源都达到其局部最优，从而提高对不同数据源的适应性。论文将问题形式化为双层优化问题，并用一阶近似方法求解，讨论了与MAML的联系。实验（多域与多语言数据集）表明该方法显著提升了预训练模型在下游有监督微调任务上的适配性。

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [132] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop：用决策树蒸馏把可解释性和人类反馈引入神经网络的偏差缓解，实现情境感知的选择性公平修正。


<details>
  <summary>Details</summary>
Motivation: 敏感属性（如性别、年龄）在预测任务中可能导致不公平结果，且其影响常依赖上下文。现有自动化公平方法可能过于粗糙（例如一律移除敏感特征），缺乏情境感知和人类判断。

Method: 从训练好的神经网络中蒸馏出决策树表示；让用户基于该可解释表示识别并手工修改不公平分支；将修改后的决策逻辑回馈并用于对原神经网络进行微调以获得更公平的预测。

Result: 提出并实现了一个叫FairLoop的链路——将可解释模型与人类反馈结合到神经网络偏差缓解流程中，主张通过交互式、选择性的干预来改善模型公平性。抽象中未给出详细定量评估结果或对比实验。

Conclusion: FairLoop通过从神经网络中蒸馏出可解释的决策树，允许用户检查并修改不公平的决策逻辑，再用修改后的逻辑对原始模型进行微调，从而实现人类引导的、情境感知的偏差缓解。它选择性地处理敏感属性影响，而不是一律排除这些属性。

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [133] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: Use LLMs to generate personalized thematic email titles for e-commerce recommendations, boosting engagement in simulations and multi-million user experiments; they highlight production and safety learnings.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve engagement of personalized marketing emails in e-commerce by generating more inspiring, thematic email titles that reflect personalized content, addressing limitations of fixed template titles.

Method: Applied large language models to generate titles conditioned on personalized recommendation content; evaluated via offline simulations and online A/B tests across millions of users; implemented safety filters and production pipelines for automated title generation.

Result: LLM-generated thematic titles improved customer engagement in both offline simulations and large-scale online experiments; the authors successfully productionized safe, automated title generation for millions of users.

Conclusion: LLMs can effectively generate thematic, personalized email titles that increase engagement; careful evaluation and safety measures enable production deployment at large scale.

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [134] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: 通过六种注意力头剪枝策略，本文证明可在无触发器知识下对抗保存在微调后的语言模型中的后门，且不同策略在语法与风格类触发器下的效果有差异。


<details>
  <summary>Details</summary>
Motivation: 动机是预训练语言模型在微调后仍易受后门攻击（尤其是通过隐蔽的句法或文体修改注入的触发器），且最终用户通常不了解触发器，因此需要一种无需触发器信息、可在下游任务上做事后净化的通用防御方法。

Method: 作者提出并实现了六种基于剪枝的防御策略，均以迭代方式删除“最不重要”的注意力头并通过验证集准确率监控防止过度剪枝：1) 梯度驱动剪枝，2) 分层方差剪枝，3) 结合结构化L1/L2稀疏化的梯度剪枝，4) 随机化集成剪枝，5) 强化学习指导的剪枝，6) 贝叶斯不确定性剪枝。

Result: 实验表明：所有方法在一定程度上可以降低后门激活效果且保持下游验证准确率。梯度驱动剪枝在对抗语法触发器时效果最佳；而在风格/文体触发器场景中，强化学习和贝叶斯不确定性剪枝表现更稳健。

Conclusion: 本文结论是：注意力头剪枝能够在无需触发器信息或干净参考模型的前提下，对预训练语言模型中的后门攻击进行有效缓解。不同剪枝策略对不同类型触发器表现差异，梯度驱动剪枝对语法类触发器最有效，而强化学习和贝叶斯不确定性剪枝在风格/文体类触发器下更稳健。

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [135] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: 把FDS的分支排序看成MAB问题，用强化学习优化选择规则，在调参与问题特化改进下，在JSSP/RCPSP基准上显著加速并改进了下界与若干实例的最优性证明。


<details>
  <summary>Details</summary>
Motivation: FDS效率取决于分支决策的排序，目标是最小化搜索树规模。将决策选择视为一个在线选择问题可以用MAB策略学习更优的排序，从而加速完整性搜索。

Method: 理论上建立FDS的分支决策排序与MAB的对应关系；在此基础上应用并扩展MAB强化学习算法（含问题特定改进与参数调优）；在新求解器OptalCP上对JSSP与RCPSP标准基准进行大量实验比较。

Result: 在OptalCP中，最佳扩展MAB配置比原始实现对JSSP快1.7倍，对RCPSP快2.1倍；比IBM CP Optimizer 22.1的FDS实现，JSSP快3.5倍、RCPSP快2.1倍。以900s限制，改进算法在84个JSSP中改进了78个下界，在393个RCPSP中改进了226个并关闭了若干实例。

Conclusion: 将失败导向搜索（FDS）中的分支决策选择视作多臂老虎机（MAB）问题，并用强化学习的MAB算法来优化，可显著缩小搜索树、加速求解并提升下界与完结实例数。

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>
