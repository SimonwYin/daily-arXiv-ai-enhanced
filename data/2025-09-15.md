<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 51]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.NI](#cs.NI) [Total: 8]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Setchain Algorithms for Blockchain Scalability](https://arxiv.org/abs/2509.09795)
*Arivarasan Karmegam,Gabina Luz Bianchi,Margarita Capretto,Martín Ceresa,Antonio Fernández Anta,César Sánchez*

Main category: cs.DC

TL;DR: 提出并实现三种Setchain方案以放宽交易全序约束，实现在CometBFT上大幅提高吞吐量并保持低延迟；使用epoch-proofs确保轻客户端安全。


<details>
  <summary>Details</summary>
Motivation: 传统区块链对事务强序要求限制了可扩展性。Setchain通过放宽顺序约束、将事务组织成集合（epoch），旨在提升并行处理能力与整体系统吞吐量，同时保持安全性和轻客户端可验证性。

Method: 在CometBFT平台上实现并比较三种设计：Vanilla作为基线；Compresschain将元素批量化并压缩后作为epoch写入；Hashchain以固定长度哈希表示批次，依赖分布式服务提供批次内容。为支持轻客户端，仅需与单个服务器交互，三者在Setchain中维护epoch-proof（已签名的epoch哈希），客户端可通过收集f+1个epoch-proofs验证epoch正确性。评估在不同服务器数量和配置下衡量吞吐量与延迟。

Result: 实验表明，三种Setchain方案在各种配置下都实现了比底层CometBFT高数量级的吞吐量，且最终确定性延迟均低于4秒；不同方案在存储、通信和对轻客户端的依赖性上存在权衡：Compresschain通过压缩减少存储/带宽，Hashchain通过哈希减少账本写入量但需外部服务。

Conclusion: 本文提出并实现了三种基于区块链账本的Setchain算法（Vanilla、Compresschain、Hashchain），通过将事务按epoch（集合）组织、在集合内无序，从而显著提高吞吐量并降低确认延迟。实验证明在4、7、10节点集群上，三种方案在吞吐量上比底层区块链高出数量级，且最终确定性延迟低于4秒。

Abstract: Setchain has been proposed to increase blockchain scalability by relaxing the
strict total order requirement among transactions. Setchain organizes elements
into a sequence of sets, referred to as epochs, so that elements within each
epoch are unordered. In this paper, we propose and evaluate three distinct
Setchain algorithms, that leverage an underlying block-based ledger. Vanilla is
a basic implementation that serves as a reference point. Compresschain
aggregates elements into batches, and compresses these batches before appending
them as epochs in the ledger. Hashchain converts batches into fixed-length
hashes which are appended as epochs in the ledger. This requires Hashchain to
use a distributed service to obtain the batch contents from its hash. To allow
light clients to safely interact with only one server, the proposed algorithms
maintain, as part of the Setchain, proofs for the epochs. An epoch-proof is the
hash of the epoch, cryptographically signed by a server. A client can verify
the correctness of an epoch with $f+1$ epoch-proofs (where $f$ is the maximum
number of Byzantine servers assumed). All three Setchain algorithms are
implemented on top of the CometBFT blockchain application platform. We
conducted performance evaluations across various configurations, using clusters
of four, seven, and ten servers. Our results show that the Setchain algorithms
reach orders of magnitude higher throughput than the underlying blockchain, and
achieve finality with latency below 4 seconds.

</details>


### [2] [Ordered Consensus with Equal Opportunity](https://arxiv.org/abs/2509.09868)
*Yunhao Zhang,Haobin Ni,Soumya Basu,Shir Cohen,Maofan Yin,Lorenzo Alvisi,Robbert van Renesse,Qi Chen,Lidong Zhou*

Main category: cs.DC

TL;DR: 为防止区块链中的排序不公，论文将平等机会纳入有序共识，利用秘密随机预言机（SRO）与随机化方法，并提出Bercow协议与两种SRO实现以近似保证公平性。


<details>
  <summary>Details</summary>
Motivation: 传统SMR对命令的最终全序并不要求，但在区块链中排序会产生不同经济回报，且即便无拜占庭行为也会因网络速度或地理接近性导致不公平排序，需在共识层面引入公平性保障。

Method: 使用随机性来限制偏见，设计了两种SRO实现：基于可信硬件和基于阈值可验证随机函数（VRF）；在此基础上提出新协议Bercow，通过可配置因子近似实现平等机会。

Result: 提出SRO的两种实现和Bercow协议，理论上可在可配置范围内近似实现平等机会，从而有效缓解已知的排序操控攻击。

Conclusion: 本文提出在有序共识中引入平等机会（equal opportunity）的公平性要求，并通过引入秘密随机预言机（SRO）来控制偏见，从而减轻SMR区块链中的排序攻击。

Abstract: The specification of state machine replication (SMR) has no requirement on
the final total order of commands. In blockchains based on SMR, however, order
matters, since different orders could provide their clients with different
financial rewards. Ordered consensus augments the specification of SMR to
include specific guarantees on such order, with a focus on limiting the
influence of Byzantine nodes. Real-world ordering manipulations, however, can
and do happen even without Byzantine replicas, typically because of factors,
such as faster networks or closer proximity to the blockchain infrastructure,
that give some clients an unfair advantage. To address this challenge, this
paper proceeds to extend ordered consensus by requiring it to also support
equal opportunity, a concrete notion of fairness, widely adopted in social
sciences. Informally, equal opportunity requires that two candidates who,
according to a set of criteria deemed to be relevant, are equally qualified for
a position (in our case, a specific slot in the SMR total order), should have
an equal chance of landing it. We show how randomness can be leveraged to keep
bias in check, and, to this end, introduce the secret random oracle (SRO), a
system component that generates randomness in a fault-tolerant manner. We
describe two SRO designs based, respectively, on trusted hardware and threshold
verifiable random functions, and instantiate them in Bercow, a new ordered
consensus protocol that, by approximating equal opportunity up to within a
configurable factor, can effectively mitigate well-known ordering attacks in
SMR-based blockchains.

</details>


### [3] [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)
*Seokjin Go,Joongun Park,Spandan More,Hanjiang Wu,Irene Wang,Aaron Jezghani,Tushar Krishna,Divya Mahajan*

Main category: cs.DC

TL;DR: 论文通过实测H100/H200/MI250平台上多种并行策略和优化，揭示了LLM训练中硬件、拓扑与并行策略的复杂交互，并给出系统/硬件设计建议以提升可扩展性与可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM规模扩大，单机分析不足以指导多GPU系统设计，需要系统性理解模型并行策略与硬件拓扑如何共同影响训练效率与可靠性。

Method: 在真实多GPU平台（NVIDIA H100/H200、AMD MI250）上，针对稠密与稀疏模型，比较张量并行、流水线并行、数据并行与专家并行等策略；测量硬件利用率、功耗与温控；评估激活重计算、计算-通信重叠等优化。

Result: 发现规模扩展并非万能：少数高内存GPU的scale-up在通信受限时能优于scale-out，但需精调；某些并行组合（如张量+流水线）造成带宽未充分利用；过大微批量引发突发执行与峰值功耗，加剧热节流；激活重算与通信重叠在不同场景效果不同。

Conclusion: 该论文通过跨平台、跨策略的系统性实验，揭示LLM训练性能受硬件、拓扑和并行策略复杂交互影响，提出了硬件与系统设计优化建议。

Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training
workloads far beyond the limits of single-node analysis, demanding a deeper
understanding of how these models behave across large-scale, multi-GPU systems.
In this paper, we present a comprehensive characterization of LLM training
across diverse real-world workloads and hardware platforms, including NVIDIA
H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various
parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate
their effects on hardware utilization, power consumption, and thermal behavior.
We further evaluate the effectiveness of optimizations such as activation
recomputation and compute-communication overlap. Our findings show that
performance is not determined solely by scaling hardware capacity. Scale-up
systems with fewer, higher-memory GPUs can outperform scale-out systems in
communication-bound regimes, but only under carefully tuned configurations; in
other cases, scale-out deployments achieve superior throughput. We also show
that certain parallelism combinations, such as tensor with pipeline, lead to
bandwidth underutilization due to inefficient data chunking, while increasing
microbatch sizes beyond a certain point induces bursty execution and peak power
excursions that worsen thermal throttling. These insights reveal how training
performance is shaped by complex interactions between hardware, system
topology, and model execution. We conclude by offering recommendations for
system and hardware design to improve the scalability and reliability of future
LLM systems and workloads. The source code of this project is available at
https://github.com/sitar-lab/CharLLM-PPT.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [4] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: 提出SAM-BG：用边掩码器学结构先验，再用于结构感知的SSL增强，在小标签脑网络诊断中既提升性能又增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 标注脑网络数据稀缺且现有SSL数据增强可能破坏脑图的结构语义，影响诊断准确性与可解释性。

Method: 两阶段方法：第一阶段在小规模有标签子集上训练边掩码器以捕捉关键结构语义；第二阶段利用该结构先验指导结构感知的数据增强进行自监督预训练，从而学习更鲁棒、有语义意义的表示。

Result: 在两个真实精神病学数据集上，SAM-BG较现有方法表现更好，特别是在标签稀缺情形；并能发现临床相关的连接模式，提高模型可解释性。

Conclusion: SAM-BG通过结构语义保持的自监督学习，有效提升了脑网络分析中在小样本下的诊断性能并增强了解释性。

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [5] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT通过自注意力特征提取和跨注意力对齐损失，实现训练时多模态知识融合、推理时单模态使用，显著提升资源受限场景下的分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态迁移需在训练与推理都存在配对传感器数据，限制了在资源受限或可变传感器可用性的部署；因此希望实现训练时利用多模态信息而推理时只需单模态。

Method: 引入自注意力模块提取每个模态特征，并通过新颖的跨注意力对齐损失（cross-attention alignment loss）强制不同传感器特征空间对齐，同时保持各模态分类流水线解耦以支持单模态推理。

Result: 在三个人体活动多模态数据集（IMU、视频、音频）上评估：在同分布场景下，从高性能模态（如视频）向目标模态（如IMU）转移可带来最高约10% F1提升；在分布外场景下，只要目标模型不过拟合，哪怕源模态较弱（如IMU到视频）也能改善目标性能。

Conclusion: D-CAT能在推理时只用单一传感器，同时通过跨模态对齐提升目标模态性能，减小硬件冗余并适用于资源受限场景。

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [6] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF通过在本地训练中逐位学习量化参数（每轮仅更新1比特）实现高效通信压缩，在多种数据集与非IID场景下能维持与FedAvg相近的精度并促成模型稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法大多在本地训练后再对模型更新进行量化，导致训练过程中引入量化误差并可能降低精度；因此提出直接在训练中学习量化参数以减少这种误差并进一步压缩通信开销。

Method: 在每轮通信中，服务器先量化模型参数并下发至客户端。客户端在本地训练时仅允许更新量化表示中的一个比特位，冻结其他比特。通过逐位更新策略，将每个参数的上传更新压缩为1比特，同时保持多比特的参数表示精度。

Result: 在五个常用数据集的IID和Non-IID设置下的大量实验表明，FedBiF在通信压缩方面优于现有方法，并能在仅使用上行1 bpp、下行3 bpp的情况下达到与FedAvg相当的准确率，同时生成更稀疏的模型。

Conclusion: FedBiF在通信效率与模型精度间实现了良好折衷，通过在本地训练阶段直接学习量化参数并采用“逐位更新、其余位冻结”的策略，显著降低上行通信开销，并在多个数据集和非IID场景下达到与FedAvg相当的精度，同时促进模型稀疏性。

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [7] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: 作者提出Meta-RL-Crypto：一个将元学习与强化学习融合的、基于变压器的自我改进加密货币交易系统，通过actor/judge/meta-judge闭环迭代和内部偏好反馈，无需人工监督，能处理多模态输入并在实证上优于LLM基线。


<details>
  <summary>Details</summary>
Motivation: 加密货币价格受链上活动、新闻和社交情绪等快速变化因素驱动，标注数据稀缺且昂贵，故需要一种无需额外人工监督、可持续自我改进的交易代理。

Method: 基于指令调优的变压器LLM，系统在actor、judge、meta-judge三个角色之间闭环迭代，通过内部偏好反馈和无监督循环提升交易策略与评估标准，支持多模态市场输入。

Result: 在不同市场环境下，Meta-RL-Crypto在真实市场的技术指标上表现良好，优于其他基于LLM的基线方法。

Conclusion: 该论文提出了一个结合元学习与强化学习的自我改进加密货币交易代理。

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [8] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa将KV缓存压缩归结为最小化残差流信息损失，提出动态按层和按头预算分配的统一无训练方案，在多项长上下文基准上显著优于现有方法，并揭示了层/头预算对不同任务类型的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存压缩方法多为启发式且缺乏动态预算分配，导致在长上下文推理时内存效率不足和性能下降，需一种统一、动态且无训练需求的压缩策略。

Method: 基于残差流信息损失分析，推导出衡量不同头间缓存条目的重要性的新度量，用于按头压缩与动态分配预算；通过跨层信息对比确定动态层预算；统一了缓存驱逐与预算分配，且无需额外训练或多方法组合。

Result: 在LongBench、Needle-In-A-Haystack、Ruler和InfiniteBench等基准上，LAVa优于现有方法；实验证明动态层预算对生成任务至关重要，动态头预算对抽取任务更重要，LAVa在各类任务中均保持顶级表现。

Conclusion: LAVa通过将KV缓存压缩问题统一为最小化Transformer残差流信息损失，提出了动态的按层与按头预算分配策略，实现了比现有启发式方法更优的缓存压缩与淘汰。

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [9] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: HACO将保形风险门控与离线RL结合，提供保守、可审计的决策支持，兼顾风险控制与操作覆盖，但需注意群体公平性问题。


<details>
  <summary>Details</summary>
Motivation: 在Medicaid人群的群体健康管理中，需要在大规模协调行动中保证决策的安全性、公平性与可审计性，避免增加急诊或住院等不良利用事件。

Method: HACO包括三步：训练一个轻量级风险模型预测不良利用事件；使用保形（conformal）方法确定阈值以在目标风险水平上屏蔽不安全动作；在剩余的安全子集中训练偏好策略（离线RL），并用版本无关的FQE进行评估与分层审计。

Result: 在Waymark的去标识运营数据（2.77M次决策、168,126名患者）上，风险模型AUC约为0.81；在α=0.10时，保形阈值τ≈0.038，保持较高的安全覆盖率；但分组分析显示不同人口统计组间存在系统性估计价值差异。

Conclusion: 该论文提出了HACO框架，通过将风险校准与偏好优化分离，实现对医疗协调决策的保守且可审计的离线强化学习推荐。

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [10] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: 在灾难场景下，提出“Proof of AutoML”思路：在SDN控制的能源交易系统中，用AutoML挑选的回归模型（尤其是树模型）作为区块链Nonce的轻量随机源；实验证明Random Forest和Extra Trees随机性最好，其他模型也很高。


<details>
  <summary>Details</summary>
Motivation: In disaster scenarios with compromised infrastructure, secure and traceable energy trading needs unpredictable nonces for blockchain transactions; standard RNGs may be unavailable or undesired, so leveraging ML model randomness within SDN-managed energy networks could be a lightweight alternative.

Method: Use SDN for flexible control and AutoML to select five regression models; evaluate models on 9000-sample dataset by shuffling inputs and measuring output diversity/randomness rather than accuracy.

Result: Random Forest and Extra Trees show complete dependency on randomness; Gradient Boosting, KNN, LightGBM show high randomness scores (97.6%, 98.8%, 99.9%). This suggests feasibility of using certain ML models as nonce generators.

Conclusion: Tree-based ensemble regressors (Random Forest, Extra Trees) produce highly randomized outputs suitable for nonce generation; Gradient Boosting, KNN, LightGBM also perform strongly. The proposed Proof of AutoML within an SDN-enabled energy trading system can provide lightweight nonce generation for blockchain in disaster scenarios.

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [11] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: 提出一种基于单头交叉注意力的轻量级成本感知LLM路由器，能联合建模查询与模型特征，预测质量与成本，在大规模基准上显著优于现有方法并提供更稳健的性能-成本权衡。


<details>
  <summary>Details</summary>
Motivation: 面对性能与计算开销各异的大规模LLM库，亟需一种可扩展且成本敏感的路由机制，以在不同查询下选择最合适的模型。

Method: 使用单头cross-attention将查询和模型嵌入联合编码，预测响应质量与生成成本；在RouterBench基准上训练和评估；引入指数回报函数用于平衡性能与成本。

Result: 在RouterBench上相比现有路由方法，AIQ提升最多6.6%，最大性能提升2.9%；路由器更稳定、轻量且跨领域泛化更好。

Conclusion: 该论文提出了基于单头交叉注意力的统一路由框架，通过联合建模查询与模型嵌入实现针对每个输入动态选择最优LLM，兼顾质量与成本。

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [12] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: 提出并分析Gradient-Step Denoiser，将去噪器设计为对应显式能量的梯度/近端算子，在保持优秀去噪性能同时提升PnP方法的可解释性与收敛性。


<details>
  <summary>Details</summary>
Motivation: 传统PnP方法依赖强性能的黑箱去噪器，但这些去噪器通常对应的图像先验是隐式的，难以分析与保证收敛性。GSD旨在设计一个既有数学可解释性（对应显式能量或梯度）的去噪器，同时保持state-of-the-art的去噪效果，从而提升PnP算法的理论可控性与实用性。

Method: 作者通过构造并训练GSD使其在小幅度噪声条件下逼近某一显式能量函数的梯度（或近端映射），并在PnP框架中用GSD替换传统的近端或梯度步算子。理论上分析了GSD对应的能量函数存在性与性质，并在若干逆问题（如去噪、去模糊、重建）中做实验验证。

Result: 结果表明：训练得到的GSD既能在数值上逼近显式函数的梯度/近端算子，又在多种逆问题中实现与或超过现有PnP方法的恢复质量；理论分析支持该方法的收敛性和稳定性，且能将先验能量显式化，便于进一步分析与调优。

Conclusion: 该论文研究了Gradient-Step Denoiser（GSD）在Plug-and-Play（PnP）算法中的应用与分析，证明GSD可以作为显式能量函数的梯度下降或近端算子使用，同时保持先进的去噪性能，从而将隐式先验的PnP方法与显式能量建模桥接起来。

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [13] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: 通过多模态生理信号与Late Fusion策略，SVM在二分类达85.7%、XGBoost在三分类达74.9%，实现对startle、surprise及基线状态的有效区分。


<details>
  <summary>Details</summary>
Motivation: 在高风险环境（如航空）中，意外事件会引起不同类型的反应（startle与surprise），这些反应会影响注意力和决策安全。现有研究常分别研究两种反应，缺乏联合分析及基于生理信号的区分方法。

Method: 使用多模态生理信号（未具体列出信号类型）结合机器学习分类器（SVM、XGBoost等）和Late Fusion融合策略进行判别。分别进行了二分类（Startle vs Surprise）和三分类（Startle vs Surprise vs Baseline）实验并报告准确率。

Result: 二分类任务最高平均准确率85.7%（SVM + Late Fusion）；三分类任务最高平均准确率74.9%（XGBoost + Late Fusion），表明方法具有较好区分能力和一定鲁棒性。

Conclusion: 本文表明基于生理信号和多模态融合的方法可以有效区分惊跳（startle）与惊讶（surprise）反应，并在包括基线条件的三分类任务中仍表现稳健。

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [14] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: 将actor与critic的熵耦合视为DSAC性能差的关键，解耦后构建了可使用m步Bellman和多种策略优化的泛化off-policy actor-critic框架，理论保证收敛并在Atari上接近DQN性能。


<details>
  <summary>Details</summary>
Motivation: 现有离散动作的离线/离散off-policy强化学习常用DQN类值方法；现有策略方法要么为on-policy（如PPO）不擅长利用off-policy数据，要么在离散动作下表现差（如SAC）。因此研究如何在离散动作下让off-policy actor-critic方法达到与DQN可比的性能。

Method: 从DSAC出发，识别并解耦actor与critic的熵项；提出一个泛化的off-policy actor-critic框架，允许使用m步Bellman更新critic，并将任意策略优化方法与熵正则相结合以构造actor目标；理论上在tabular情景下证明收敛性；在Atari基准上进行对比实验（含无熵正则情形）。

Result: 纯粹通过解耦actor和critic的熵项，DSAC性能大幅提升，可与DQN接近；提出的通用框架在理论上保证收敛并在Atari上取得接近DQN的实证结果，且在无熵正则或无显式探索下也能表现良好。

Conclusion: 本文研究离散动作下的off-policy actor-critic方法，指出DSAC性能差主要由actor与critic熵耦合引起，提出将其解耦可显著提升性能；构建了一个更通用的框架，容许m步Bellman算子和不同策略优化+熵正则化的组合，并在表格环境下证明收敛性；实验证明方法能在Atari上接近DQN表现，即使在无熵正则或显式探索时也能取得良好效果。

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [15] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN为异构图设计了元路径+随机丢弃生成的多样化GNN子模型，配合残差注意力与相关性正则化，提升基学习器准确性与多样性，从而在节点分类任务上取得领先效果。


<details>
  <summary>Details</summary>
Motivation: 异构图中节点类型、属性与局部拓扑差异大，单一GNN难以兼顾不同结构信息，因而需要一种能整合多样化图学习器并保证准确性与多样性的集成方法。

Method: 提出基于元路径与随机丢弃生成Allele GNNs的框架，采用残差注意力对齐不同元路径的子模型并用相关性正则化扩大嵌入矩阵间差异，最终通过优化管道进行集成。

Result: 理论上分析了HGEN的收敛性并显示其正则化强度高于简单投票；在五个异构网络上的实验证明HGEN显著优于现有最先进方法。

Conclusion: HGEN通过结合基于元路径的多样化子模型与变换与正则化手段，实现了在异构图节点分类上优于单模态GNN与简单集成方法的性能。

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [16] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: 提出一个在推理时动态选择生成方法并分配计算预算的框架，联合考虑token成本与壁钟延迟，在推理基准上较静态策略取得更好的准确率/成本折衷，适合延迟敏感的应用场景。


<details>
  <summary>Details</summary>
Motivation: 现有动态分配研究多聚焦并行生成并以token数为成本度量，忽视了增量解码方法与对延迟的直接考虑；然而实际部署（尤其是需多次查询的agent流程）对响应延迟敏感，因此需一个同时兼顾方法选择与延迟的动态分配框架。

Method: 建立一个包含并行生成（如best-of-N）与增量解码（如beam search）的候选策略池，针对每个输入动态选择策略与分配计算预算，评价指标同时包含token使用量与真实壁钟延迟；在实验中在推理基准上比较动态策略与静态策略的准确率—成本权衡。

Result: 在多个推理基准上，提出的方法优于静态策略，能在相同或更低的token与延迟成本下取得更高准确率，表现出更优的准确率-成本折衷并具有实际部署可行性。

Conclusion: 本文将推理時的计算分配和方法选择视为动态决策问题，通过同时考虑token成本与实际延迟，在每个查询上选择并分配最合适的生成策略，从而提升LLM在推理任务上的效率与准确率。

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [17] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: 基于热力学拉格朗日构建的神经网络仅用可观测量即可对耗散动力系统进行数据驱动建模，保证熵增并能用少量数据与参数有效预测相空间演化。


<details>
  <summary>Details</summary>
Motivation: 在耗散系统中，实际观测数据常只能获得坐标类可观测量，而动量和熵通常不可直接测量，传统数据驱动方法难以重构完整相空间和保证热力学一致性；因此需要一种只依赖可观测变量且内嵌热力学约束的建模框架。

Method: 构建热力学拉格朗日形式，将不可观测的动量和熵隐式嵌入到可观测变量的神经网络结构中；设计网络结构或损失函数以满足能量守恒和熵增原则（熵非减）；利用少量数据点和较小参数量训练得到可有效描述相空间演化的模型。

Result: 提出的神经网络模型在数值实验（或示例系统）中能用有限数据和较少参数准确近似相空间演化，同时保证熵单调增加，表现出对热力学一致性和泛化预测的良好性能。

Conclusion: 本文提出了一种基于热力学拉格朗日的仅用可观测变量进行耗散动力系统数据驱动建模的方法，能够保证熵非减并符合热力学约束，从而恢复相空间演化和预测未观测到的动力学行为。

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [18] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 将LTSSL迁移到参数高效的基础模型微调范式，提出LoFT与LoFT-OW，利用更可靠伪标签和OOD处理在长尾半监督任务上取得明显优势。


<details>
  <summary>Details</summary>
Motivation: 现有LTSSL多从头训练，导致模型过度自信和低质量伪标签；利用基础模型微调能生成更高质量伪标签并提升不平衡学习性能；实际场景还存在未标注数据含OOD样本的问题，需要专门处理。

Method: 在预训练大型基础模型上采用参数高效微调（PEFT）来适配长尾半监督学习，通过微调得到更可靠的伪标签；对含OOD样本的开放世界场景，LoFT-OW加入机制以增强判别能力以拒绝或过滤OOD样本。

Result: 在多个基准上，LoFT/LoFT-OW优于先前方法，在仅使用远少于（1%）先前工作的未标注数据时仍取得更好性能，表明方法在数据效率与鲁棒性上有显著提升。

Conclusion: 本文提出将长期尾部分布半监督学习（LTSSL）扩展到基础模型微调范式，提出LoFT框架及其在开放世界场景下的扩展LoFT-OW，利用参数高效微调生成更可靠的伪标签以改善长尾学习。

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [19] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: 扩展CSB到可选择非负整数动作的MP-CSB，提出两种分别兼顾计算效率与对抗/随机环境性能的算法，理论与实验均优于现有工作。


<details>
  <summary>Details</summary>
Motivation: 传统CSB限制为二元决策，无法处理多次选择或流量分配问题（非负整数决策），因此需要扩展模型以涵盖最优运输、背包等实用问题，并设计高效算法。

Method: 提出两种算法：1) 基于Thompson采样的算法，适用于动作空间呈指数级的情形，随机环境下获得O(log T)的分布依赖后悔；2) 一种best-of-both-worlds算法，在随机环境下获得与方差相关的O(log T)后悔，在对抗环境下达到最坏情况\tilde{O}(\sqrt{T})后悔，并且对对抗情形的后悔是数据依赖的，可适配最优动作的累积损失、总二次变化和路径长度。

Result: 理论上证明：Thompson采样算法在随机环境下达成O(log T)分布依赖后悔；best-of-both-worlds算法在随机环境下达成O(log T)（方差依赖）后悔，在对抗环境下达成\tilde{O}(\sqrt{T})最坏情况后悔，并具备对数据统计量的适应性。数值实验显示所提算法优于现有CSB方法。

Conclusion: 本论文将经典的组合半带臂问题(CSB)从二元决策扩展到允许非负整数动作的多次选择情形(MP-CSB)，解决了如最优运输和背包等需要流量/配额分配的问题，并提出了两类算法，分别针对随机和对抗两种环境，兼顾计算可行性与渐进最优性。

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [20] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: Use LLMs to generate numerical solver code for ODEs; build diagnostic and large benchmarks; guided prompting and fine-tuning let modern LLMs reliably solve many simple ODE tasks.


<details>
  <summary>Details</summary>
Motivation: Instead of training neural networks to directly predict solutions, leverage LLMs to write code that uses decades of numerical analysis to improve accuracy and robustness in scientific computing.

Method: Created two datasets (diagnostic adversarial set and large-scale 1000 ODE benchmark); evaluated multiple LLMs (open/closed-source, off-the-shelf and fine-tuned) under unguided vs guided prompting; measured executability and numerical validity against reference solutions.

Result: With sufficient context and domain-specific guiding prompts, newer instruction-following LLMs perform well on selecting appropriate solvers and producing stable, correct ODE code; open-source models can perform strongly without fine-tuning, while older/smaller models benefit from fine-tuning.

Conclusion: LLMs, when properly prompted and fine-tuned, can act as effective SciML agents by generating runnable, numerically appropriate code for ODE tasks, achieving high executability and numerical validity on many simple ODEs.

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [21] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: 作者通过将视听信息转成逐token的动态卷积核来调制文本编码，避免了传统融合带来的噪声污染，显著改善了多模态意图识别与异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态意图识别中，非语言模态可能包含与意图无关或冲突信息，传统的特征级融合容易将噪声注入主要语言特征，缺乏细粒度token级别的调制机制。

Method: 提出DyKen-Hyena模型：将音视频信号映射为按token的动态卷积核，应用于文本编码器中以调制（而非简单融合）文本特征，替代常见的多头注意力加残差的融合策略。

Result: 在MIntRec和MIntRec2.0基准上达到SOTA；在异常（out-of-scope）检测任务上F1提升约+10.46%，表明方法在构建更鲁棒的意图表示方面显著有效。

Conclusion: 该文提出了一种从特征融合转向处理调制的方法，通过将视听线索转换为逐标记的动态卷积核来调制文本特征提取，从而避免无关或冲突的非语言信息污染语言特征，显著提升MIR在意图识别和异常意图检测上的鲁棒性。

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [22] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 无训练、自适应的按层相似性阈值token合并，通过贝叶斯优化找到准确率-成本-通信的帕累托解，能在边缘场景显著降低计算与通信开销并提升隐私性，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 大规模Transformer在边缘设备部署受限于高计算与通信开销，且固定比例的token压缩无法适应输入语义冗余的变化，因而需要一种无需重训、能按输入自适应压缩表示以节省资源同时保持任务性能的方法。

Method: 提出了基于每层相似性阈值的自适应token合并机制，按输入冗余动态决定合并比例；将合并策略搜索视为多目标优化问题，使用贝叶斯优化得到在准确率、推理成本和通信成本之间的帕累托最优解；整个流程无需再训练（training-free），在运行时应用于预训练Transformer。

Result: 在ImageNet分类上，在不降低精度的前提下实现约30% FLOPs减少和低于20%原始通信成本；在视觉问答上，性能接近完整LLaVA模型，同时计算量不到1/3、带宽不到1/10；在不同信道条件下保持稳健，并显著降低模型反演攻击效果。

Conclusion: 本文提出了一种无训练的自适应token合并框架，通过在运行时根据每层相似性阈值选择性合并语义冗余token，从而在不重新训练模型的情况下实现计算与通信开销的显著降低。实验证明在ImageNet和VQA任务上均能在大幅度减少FLOPs和带宽的同时保持接近或匹配原模型的性能，并在不同信道条件下表现稳健，还带来对模型反演攻击的隐私增强效果。

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [23] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: 通过规则驱动的提示与双重筛选，ReFine在少样本表格数据生成上显著提升下游回归与分类表现。


<details>
  <summary>Details</summary>
Motivation: 在领域特定且样本稀少的表格数据库中，传统生成方法依赖大量参考数据，而纯提示式LLM难以捕捉特征-标签依赖并容易产生冗余样本，导致下游任务表现下降。

Method: 使用可解释模型提取符号化的if-then规则，作为提示嵌入到大语言模型生成过程中；随后采用宏观和微观两层筛选策略，宏观抑制过采样与重复数据，微观保留并细化稀有但有信息量的样本。

Result: 在多种回归与分类基准上，ReFine优于现有最先进方法，回归任务R^2最高提升0.44，分类任务F1相对提高10.0%。

Conclusion: ReFine通过将可解释模型得出的规则嵌入提示并结合双粒度筛选，有效提升了小样本域特定表格数据生成质量，从而改善下游任务性能。

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [24] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: 利用虚拟机内部的资源指标训练Gradient Boosting回归器以预测主机RAPL测得的能耗，在多种负载下实现了高精度（R^2=0.90–0.97），证明了无需主机特权即可进行有效的能耗估计，可支持能耗感知的调度与成本优化。


<details>
  <summary>Details</summary>
Motivation: 在虚拟化环境（如云）中，客机无法访问主机级的物理功率测量接口，导致难以获取准确的能耗信息，限制了能耗感知调度与成本优化等应用。研究旨在填补这一空白，通过仅依赖guest侧数据估计能耗。

Method: 收集guest虚拟机内的资源利用率指标（如CPU、内存、磁盘、网络等），并以主机上通过RAPL测量到的能耗作为标签，使用Gradient Boosting Regressor进行监督学习建模。模型训练与评估在多种工作负载下进行，对预测精度和解释方差进行量化。

Result: 在不同工作负载下，模型达到了高预测精度和解释方差（R^2介于0.90至0.97），表明基于guest的资源指标可以很大程度上反映实际主机能耗，支持能耗感知调度和成本估计的可行性。

Conclusion: 本文提出了一种基于机器学习的方法，利用虚拟机内的资源利用率指标在无主机特权访问的情况下估计虚拟服务器的能耗，结论表明该方法在多种工作负载下能够高精度预测能耗（R^2在0.90到0.97之间），证明了仅在guest端也能进行有效能耗估计，具有实际可用性。

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [25] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: 深度回归也服从神经尺度律：损失随训练数据和模型容量呈幂律下降，指数在1~2，意味着更多数据能大幅提升回归精度。


<details>
  <summary>Details</summary>
Motivation: 现有神经尺度律研究多集中在语言模型和分类任务，深度回归任务（尤其物理参数估计）中的尺度规律尚不明确，本研究旨在填补该空白并评估数据与模型放缩对回归性能的影响。

Method: 使用参数估计任务（扭曲的范德瓦尔斯磁体模型）作为回归目标，训练多种架构（全连通、残差、视觉Transformer）并系统变换训练集大小和模型参数量，记录验证损失，拟合损失与数据量/容量的幂律关系，估计尺度指数。

Result: 在广泛设置和架构下，损失与训练集大小及模型容量均呈显著幂律关系；尺度指数分布在1到2之间，因回归目标和模型细节而异；大指数值暗示通过增加数据量可以取得显著误差降低。

Conclusion: 本文实证表明，深度回归模型也遵循神经尺度律，损失关于训练集大小和模型容量呈幂律下降，且幂律指数在1到2之间，表明数据增大能显著提升回归性能。

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [26] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA是一种带投影重构损失和重加权CancelOut层的自编码器，能同时估计数据内在维度并在相应潜在空间重构原始数据，在理论和流体力学数值数据上表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前内在维度估计方法对非线性流形或后续重构能力可能有限。作者希望设计一个既能准确估计内在维度又能在相应潜在空间内重构原始数据的模型，特别用于复杂物理数值解数据。

Method: 提出一种名为IDEA的自编码器，使用重加权双CancelOut层构造潜在空间，并引入投影重构损失项：在训练过程中不断评估在移除额外潜在维度时的重构质量，以识别和保留必要的内在维度。

Result: 在理论基准上，IDEA在内在维度估计上展示了良好准确性并能稳健重构数据；在垂直分辨的一维自由表面流生成的数据上，模型成功估计出内在维度并在识别的投影空间内重构出原始解。

Conclusion: IDEA通过在自编码器中引入投影重构损失与重加权CancelOut层，有效估计并重构线性与非线性流形上的内在维度，在理论基准与一维自由表面流数值数据上表现出稳健性与高适应性。

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [27] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: 提出SMoE-VAE，在QuickDraw上发现无监督专家路由优于有监督路由，专家学到超越标签的子类结构；通过可视化与数据量实验分析专家分工与性能权衡。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络内部组织与专家模块化如何揭示数据的内在结构，以及无监督路由是否能学习比人工标签更符合模型目标的分解，从而提高重建与表示效果。

Method: 提出并实现Sparse Mixture of Experts Variational Autoencoder（SMoE-VAE），在QuickDraw数据集上训练比较无监督路由与有监督路由（基于真实标签）两种配置；通过重建误差、t-SNE可视化与重建样本分析评估专家分工与表示；还研究不同训练集规模对专家专业化与性能的影响。

Result: 无监督路由在重建方面持续优于有监督基线；专家自动学出有意义的子类别，常跨越人类定义的类别边界；t-SNE与重建示例表明MoE偏向于基于模型目标分组数据；数据量研究显示样本规模影响专家分化与性能，揭示数据量与专家专业化间的权衡。

Conclusion: SMoE-VAE展示了无监督专家路由能优于有监督标签引导的重建性能，专家捕捉到超越人工类别的子类别结构，提示模型学习目标比预定义标签更贴合数据内在结构。

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [28] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: 提出一种针对双字典的低秩稀疏编码与凸松弛求解（AODL），理论给出样本复杂度与收敛性证明，实验证明在重建与缺失插补上能显著提高稀疏性并提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 在多字典（例如时空分解）情形下，编码系数数目呈字典原子组合爆炸，学习字典与系数变得困难；通过低秩假设减少自由度以降低样本复杂度并提高稀疏性和可泛化性。

Method: 构建2字典低秩编码模型并进行凸松弛；证明松弛问题的解即原问题的解；采用在稀疏编码矩阵与字典间交替优化的迭代算法求解，并证明收敛性。

Result: 给出了样本复杂度界，提出AODL方法并证明其解的等价性与算法收敛性；实验上在合成与真实数据中，在相同重建质量下相比非低秩学习字典和固定解析字典达到最多90%更高稀疏度，且学得字典具有可解释模式。

Conclusion: 本文提出了用于双字典场景的低秩稀疏编码模型（AODL），并证明其凸松弛的可行性和交替优化算法的收敛性，实验显示在重建和缺失值插补任务上显著提高稀疏性且可解释性强。

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [29] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: 本文构造并证明了一类符号前馈神经网络能精确且可学习地模拟概率有限自动机，建立了两者在表达与学习方面的严格联系。


<details>
  <summary>Details</summary>
Motivation: 将概率自动机理论与神经网络架构联系起来，为可解释、并行且可微的符号模拟器提供代数化、构造性理论，弥合符号计算与深度学习的差距。

Method: 将状态分布表示为向量，转换表示为随机矩阵，通过无循环的矩阵-向量乘法实现概率状态传播，使用分层符号计算刻画概率子集构造、ε-闭包和精确模拟，并证明与特定类神经网络等价；通过梯度下降训练验证学习性，提出命题5.1证明可学习性。

Result: 给出形式化等价性证明、可学习性的理论保证（命题5.1），并构造能在无循环前馈网络中精确模拟PFA的体系结构，实验（或理论）表明通过梯度训练可恢复真是PFA行为。

Conclusion: 本文证明了可概率有限自动机（PFA）可被符号前馈神经网络精确模拟，二者在表达能力上等价，并且该符号模拟器在带标签序列数据上通过标准梯度下降可学习到真实PFA的行为。

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [30] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: 提出FedRP：在客户端先做随机投影降维再用ADMM优化，兼顾差分隐私与通信效率，实验优于传统DP和FedADMM。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中两大痛点：一是防止服务器或中间人通过参数/梯度重构用户敏感数据，二是减少模型参数在设备与服务器间传输导致的高通信成本。

Method: 在联邦学习框架下，客户端先对模型参数施加随机投影降维，再基于ADMM进行分布式优化；降维后的参数传输至服务端以降低通信开销，并通过随机化机制满足(ε,δ)-差分隐私。

Result: 理论上给出(ε,δ)-差分隐私证明与抗重构能力，实验上在多个任务/数据集对比中，FedRP在保持或提高模型准确率的同时，通信开销和隐私泄露风险均低于基线方法。

Conclusion: FedRP通过将随机投影与ADMM结合，在降低通信成本的同时强化隐私保护，能实现(ε,δ)-差分隐私并抵抗重构攻击，且在实验中比传统差分隐私方法和FedADMM在精度、隐私与通信效率间表现更好。

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [31] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: 尝试将轻量级变分贝叶斯最后层（VBLL）加入TabPFN以提升不确定性校准，但实验表明这种集成在三个医疗数据集上反而降低了校准性能，原始TabPFN表现更好。


<details>
  <summary>Details</summary>
Motivation: 评估轻量级变分贝叶斯最后层（VBLL）在提升已存在生成式Transformer表格模型（TabPFN）不确定性估计上的效果，期望通过小成本改动改善校准性能。

Method: 将VBLL模块集成到TabPFN的最后层，保持其他训练和推理流程不变；在三个基准医疗表格数据集上比较原始TabPFN与VBLL-TabPFN的校准性能，使用标准不确定性/校准指标评估。

Result: 实验结果显示，与预期相反，VBLL集成并未改善反而降低了TabPFN的校准性能；原始TabPFN在所有三个数据集上的校准指标均优于VBLL版本。

Conclusion: 在三个医疗表格数据集上，原始TabPFN在不确定性校准方面持续优于与VBLL集成的版本。

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [32] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: 提出KAN-SR：基于Kolmogorov Arnold Networks的分治符号回归框架，结合简化策略和神经控制微分方程，能准确恢复物理方程并建模工程动力学。


<details>
  <summary>Details</summary>
Motivation: 改进传统遗传编程在符号回归上的局限，利用深度学习与数学结构化先验提高精确恢复解析表达式及动力学模型能力。

Method: 提出基于Kolmogorov Arnold Networks的分而治之框架，结合更专门化的KAN结构、平移对称性和可分解性等简化策略，并与神经控制微分方程整合用于动态系统建模。

Result: 在Feynman SRSD数据集上成功恢复真值方程；并在仿真生物工艺系统中证明与神经控制微分方程结合可精确建模动态过程。

Conclusion: KAN-SR利用KANs与拆分策略实现了高效符号回归，能在SRSD数据集上恢复真实表达式并用于动力学建模。

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [33] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: 提出了一种基于信息几何投影的参数化BFL个性化方法，等价于统计流形上的重心计算，能在不显著增加计算成本下实现可调的全局/本地性能折中。


<details>
  <summary>Details</summary>
Motivation: 在异构数据与隐私约束下，希望结合不确定性建模与去中心化训练以实现可靠且个性化的模型，同时避免MCMC或复杂变分推断带来的高计算成本。

Method: 在参数化贝叶斯模型上引入信息几何投影步骤，证明该步骤在统计流形上等价于计算重心，从而推导出闭式解并实现低成本个性化。将方法应用于变分学习并结合IVON优化器，同时扩展到通用聚合方案。

Result: 理论上给出投影等价于统计流形重心的证明并导出闭式解；在异构数据集上的实证结果显示该方法在保持全局表现的同时能提高本地性能，且计算开销低。

Conclusion: 该论文提出了基于信息几何投影的个性化参数化贝叶斯联邦学习框架，通过将全局模型投影到用户本地模型的邻域，实现了全局泛化与本地专化之间可调的折中。

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [34] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: 作者构建了标准化ECG基准BenchECG并提出xECG（xLSTM+SimDINOv2），在多数据集多任务评测中表现最好，推动ECG基础模型研究规范化。


<details>
  <summary>Details</summary>
Motivation: 目前ECG基础模型研究缺乏统一评测，任务选择狭窄且数据集不一致，导致无法公平比较。需要一个标准化的基准和强大的预训练模型以推动表示学习进展。

Method: 构建了标准化的BenchECG评测集（整合多项公开ECG数据集与多任务设置），提出xECG——基于xLSTM的循环模型，并采用SimDINOv2自监督学习进行预训练；在BenchECG上与现有公开模型比较。

Result: BenchECG提供了全面、可复现的评测框架；xECG在BenchECG上获得最高分，是唯一在所有数据集和任务上表现强劲的公开模型，从而设定了新的基线。

Conclusion: 该论文提出了BenchECG基准和xECG模型，解决了ECG基础模型评估不一致的问题，并在多个公开数据集和任务上取得了最优性能，成为新的基线。

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [35] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: 提出Fed-MARL：联邦多智能体DRQN+安全聚合，跨层协同优化6G边缘的任务卸载、频谱与能耗管理，兼顾性能与隐私，优于集中式和启发式基线。


<details>
  <summary>Details</summary>
Motivation: 6G边缘网络呈现超密集与智能化趋势，资源管理面临隐私、移动性和能耗等约束，需要一种能在分布式、动态与异构环境中高效且隐私保护的资源调度方案。

Method: 将资源管理建模为POMMDP，使用多目标奖励联合优化时延、能耗、频谱效率、公平性与可靠性；每个智能体采用DRQN处理部分可观测性并学习去中心化策略；引入椭圆曲线Diffie–Hellman密钥交换实现安全聚合以防半诚实攻击；采用联邦学习框架进行模型聚合以提高可扩展性与隐私性。

Result: 仿真显示Fed-MARL在任务成功率、时延、能效和公平性方面均优于集中式MARL和启发式方法，同时安全聚合确保在半诚实威胁下模型更新不泄露原始数据，且框架对设备规模和动态环境具有良好可扩展性与鲁棒性。

Conclusion: 该论文提出了一个结合联邦学习与多智能体强化学习的跨层资源管理框架（Fed-MARL），实现了在6G边缘异构设备上对于任务卸载、频谱访问和CPU能耗调节的去中心化优化。通过DRQN每个智能体在局部观测下学习策略，并引入基于椭圆曲线Diffie–Hellman的安全聚合协议保护模型更新隐私。总体上，Fed-MARL在任务成功率、时延、能效和公平性上均优于集中式MARL与启发式基线，并具备隐私性、可扩展性与对动态环境的鲁棒性。

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [36] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: 通过把表面码译码重构为回归问题并用神经网络对连续化的 syndrome 进行插值再优化，作者在多种网络和码距上获得了统一的译码性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统表面码译码器因输入的非唯一性只能学习到错误概率分布，导致预测不确定性。为减小这种不确定性并提高译码准确率，作者提出将分类问题重构为回归问题，通过连续化 syndrome 来使神经网络输出更可区分。

Method: 将 syndrome 测量用连续函数表示，并以神经网络对该连续表示进行插值与回归优化，具体采用多层感知器（MLP）进行再训练，并在卷积神经网络（CNN）、循环神经网络（RNN）和 transformer 架构上验证方法的泛化性。

Result: 在码距 5 和 7 的 MLP 译码器，以及码距 5 的 CNN、RNN、transformer 译码器上，重新优化后的译码器均表现出比原始模型更高的准确率，表明该方法不依赖于码距或网络结构具有普适性。

Conclusion: 本文提出通过将测得的 syndrome（稳定子测量）用连续函数近似并用神经网络插值，从而将表面码译码问题重构为回归问题，并对译码器进行再优化，结果在不同码距和网络结构上均提高了译码准确率。

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [37] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: 论文建立了深度发散且固定嵌入维度下ResNet的训练动力学与Neural Mean ODE的收敛关系，给出不同残差缩放下的误差界并区分非线性特征学习与懒惰线性化两种极限，针对两层块确定了导致完整特征学习的唯一缩放并以实验验证其紧致性。


<details>
  <summary>Details</summary>
Motivation: 理解深层ResNet在实际可扩展设置（大L、固定D、任意M）下的训练动力学与特征学习行为，特别是在不同残差缩放下何时产生非线性特征学习或退化为线性（懒惰）学习；并为诸如Transformer等模型提供理论依据。

Method: 通过把随机初始化下的前向和反向传播看作对某些平均ODE的随机逼近，并利用混沌传播（propagation of chaos）使单元间渐进独立，从而把训练动力学与确定性的Neural Mean ODE关联起来；结合残差缩放分析，推导误差界并用实验验证紧致性。

Result: 1) 在残差缩放Θ_D(α/(L M))下，得到误差界O_D(1/L + α/√(L M))，当α=Θ(1)时出现完全特征学习，α→∞时进入懒惰ODE；2) 对两层感知机块，唯一起完全特征学习的缩放为Θ(√D/(L M))，对应误差界O(1/L + √D/√(L M))；3) 数学工具为随机近似+混沌传播，实验验证了误差界的紧致性。

Conclusion: 该论文证明了对于深度L发散、固定嵌入维度D及任意隐藏宽度M的ResNet，从随机初始化开始使用基于梯度的训练，网络动力学收敛到一个神经平均ODE（Neural Mean ODE）。在不同残差缩放下，给出了与极限动力学的误差界，并区分了完全特征学习与懒惰(lazy)线性化两种极限行为。对两层感知机块的ResNet，确定了唯一能导致完全特征学习的残差缩放为Θ(√D/(L M))，并给出相应误差界。证明方法基于将前向/反向传播视为均值ODE的随机近似以及通过混沌传播保持独立性。

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [38] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: 提出一个可预训练的小块+混合CNN-Transformer骨干，结合可选seq2seq融合长程依赖，能高效学习并生成高分辨率3D湍流的确定性与概率模拟，扩展到512^3并在多任务PDE上表现优越。


<details>
  <summary>Details</summary>
Motivation: 需在高分辨率3D物理仿真中同时兼顾速度、精度和可扩展性，且能提供不确定性/概率输出以捕捉湍流统计。

Method: 设计了针对3D物理仿真的混合CNN-Transformer骨干，结合局部小块预训练与可选的seq2seq模块用于长程依赖，减少内存和计算开销；对比多种基线方法，训练并扩展到高分辨率数据；将网络作为扩散模型训练以生成概率样本。

Result: 在14类3D PDE上对比基线显示更优的速度与精度；成功扩展到512^3的各向同性湍流；作为扩散模型能生成不同Reynolds数下3D通道流的概率样本并准确复现流动统计。

Conclusion: 该论文提出了一个可扩展的混合CNN-Transformer骨干网络，用于学习高分辨率3D物理仿真的确定性和概率神经代理，可在速度和精度上超越现有架构，并能以小块预训练并拼接为全局解，支持长程依赖，可训练到512^3分辨率并用于扩散模型生成湍流概率样本。

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [39] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: 提出同时最小化负期望边距和边距方差并将权重重参数化到单位球的集成学习方法，提升泛化性与计算效率，实验证明优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 传统基于边距的集成方法只最大化期望边距，忽略边距方差导致泛化能力受限且易过拟合；权重在概率单纯形上的约束带来计算和可扩展性问题。

Method: 引入边距方差项到损失函数中，联合最小化负期望边距和方差；将权重从概率单纯形重新参数化到单位球上以简化约束优化并提升计算效率。

Result: 在多个基准数据集上的广泛实验显示，所提方法在准确率、鲁棒性和训练效率方面均优于传统方法。

Conclusion: 本文提出的框架通过同时优化负期望边距和边距方差，提高了集成模型在噪声和不平衡数据上的泛化能力，理论和实验均表明性能优于传统方法。

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [40] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: 本文提出并验证了一个用于机翼疲劳寿命估计的机器学习管线，能在保证精度与不确定性评估的前提下，减少 FEM 仿真与人工成本，作为传统方法的有力补充。


<details>
  <summary>Details</summary>
Motivation: 传统疲劳寿命预测依赖耗时的 FEM 仿真、载荷谱推导和循环计数等复杂流程，需多团队与工具协作；机器学习可提供更快的迭代和泛化能力，作为补充以减少计算与人工资源。

Method: 构建并训练机器学习模型（具体模型类型摘要中未说明），输入为任务级飞行参数、输出为机翼不同位置的疲劳寿命；在现实用例上验证，并进行了统计验证与不确定性量化。

Result: 在现实使用场景下，所提管线给出准确的预测，并伴有充分的统计验证与不确定性量化，表明可降低所需仿真次数与资源消耗。

Conclusion: 该论文提出了一个基于机器学习的管线，用于根据飞机不同任务的飞行参数预测机翼各位置的疲劳寿命，作为传统 FEM 等方法的补充，从而减少昂贵的仿真与人工成本。

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [41] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: 对2024 ICLR上1000篇论文用多种LLM生成审稿并注入隐藏提示的实证研究表明：简单提示注入能显著提高接受率（最高达100%），且LLM本身普遍偏向接受，表明LLM在同行评审中易被滥用并存在系统性偏差。


<details>
  <summary>Details</summary>
Motivation: 动机是回应关于作者使用隐藏提示（prompt injection）以操纵由LLM生成的同行评审的报道，评估此类攻击的可行性和实际影响，从而为关于LLM在学术评审中的使用安全性与道德性提供实证依据。

Method: 作者系统地使用多种LLM对2024年ICLR的1000篇论文生成审核，并在生成过程中加入不同复杂度的隐藏提示注入来测试操纵效果，统计接受率和评分变化，再分析模型间偏好差异。

Result: 结果显示：1) 简单的提示注入即可达到极高的操纵成功率（某些情形下接受率达100%）。2) 多数LLM在默认情况下对接受倾向明显偏高（许多模型的接受率超过95%）。两点均对学术同行评审流程的公正性构成严重挑战。

Conclusion: 本文结论为：简单的隐藏提示注入能显著操纵LLM生成的同行评审，使接受评分大幅上升，同时多数LLM本身对接受持偏向性，表明在科研同行评审中使用LLM存在严重风险。

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [42] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: 使用COSMO-RS模拟数据预训练NRS获得离子嵌入，再在实验数据上微调小型网络，通过迁移学习显著改善在稀疏实验数据下对IL五项性质的预测并实现大规模筛选。


<details>
  <summary>Details</summary>
Motivation: IL化学空间巨大且实验数据稀缺，直接用实验数据训练难以泛化；利用大量模拟数据+迁移学习以缓解数据稀疏性并提升预测性能。

Method: 两阶段方法：先在固定温度与压力下用COSMO-RS模拟数据预训练NRS以获取针对性质的离子嵌入；再将这些嵌入作为特征，微调简单前馈神经网络以拟合不同温压下的实验数据。支持同性质和跨性质迁移学习。

Result: 针对密度、粘度、表面张力、热容和熔点五种性质，使用预训练的密度、粘度和热容模型对五个目标性质进行微调，四项性质显著提高；模型能稳健外推到未见IL，并可对70万+种IL组合进行性质预测，适用于规模化筛选。

Conclusion: 本文提出了将基于COSMO-RS的模拟数据与实验数据相结合的迁移学习框架，使用神经推荐系统（NRS）学习离子液体（IL）中阳离子和阴离子的结构嵌入，并通过微调小型前馈神经网络以在稀疏实验数据下实现可靠的性质预测。

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [43] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: 提出CDQAC，一种面向JSP/FJSP的离线RL算法，通过分位数回报估计与保守策略更新从历史数据高效学习，样本需求低且表现优于多种基线，尤其在随机生成数据上效果意外更好。


<details>
  <summary>Details</summary>
Motivation: 在线RL方法样本效率低、需要大量仿真交互且随机初始化导致收敛慢，离线RL可直接利用历史数据并保留超越次优训练数据的潜力。

Method: CDQAC结合了分位数价值估计的critic和延迟策略更新，通过估计每个机床-工序对的回报分布而非直接选择动作，实现保守离散化决策；训练过程中采用离线数据批次学习以避免在线交互。

Result: 实验显示CDQAC在多种数据源下表现稳健，超越原始数据生成启发式、以及当前离线和在线RL基线；仅需10-20个训练实例即可学到高质量策略；在随机启发式生成的数据上表现优于在高级生成器（遗传算法、优先规则）上的训练。

Conclusion: 该论文提出了一种新的离线强化学习算法CDQAC，用于JSP和FJSP调度问题，能从历史数据学习高效策略并提升数据生成启发式性能。

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [44] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: 提出GraphCSVAE，用图模型与分类型VAE对时序卫星数据进行弱监督推断，刻画灾后物理脆弱性的时空转变，并在两处受灾社区展示了可解释的脆弱性动态。


<details>
  <summary>Details</summary>
Motivation: 尽管地震、洪水等灾害的危险性和暴露已经可通过遥感与数据驱动方法大尺度建模，但物理脆弱性（易损性）建模进展滞后，限制了评估减灾框架（如Sendai）进展的能力，尤其在灾后需持续监测脆弱性变化以指导决策。

Method: 设计Graph Categorical Structured VAE：采用图神经网络表征空间邻接与互相关，结合分类型潜变量与弱监督一阶转移矩阵来表示脆弱性类别间的时序转变；输入为卫星时序与外部先验信念，输出为每个空间单元的类别后验分布与转移概率。

Result: 在孟加拉国Khurushkul沿海与塞拉利昂Freetown泥石流受灾地区的案例研究中，GraphCSVAE揭示了灾后不同区域的脆弱性类别时空迁移模式，能够提供局部化的脆弱性审计线索并支持可持续的减灾策略制定。

Conclusion: 本文提出了一种面向物理脆弱性建模的新颖概率数据驱动框架GraphCSVAE，将图表示、变分自编码器和分类型概率推断融合，以利用时序卫星数据与专家先验来刻画灾后脆弱性时空演化。

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [45] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: 提出受ARIMA启发的双分支卷积模块ARMA，直接多步长序列预测，可扩展到多变量，简单有效，特别适合趋势显著的数据，并能内嵌位置信息。


<details>
  <summary>Details</summary>
Motivation: 传统长序列预测方法（如ARIMA或复杂深度模型）要么需要迭代多步预测、要么结构复杂且难以扩展到多变量。作者希望设计一个既简单又有效、能捕捉趋势与局部波动并直接进行多步预测的卷积模块。

Method: 设计了一个包含两部分卷积的模块：一部分模拟自回归用于捕捉趋势，另一部分模拟移动平均用于修正局部波动；直接输出多步预测而非迭代步骤；模块可无缝用于多变量预测，并且通过分析显示其卷积结构内嵌绝对位置信息。

Result: 在9个常用基准数据集上的实验显示，ARMA在整体精度上具有竞争力，尤其在趋势明显的数据集上优于其他方法；此外分析表明该模块能够编码绝对位置信息，有望作为轻量级的位置嵌入替代方案。

Conclusion: 该文提出了一种受ARIMA启发的卷积模块（ARMA），用于长序列时间序列预测，能够直接进行多步预测并扩展到多变量；在多个基准数据集上表现出与现有方法竞争的准确性，尤其在趋势变化强的数据集上表现突出，同时结构简单并可内在编码绝对位置信息，可能替代位置嵌入。

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [46] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: 对比学习的预训练队列组成会显著影响临床ECG基座模型的公平性与泛化；IDB策略能减少多中心预训练带来的OOD性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究对比自监督预训练对队列组成（人口学与健康状态）敏感性，并寻求提高模型在临床应用中对不同人群的公平性与泛化能力。

Method: 构建CAPE基座模型，在四个包含5,203,352个心电图样本的队列上进行对比学习预训练，系统评估不同队列的年龄/性别/健康状态和人口学多样性对下游任务（含来自两个人口体的额外队列）预测性能的影响；设计IDB训练策略，在批次采样过程中保持同队列样本一致性以减少队列伪影的学习。

Result: 发现下游性能受预训练队列分布属性影响；多中心多样化预训练提升同分布准确率但降低OOD泛化；IDB能在多数情况下提升OOD鲁棒性，减弱队列特异伪影的影响，同时维持或接近同分布性能。

Conclusion: 预训练队列组成显著影响对比学习表征质量；多中心多样化预训练提升同分布性能但会引入队列/队列特异伪影，削弱OOD泛化；提出的In-Distribution Batch(IDB)通过保留同队列内一致性来增强跨队列鲁棒性。

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [47] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: 作者用结合FEEC结构与Transformer的条件神经Whitney形式构建物理一致的数字孪生，实现实时数据同化与传感器自适应部署，显著改善复杂几何下的点源定位。


<details>
  <summary>Details</summary>
Motivation: 在复杂水动力-传输耦合系统中实现实时、物理一致的源定位需要既保留数值守恒结构又具备数据驱动的适应性；纯数据驱动模型在复杂几何下缺乏物理约束，精度受限，因此引入结构保持的数字孪生以提升定位可靠性。

Method: 提出条件神经Whitney形式(CNWF)，将有限元外微积分(FEEC)的结构性离散守恒与基于Transformer的算子学习相结合；通过条件注意力机制选取降维Whitney基、降维积分平衡方程和源场；采用交错方案在评估数字孪生与应用Lloyd算法之间交替以引导传感器布置；并用最优恢复方案以预测源场作为重要性函数实现点源恢复。

Result: 构建的CNWF数字孪生在复杂几何下较无物理约束的Transformer模型表现出更高的定位精度；提供了覆盖泛函单调改进的条件分析；在连续性假设下，展示了利用预测源场作为重要性函数可恢复点源的可行性。

Conclusion: 该论文构建了一个结合物理约束与Transformer算子学习的数字孪生框架，用于自适应源定位，证明在保守性和数值稳定性下能实时同化传感器数据并改进传感器布局，从而提高点源恢复精度。

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [48] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: 将数据浓缩问题形式化为基于分歧的分布近似问题，提出统一框架，能整合现有方法并扩展到鲁棒性、隐私等多目标设置。


<details>
  <summary>Details</summary>
Motivation: 现有DC方法多侧重于训练后泛化性能，且方法种类繁多、目标不一致；需要一个理论化、可对比且可扩展的框架来统一理解并扩展DC至如鲁棒性、隐私等其他目标。

Method: 通过引入分歧的形式化定义，将数据集浓缩视为用少量点近似原始数据分布的问题；在统一框架下分析并整合现有方法的目标与优化策略，允许在不同任务/范式下采用不同的分歧度量和约束。

Result: 构建了一个基于分歧的通用定义与优化目标，证明该框架能涵盖现有方法并自然扩展到其它目标，提供了更灵活的目标设计空间以实现鲁棒性、隐私等附加属性。

Conclusion: 本文提出了一个统一框架，将现有的数据浓缩(dataset condensation, DC)方法纳入其中，并将特定任务的DC推广为基于分歧(discrepancy)的更一般形式，从而超越单一的泛化目标。

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [49] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: 本文在无限维希尔伯特空间中给出rectified flow的严格函数形式，并将其扩展到functional flow matching和概率流ODEs，理论上放宽了先前的测度假设，且实验展示了更好的生成性能。


<details>
  <summary>Details</summary>
Motivation: 许多生成模型在有限维欧氏空间已有成功，但其在无限维函数空间（如随机过程、函数数据）上的推广尚不充分，特别是rectified flow尚未被扩展到函数空间；需要一个严格的理论框架以保证方法的可行性和拓展性。

Method: 基于无限维空间中连续性方程的叠加原理构建框架；将rectified flow推广到函数空间，并把functional flow matching与probability flow ODE解释为该框架下的非线性变体；在理论上论证存在性与一致性，并在实验上与现有函数生成模型比较性能。

Result: 提供了理论上的函数空间rectified flow构造，扩展到functional flow matching和probability flow ODEs，移除了kerrigan2024functional中的限制性测度假设；实验表明所提方法在性能上优于现有函数生成模型。

Conclusion: 该论文建立了在无限维希尔伯特空间中对rectified flow的严格函数空间表述，并将其推广到functional flow matching和functional probability flow ODEs，揭示它们为rectified flow的非线性推广，同时消除了现有工作中对测度论假设的限制。

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [50] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: VIG是一种面向数据集整体不确定性的主动学习策略，能在大幅减少标注量的情况下，保持接近全监督的物种识别性能，适合生态学和其他数据有限场景。


<details>
  <summary>Details</summary>
Motivation: 相机捕获的生物多样性图像标注昂贵且稀缺，传统主动学习侧重个体预测不确定性，忽视跨数据集的不确定性和多样性，限制了标注效率。

Method: 提出Vendi information gain (VIG)策略，基于对整个数据集预测分布的不确定性变化来评估每一批待标注样本的价值；在Snapshot Serengeti数据集上与常见基线（如不确定性采样、随机采样等）比较，并分析特征空间的多样性。

Result: 在Snapshot Serengeti数据集上，使用不到10%标注数据，VIG的预测准确度接近全监督训练，并持续优于基线方法，在不同批量大小和评估指标下均表现更好，同时采集到在特征空间上更为多样的样本。

Conclusion: VIG通过衡量对全数据集预测不确定性的减少来选择标注样本，有效兼顾信息性与多样性，从而在有限标注下显著提升物种识别性能。

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [51] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: 利用dLLMs的inpainting特性插入部分真实推理片段以指导RL探索，提出IGPO并配合集成训练技巧，在数学题解基准上显著提高样本效率与性能，达成masked dLLMs的新SOTA。


<details>
  <summary>Details</summary>
Motivation: dLLMs支持inpainting，可在强化学习中用作引导探索的工具，从而缓解稀疏奖励和样本浪费问题；希望在保持模型自生成推理的同时提高发现正确解的概率与训练效率。

Method: 提出在RL框架中使用inpainting插入部分ground-truth推理轨迹以引导探索；将该方法应用于基于组的优化方法（如GRPO）以恢复有意义的梯度；并结合对合成重写的简洁推理片段进行监督微调及基于熵的筛选等训练技巧。

Result: 在GSM8K、Math500和AMC三套数学基准上，结合IGPO与相关训练技巧，带来显著提升并实现对全注意力masked dLLMs的新SOTA成绩。

Conclusion: 该论文提出了IGPO（Inpainting Guided Policy Optimization），利用dLLMs的inpainting能力在在线采样时插入部分真实推理片段，改善强化学习中的探索问题，从而在数学题解任务上显著提升样本效率与性能，达到了masked dLLMs的新SOTA。

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [52] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe 用语义独立聚类加多极（包含偶极修正）近似注意力，在长序列下显著加速且误差可控，可作为注意力层的直接替换用于高效预训练与推理。


<details>
  <summary>Details</summary>
Motivation: 缓解 Transformer 在上下文长度 N 增长时二次复杂度带来的计算与内存瓶颈，同时保留注意力机制的精细信息以在预训练与任务中维持性能。

Method: 对查询和值（keys）在其学习到的表示空间中分别聚类，构建两阶段层次化注意力；使用质心（单极）近似并加入偶极（dipole）修正以捕捉簇内方向性方差；对因果注意力采用分层块分解结合精确局部计算与高效远程近似。时间复杂度为O(N C D)（非因果）和O(N C D log N)（因果）。

Result: 在孤立注意力层评估中，8k 上下文长度下相比 CUDNN Flash Attention 达到约3倍加速，相对平方误差低于20%；在 30M 参数模型、16k 上下文的端到端预训练中，实现12.2% 的运行时间减少，仅带来0.36% 的性能损失。

Conclusion: MuSe 提出了一种结合语义聚类和多极展开的高效注意力近似方法，实现了在长上下文下显著降低计算复杂度并保持较低误差率，适合作为标准注意力的无缝替换。

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [53] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 作者用过程挖掘+在线符合性检测检测ERTMS/ETCS L2的控制流异常，配合无监督学习实现异常定位，在RBC切换场景上验证了高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 尽管铁路软件经过严格验证与认证，运行时仍可能出现未知故障与威胁，因此需要运行时监测与快速定位机制来提高系统弹性。

Method: 从系统执行日志中通过过程挖掘学习实际控制流，并在运行时使用在线符合性检测来发现偏离；随后用无监督机器学习对偏差事件进行聚类/归因，实现异常定位。

Result: 在RBC/RBC切换场景上评估，方法在检测准确性、效率和可解释性方面表现良好，能够实时发现并定位控制流异常。

Conclusion: 该论文提出了一种基于过程挖掘的运行时控制流异常检测与定位方法，用于提升ERTMS/ETCS L2铁路系统的弹性，应对运行时残留故障、未知修改和网络威胁。

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [54] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: 本文研究Local SGD中外部优化器的作用，证明调节外学习率（甚至>1）和动量能在收敛率与噪声间取得更好权衡、补偿内学习率误设，并通过加速方法改善通信轮数相关收敛，理论与语言模型实验证实。


<details>
  <summary>Details</summary>
Motivation: 在分布式/大批量训练中通信成为瓶颈，Local SGD能减少通信但外部优化器的选择与超参调优尚不明确，需理解外学习率及动量如何影响收敛与噪声权衡。

Method: 作者在理论上分析了Local SGD，将算法分为局部优化、聚合机制与外部优化器三部分，证明了在不同设定下关于外学习率与动量调整的收敛性界；扩展到带动量及加速的外部优化器，推导通信轮数相关的改进收敛率；并给出数据相关的分析。最后通过大规模语言模型实验验证理论。

Result: 理论上证明外学习率可用来平衡优化误差与噪声、补偿内学习率调参失误；外学习率有时需>1；带动量和加速的外优化器能进一步提升通信轮数下的收敛性；数据依赖分析给出额外调参指引。实验在语言模型上支持理论结论。

Conclusion: 本文结论是：外部优化器（outer optimizer）及其学习率在Local SGD中起关键作用，合理调整外学习率可以在优化误差与随机梯度噪声之间权衡，并能弥补内部学习率的不良选择；当引入动量或加速方法时，外学习率的调整同样有效且可改善通信轮数相关的收敛率。此外，数据相关的分析为外学习率调参提供了进一步洞见。

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [55] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: AutoIND显著降低IND非临床总结初稿时间（约97%），质量尚可但需资深人员润色，无关键监管错误，指出了需改进的系统性缺陷。


<details>
  <summary>Details</summary>
Motivation: IND申请准备耗时且依赖专业知识，延缓早期临床开发。评估LLM平台能否在保持质量前提下缩短首次草稿撰写时间。

Method: 使用AutoIND自动生成eCTD模块2.6.2、2.6.4、2.6.6的草稿并直接记录撰写时间，与基于≥6年经验的监管撰稿人估算的手工撰写时间进行比较；质量由盲审监管写作评估者按七个指标评分（各0-3），归一化为百分比，并检验关键监管错误。

Result: 首次草稿时间约缩短97%（从约100小时降至3.7小时或2.6小时）；质量评分分别为69.6%（IND-1）和77.9%（IND-2）；未发现关键监管错误，但在强调、简洁性和清晰度方面存在不足。

Conclusion: AutoIND极大加速了IND非临床总结草稿的初始撰写速度，但仍需资深监管撰稿人对输出进行润色以达到提交质量。

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [56] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: 本文提出沿智能化与组合化两维演进的概念框架与架构蓝图，旨在把AI智能体整合入分布式科学工作流，推动实现自主化实验室与显著加速的科学发现。


<details>
  <summary>Details</summary>
Motivation: 当前科研需要协调分布式设施和异构资源，研究人员被迫手动管理工作流；AI智能体的进步提供了将智能作为生态系统组件的机会，但如何实际整合尚不清楚，因此提出框架与蓝图以促进落地。

Method: 提出二维演进路径（智能化：静态→智能；组合化：单体→群体），并基于此给出架构蓝图，用以指导将AI智能体集成到实际科学工作流中。

Result: 给出演进维度和架构蓝图，指出潜在带来100倍发现加速与变革性工作流的可能性，为社区下一步研究和系统构建提供方向。

Conclusion: 作者提出了一个概念框架，描绘从当前工作流管理到完全自主分布式科学实验室的发展路线，主张通过智能体和多智能体协作推动科学发现效率的大幅提升。

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [57] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: boldsea用可执行本体（BSL）和解释型引擎实现事件语义与数据流融合，支持运行时修改与时间透明，解决了传统BPM/语义方法的局限。


<details>
  <summary>Details</summary>
Motivation: 传统BPM和面向对象语义技术在动态系统建模、运行时可变性与逻辑/数据分离方面存在局限，需一种可执行语义模型来更自然地表达事件驱动的复杂动态系统。

Method: 提出BSL（BSL的BNF语法），并设计boldsea-engine直接解释语义模型为可执行算法，支持运行时修改事件模型与时间透明性，将数据与业务逻辑统一到语义框架内。

Result: 系统实现了无编译的解释执行、运行时修改模型、时间透明性以及数据与业务逻辑融合，展示了事件语义与数据流结合在动态过程控制中的可行性。

Conclusion: boldsea提出将事件语义与数据流架构结合，形成可执行本体以直接控制流程，从而克服传统BPM与面向对象语义技术的局限。

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [58] [How well can LLMs provide planning feedback in grounded environments?](https://arxiv.org/abs/2509.09790)
*Yuxuan Li,Victor Zhong*

Main category: cs.AI

TL;DR: 预训练大模型能为规划任务提供多样且高质量的反馈，模型越大、推理能力越强效果越好，但在复杂动力学或连续空间环境中效果下降。


<details>
  <summary>Details</summary>
Motivation: Reduce reliance on carefully designed reward functions and annotated demonstrations by leveraging pretrained foundation models' background knowledge to provide planning feedback.

Method: Evaluated LLMs and VLMs across domains and feedback types (binary, preference, action advising, goal advising, delta action), and inference methods (in-context learning, chain-of-thought, environment dynamics access), measuring accuracy, bias, and sensitivity to model size and inference technique.

Result: Foundation models provided diverse useful feedback; larger/reasoning models gave more accurate, less biased feedback and benefited more from advanced inference. Feedback quality degrades in environments with complex dynamics or continuous state/action spaces.

Conclusion: Foundation models (LLMs and VLMs) can supply varied, high-quality feedback for planning across symbolic, language, and continuous control tasks, with larger/reasoning models outperforming smaller ones; however feedback drops for complex dynamics and continuous spaces.

Abstract: Learning to plan in grounded environments typically requires carefully
designed reward functions or high-quality annotated demonstrations. Recent
works show that pretrained foundation models, such as large language models
(LLMs) and vision language models (VLMs), capture background knowledge helpful
for planning, which reduces the amount of reward design and demonstrations
needed for policy learning. We evaluate how well LLMs and VLMs provide feedback
across symbolic, language, and continuous control environments. We consider
prominent types of feedback for planning including binary feedback, preference
feedback, action advising, goal advising, and delta action feedback. We also
consider inference methods that impact feedback performance, including
in-context learning, chain-of-thought, and access to environment dynamics. We
find that foundation models can provide diverse high-quality feedback across
domains. Moreover, larger and reasoning models consistently provide more
accurate feedback, exhibit less bias, and benefit more from enhanced inference
methods. Finally, feedback quality degrades for environments with complex
dynamics or continuous state spaces and action spaces.

</details>


### [59] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: 作者提出用生成式AI从公开住宅信息合成能量建模所需数据的模块化框架，并演示了一个管道与评估，结果显示可生成逼真带标签数据，提升研究可及性与可重复性。


<details>
  <summary>Details</summary>
Motivation: 现实中的能量模型需要大量数据，但这些数据常常难以获取或有隐私与成本问题。作者希望通过生成式AI从公开信息合成所需数据，提升研究可及性和可重复性。

Method: 构建一个模块化管道，将公开的住宅元数据和图像作为输入，通过多个生成式AI模型（可能包括图像生成、结构化数据填充和标签生成等模块）合成具有真实感且带标签的数据，最后用于能量建模。

Result: 实验表明，框架能生成现实感强、带标签的数据，并且在生成式AI组件的评估中避免了生成模型的一些常见问题；整体上可降低对昂贵或受限数据源的依赖。

Conclusion: 该论文提出了一个模块化多模态框架，使用生成式AI从公开可得的住宅信息和图像合成能量模拟所需的数据，从而减少对不可及、昂贵或有隐私问题数据的依赖。

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [60] [Towards a Common Framework for Autoformalization](https://arxiv.org/abs/2509.09810)
*Agnieszka Mensfelt,David Tena Cucala,Santiago Franco,Angeliki Koutsoukou-Argyraki,Vince Trencsenyi,Kostas Stathis*

Main category: cs.AI

TL;DR: 将autoformalization定义为把非形式化输入翻译为形式化逻辑表示，综述相关领域工作，提出统一框架、评估指标与研究路线，旨在促进跨域协同与加速发展。


<details>
  <summary>Details</summary>
Motivation: 目前相关研究领域虽解决相似问题，但各自独立，缺乏共同的术语、基准和方法，阻碍了技术积累和跨域迁移；统一框架可以促进资源共享与协同进展。

Method: 综述现有文献，比较不同领域（数学形式化、程序合成、知识表示、规划与推理）中的工作，提出统一术语、任务定义与评估指标，并讨论模型架构、数据集、自动化流水线与人机协作的组合策略。

Result: 给出一个统一的概念框架、分类体系与研究路线图；总结若干挑战（可解释性、可证明的正确性、数据稀缺、跨域泛化、评估难题）并提出未来研究方向与建议，如构建大规模对齐数据、混合证明引擎与LLM、人机协作协议与标准基准集。

Conclusion: 本文提出将autoformalization扩展为“把非形式化输入映射到形式化逻辑表示”的统一任务，主张整合数学形式化与更广泛的语言到逻辑翻译研究，促进方法、基准与理论框架的共享。

Abstract: Autoformalization has emerged as a term referring to the automation of
formalization - specifically, the formalization of mathematics using
interactive theorem provers (proof assistants). Its rapid development has been
driven by progress in deep learning, especially large language models (LLMs).
More recently, the term has expanded beyond mathematics to describe the broader
task of translating informal input into formal logical representations. At the
same time, a growing body of research explores using LLMs to translate informal
language into formal representations for reasoning, planning, and knowledge
representation - often without explicitly referring to this process as
autoformalization. As a result, despite addressing similar tasks, the largely
independent development of these research areas has limited opportunities for
shared methodologies, benchmarks, and theoretical frameworks that could
accelerate progress. The goal of this paper is to review - explicit or implicit
- instances of what can be considered autoformalization and to propose a
unified framework, encouraging cross-pollination between different fields to
advance the development of next generation AI systems.

</details>


### [61] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: 提出基于RAG的山羊养殖知识助理：表格与决策树文本化融合异构知识，构建领域知识库并加在线检索，消融实验证明性能稳健，平均准确率约85%以上，主要问题为检索遗漏。


<details>
  <summary>Details</summary>
Motivation: LLM在畜牧业尤其是山羊养殖领域应用受限，主要因知识来源稀缺、格式多样且复杂，因而需要一种能整合异构数据并提升模型跨场景泛化能力的系统。

Method: 基于RAG框架，提出两种结构化知识处理方法：表格文本化与决策树文本化，构建覆盖疾病防治、营养管理、饲养管理、羊奶管理与基础养殖知识的领域知识库，并集成在线搜索模块实现实时信息检索；通过六组消融实验评估各组件贡献，使用文本/表格/决策树三类问答任务进行验证。

Result: 异构知识融合方法表现最佳，验证集平均准确率87.90%，测试集84.22%；在文本、表格与决策树问答任务上准确率均高于85%；主要错误类型为遗漏，提示可改进检索覆盖与上下文整合。

Conclusion: 本研究提出的智能知识助理系统在山羊养殖健康管理场景中表现稳定可靠，通过将多源异构知识结构化并结合RAG提升了LLM对不同数据格式的理解能力，实验结果显示融合表格与决策树的结构化知识能显著提高问答准确率。

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [62] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: 将解码器型LLM作为UNO代理可行，但大多数模型难以显著帮助队友赢；规模有益但效果有限。


<details>
  <summary>Details</summary>
Motivation: 研究LLM能否不仅回答问题，还能作为主动参与者在具体任务中为人类提供有效指导与协助，量化其在合作性博弈任务中的能力。

Method: 搭建工具让解码器型LLM接收完整博弈状态并通过两种提示策略以文本形式响应，在RLCard UNO环境中进行回合制交互；评估规模从1B到70B的模型，并与随机基线比较。

Result: 所有模型在打UNO时均超过随机基线，但只有少数模型能显著提高另一玩家的获胜概率；模型规模对性能有影响但并非万能。

Conclusion: 本文展示了将解码器型大语言模型接入RLCard UNO环境作为代理的可行性，但总体上模型很难作为协助者显著帮助同伴获胜。

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [63] [A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments](https://arxiv.org/abs/2509.09919)
*Franklin Yiu,Mohan Lu,Nina Li,Kevin Joseph,Tianxu Zhang,Julian Togelius,Timothy Merino,Sam Earle*

Main category: cs.AI

TL;DR: 把WFC当MDP，用WFC保证局部约束，外部优化器只关心全局目标，结果比同时优化约束和目标的方法好。


<details>
  <summary>Details</summary>
Motivation: 程序化内容生成需要同时满足设计目标与图块邻接约束，直接联合优化（同时考虑局部约束和全局目标）在复杂任务下效果差、安全收敛慢。作者希望通过将局部约束交给WFC传播处理，解耦出一个只需优化全局目标的MDP，从而提高优化效率和质量。

Method: 把WaveFunctionCollapse建模为马尔可夫决策过程：状态表示当前已确定的格子与候选图块集合，动作为选择在某位置放置某图块，转移由WFC传播完成（不可行动作被概率或约束过滤），奖励由设计目标给出；然后使用外部黑箱优化器（如进化算法、强化学习或随机搜索）在该MDP上优化累计奖励。

Result: 在多个不同难度和领域的实验中，基于WFC的MDP优化在达到目标值、收敛速度和鲁棒性上均优于传统的联合进化优化；联合优化随着任务复杂度上升表现显著下降。

Conclusion: 将WFC重构为MDP并用外部优化器只优化目标、由WFC传播保证约束，是可行且优于联合优化的方案。

Abstract: Procedural content generation often requires satisfying both
designer-specified objectives and adjacency constraints implicitly imposed by
the underlying tile set. To address the challenges of jointly optimizing both
constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a
Markov Decision Process (MDP), enabling external optimization algorithms to
focus exclusively on objective maximization while leveraging WFC's propagation
mechanism to enforce constraint satisfaction. We empirically compare optimizing
this MDP to traditional evolutionary approaches that jointly optimize global
metrics and local tile placement. Across multiple domains with various
difficulties, we find that joint optimization not only struggles as task
complexity increases, but consistently underperforms relative to optimization
over the WFC-MDP, underscoring the advantages of decoupling local constraint
satisfaction from global objective optimization.

</details>


### [64] [Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae](https://arxiv.org/abs/2509.09982)
*Stav Armoni-Friedmann,Hana Chockler,David A. Kelly*

Main category: cs.AI

TL;DR: 提出基于实际因果性的变量重要性度量，设计大规模基准并提出新XAI工具B-ReX，显著提高了黑盒解释质量（JSD≈0.072）。


<details>
  <summary>Details</summary>
Motivation: XAI评价主观且困难，尤其在表格数据与布尔函数预测场景下，需要客观、可计算的评估标准。

Method: 基于实际因果性定义变量重要性；构建大规模基准（随机10值布尔公式）；比较现有XAI工具并实现改进工具B-ReX（基于ReX）。

Result: B-ReX在随机10值布尔公式基准上实现Jensen-Shannon散度0.072±0.012，优于其它黑盒XAI工具。

Conclusion: 本文通过基于实际因果性的变量重要性度量，为评价XAI方法提供了形式化基准，并证明了新工具B-ReX在黑盒解释器中表现优越。

Abstract: Evaluating explainable AI (XAI) approaches is a challenging task in general,
due to the subjectivity of explanations. In this paper, we focus on tabular
data and the specific use case of AI models predicting the values of Boolean
functions. We extend the previous work in this domain by proposing a formal and
precise measure of importance of variables based on actual causality, and we
evaluate state-of-the-art XAI tools against this measure. We also present a
novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it
is superior to other black-box XAI tools on a large-scale benchmark.
Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\pm$ 0.012
on random 10-valued Boolean formulae

</details>


### [65] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: GAMA通过私有/公共空间分离与匿名机制，以及DRKE和DLE增强模块，有效在多智能体系统中兼顾性能与隐私保护，实验显示优于SOTA并能保护知识和逻辑隐私。


<details>
  <summary>Details</summary>
Motivation: 高性能LLM通常部署在远程服务器，直接将敏感数据发送给远程模型会带来隐私泄露风险。需要一种在保持任务性能同时保护隐私的多智能体系统方案。

Method: 提出GAMA架构；私有空间处理敏感数据，公共空间使用匿名化数据；引入DRKE（基于领域规则的知识增强）和DLE（基于反驳的逻辑增强）以减少匿名化造成的信息丢失；构建并评估了两个新隐私数据集用于隐私保护效果检验。

Result: 在Trivia Creative Writing和Logic Grid Puzzle两个公开问答数据集上，GAMA表现优于现有最先进模型；在作者设计的Knowledge Privacy Preservation和Logic Privacy Preservation数据集上，GAMA在任务性能和隐私保护方面均表现卓越。

Conclusion: 该论文提出了GAMA框架，通过将多智能体工作空间划分为私有空间和公共空间，并在公共空间仅传递匿名化信息，从而在利用远程高性能LLM时保护隐私。作者设计了两种增强模块DRKE和DLE以缓解匿名化带来的语义损失，并在多个数据集上验证了方法的有效性。

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [66] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents通过多极任务处理图和IF-THEN规则，改进了多智能体在复杂不确定任务中的动态规划与协作，实验显示优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统在面对高度复杂与不确定的真实任务时，任务规划能力欠缺，常产生误导或错误的输出，影响任务执行效果，需引入机制提升规划鲁棒性与协作一致性。

Method: 提出基于多极任务处理图（multipolar task processing graph）和域内及全局IF-THEN规则的统一多智能体协同框架。在任务规划阶段使用多极图进行动态分解与决策，在子任务执行阶段嵌入领域特定IF-THEN规则约束单体行为，并利用全局规则促进智能体间协作。评估包括三个数据集的知识型与逻辑型问答任务比较基线模型。

Result: 在三个不同数据集上的实验表明，XAgents在知识型与逻辑型问答任务上均稳定优于最先进的单智能体与多智能体方法，证明多极图与IF-THEN规则的结合能有效提高规划与执行质量。

Conclusion: XAgents通过多极任务处理图与IF-THEN规则结合，实现了对复杂、不确定任务的动态规划与约束控制，提升了多智能体在知识型与逻辑型问答任务上的性能，优于现有单/多智能体方法。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [67] [AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework](https://arxiv.org/abs/2509.10104)
*Sofia Vei,Paolo Giudici,Pavlos Sermpezis,Athena Vakali,Adelaide Emma Bernardelli*

Main category: cs.AI

TL;DR: 提出AI Harmonics與AIH指標：以序數嚴重度和事件資料為基礎，採人為本的傷害評估框架；發現政治與物理傷害是最需優先處理的類別。


<details>
  <summary>Details</summary>
Motivation: 現有AI風險評估多聚焦內部合規，忽略多元利益相關者視角與現實世界後果；因此需轉向以人為本、嚴重度自適應且基於實證事件數據的評估方式。

Method: 提出AIH指標，利用序數嚴重度（ordinal severity）而非精確數據，結合利益相關者觀點與資料驅動的方法論，對標註的事件資料進行分析與分布檢測以識別不均衡的傷害模式。

Result: 在標註事件資料上的實驗證實AIH能識別傷害集中區域—政治與物理傷害佔比最高，且有明顯不均勻分布；AI Harmonics能幫助政策制定者與組織更有效地針對緩解措施分配資源。

Conclusion: AI Harmonics提出了一種以人為本、基於事件數據的AI風險評估範式，強調利用序數嚴重度資料（ordinal severity）來衡量相對影響，避免對精確數值估計的依賴。實驗顯示政治與物理傷害在已標註事件資料中集中度最高，需優先緩解。

Abstract: The absolute dominance of Artificial Intelligence (AI) introduces
unprecedented societal harms and risks. Existing AI risk assessment models
focus on internal compliance, often neglecting diverse stakeholder perspectives
and real-world consequences. We propose a paradigm shift to a human-centric,
harm-severity adaptive approach grounded in empirical incident data. We present
AI Harmonics, which includes a novel AI harm assessment metric (AIH) that
leverages ordinal severity data to capture relative impact without requiring
precise numerical estimates. AI Harmonics combines a robust, generalized
methodology with a data-driven, stakeholder-aware framework for exploring and
prioritizing AI harms. Experiments on annotated incident data confirm that
political and physical harms exhibit the highest concentration and thus warrant
urgent mitigation: political harms erode public trust, while physical harms
pose serious, even life-threatening risks, underscoring the real-world
relevance of our approach. Finally, we demonstrate that AI Harmonics
consistently identifies uneven harm distributions, enabling policymakers and
organizations to target their mitigation efforts effectively.

</details>


### [68] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: 提出“沙箱经济”框架，分析AI代理经济的类型与风险，主张通过拍卖、任务经济和社会技术基础设施等设计实现可引导的代理市场，从而将AI经济转向有利于人类长期福祉的方向。


<details>
  <summary>Details</summary>
Motivation: 随着自主AI代理被快速采用，它们将以超越人类直接监管的规模与速度进行交易与协调，迫切需要一个分析与设计框架来理解风险与引导方向。

Method: 文中以框架构建与理论分析为主，沿两个维度（起源：自发 vs. 有意；与人类经济的隔离度：可渗透 vs. 不可渗透）对代理经济进行分类，并讨论可行的机制设计（如拍卖、任务经济）与信任与问责的社会技术设计。

Result: 推断当前轨迹趋向于自发形成且高度可渗透的代理经济，提出若干设计选项以实现资源公平分配、偏好解析与集体目标协调，并强调建立信任、安全与问责的基础设施以保障人类利益。

Conclusion: 该论文提出“沙箱经济”框架，认为自主AI代理将形成一个新兴经济层，具有巨大的协调潜力但也带来系统性风险与不平等，主张通过机制设计与社会技术基础设施主动引导其可控发展。

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [69] [Online Robust Planning under Model Uncertainty: A Sample-Based Approach](https://arxiv.org/abs/2509.10162)
*Tamir Shazman,Idan Lev-Yehudi,Ron Benchetit,Vadim Indelman*

Main category: cs.AI

TL;DR: 提出了面向RMDP的在线规划算法RSS，结合SAA实现可扩展的鲁棒采样规划，提供理论保证并在含不确定性的环境中表现更好。


<details>
  <summary>Details</summary>
Motivation: 实际应用中生成模型通常由有限数据学习得到，存在近似误差，导致传统采样方法（如Sparse Sampling、MCTS）可能性能下降或产生不安全行为；需要带有模型不确定性的鲁棒规划方法，同时要求能实时计算且有理论保证。

Method: 基于Sample Average Approximation（SAA）在采样树中估计鲁棒值函数，RSS通过从生成模型采样后构建近似不确定集并在每个节点求解SAA形式的鲁棒贝尔曼更新，从而在在线设置中高效计算鲁棒策略；算法适用于连续或无限状态空间，样本与计算复杂度与状态空间大小无关。

Result: 给出了RSS的理论性能保证（有限样本界、复杂度分析）并在不确定动力学的实验环境中展示RSS在回报及鲁棒性上优于标准Sparse Sampling。

Conclusion: 本文提出了首个具有有限样本理论保证的在线鲁棒规划算法——Robust Sparse Sampling（RSS），在存在模型不确定性的MDP环境中能有效计算鲁棒价值函数并在经验上优于标准Sparse Sampling。

Abstract: Online planning in Markov Decision Processes (MDPs) enables agents to make
sequential decisions by simulating future trajectories from the current state,
making it well-suited for large-scale or dynamic environments. Sample-based
methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely
adopted for their ability to approximate optimal actions using a generative
model. However, in practical settings, the generative model is often learned
from limited data, introducing approximation errors that can degrade
performance or lead to unsafe behaviors. To address these challenges, Robust
MDPs (RMDPs) offer a principled framework for planning under model uncertainty,
yet existing approaches are typically computationally intensive and not suited
for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the
first online planning algorithm for RMDPs with finite-sample theoretical
performance guarantees. Unlike Sparse Sampling, which estimates the nominal
value function, RSS computes a robust value function by leveraging the
efficiency and theoretical properties of Sample Average Approximation (SAA),
enabling tractable robust policy computation in online settings. RSS is
applicable to infinite or continuous state spaces, and its sample and
computational complexities are independent of the state space size. We provide
theoretical performance guarantees and empirically show that RSS outperforms
standard Sparse Sampling in environments with uncertain dynamics.

</details>


### [70] [Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction](https://arxiv.org/abs/2509.10210)
*Marko Petković,Vlado Menkovski,Sofía Calero*

Main category: cs.AI

TL;DR: 作者提出并实现了一个LLM驱动的多智能体系统，用于文献驱动的力场提取和自动化RASPA模拟设置，初步结果显示高正确性与可重复性，凸显实现全自动化材料表征的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前多孔材料表征受限于模拟设置复杂性和力场选择困难，限制了高通量和自动化材料发现。主要动机是通过自动化流程减少人工干预，加速材料表征和发现。

Method: 构建了一个多智能体系统，重点实现文献驱动的力场提取模块和自动化的RASPA模拟设置。系统利用LLM解析文献获取力场参数，多个代理协同完成任务分解、模拟脚本生成和执行。

Result: 初步评估显示该系统在力场提取和RASPA设置方面具有高正确性和可重复性，表明该方法有潜力实现可扩展的全自动材料表征流程。

Conclusion: 本文提出了一个多智能体框架，使基于大语言模型（LLM）的代理能够自主理解多孔材料表征任务、规划模拟、组装力场、执行模拟并解释结果，为自动化材料表征迈出重要一步。

Abstract: Automated characterization of porous materials has the potential to
accelerate materials discovery, but it remains limited by the complexity of
simulation setup and force field selection. We propose a multi-agent framework
in which LLM-based agents can autonomously understand a characterization task,
plan appropriate simulations, assemble relevant force fields, execute them and
interpret their results to guide subsequent steps. As a first step toward this
vision, we present a multi-agent system for literature-informed force field
extraction and automated RASPA simulation setup. Initial evaluations
demonstrate high correctness and reproducibility, highlighting this approach's
potential to enable fully autonomous, scalable materials characterization.

</details>


### [71] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: 作者提出CARENLI，通过对推理任务的分类路由与可审计三段式推理（planner/verifier/refiner），在临床NLI上显著提高可靠性，证明LLM保留事实但在推理欠定时依赖启发式，路由/分类是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 检验在临床自然语言推理中，增加模型与数据规模是否自发产生结构化、可泛化的内部推理表征；并提出可审计且模块化的方法以提高模型在细粒度临床推理任务中的可靠性。

Method: 构建包含四类推理（因果归因、组合性对地、认知验证、风险状态抽象）的基准，并提出CARENLI：将给定的前提-陈述对路由到家族特定求解器，通过planner、verifier、refiner三步实现可审计推理；在四种大模型上进行实验评估。

Result: CARENLI在四种LLM上显著提高保真度：因果归因达98.0%、风险状态抽象达81.2%，整体可提升至42个百分点；验证器能够高可靠地标记违规，refiner能修正大量认知错误；主要失败集中在路由模块，表明家族分类是瓶颈。

Conclusion: 该论文挑战了“规模化可带来结构化、可泛化内部表示”的普遍假设，提出CARE NLI框架以区分知识访问与推理，展示在临床NLI任务上能显著提升推理保真度并提供可审计流程。

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [72] [Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering](https://arxiv.org/abs/2509.10249)
*Hanna Abi Akl*

Main category: cs.AI

TL;DR: 将自然语言替换为更紧凑的逻辑语法，能在小型语言模型上保持良好推理性能，表明形式方法对于本体工程中的SLMs引导具有潜力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在推理方面存在明显不足，影响到本体工程等任务。研究旨在探索将形式方法（如更结构化的逻辑表示）引入以增强小型语言模型在推理相关任务中的性能，尤其用于本体构建的引导或引爆（bootstrap）。

Method: 通过设置初步实验，将同一预定义推理任务分别用不同语法（自然语言和更紧凑的逻辑语言）表达，使用小型语言模型（SLMs）进行测试，比较其在推理任务上的表现差异。

Result: 实验结果显示，替换自然语言为更紧凑的逻辑语言可以在保持或接近原有性能的情况下，简化问题表达，从而可能提升SLMs在本体工程中执行推理任务的可靠性和效率。

Conclusion: 作者通过实验表明，用更紧凑的逻辑语言替换自然语言（NL）在一定程度上能保持小型语言模型（SLMs）在推理任务上的较强表现，表明将形式方法引入本体工程有潜力提升SLMs的推理能力与可用性。

Abstract: Recent advances in Language Models (LMs) have failed to mask their
shortcomings particularly in the domain of reasoning. This limitation impacts
several tasks, most notably those involving ontology engineering. As part of a
PhD research, we investigate the consequences of incorporating formal methods
on the performance of Small Language Models (SLMs) on reasoning tasks.
Specifically, we aim to orient our work toward using SLMs to bootstrap ontology
construction and set up a series of preliminary experiments to determine the
impact of expressing logical problems with different grammars on the
performance of SLMs on a predefined reasoning task. Our findings show that it
is possible to substitute Natural Language (NL) with a more compact logical
language while maintaining a strong performance on reasoning tasks and hope to
use these results to further refine the role of SLMs in ontology engineering.

</details>


### [73] [The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis](https://arxiv.org/abs/2509.10297)
*Eoin O'Doherty,Nicole Weinrauch,Andrew Talone,Uri Klempner,Xiaoyuan Yi,Xing Xie,Yi Zeng*

Main category: cs.AI

TL;DR: 实验比较六个LLM在18个道德困境上的表现，结论是模型总体偏向关怀/美德价值，惩罚自由主义选择，推理能力提升解释与情境敏感性；强调可解释性与文化意识对AI对齐的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI决策能力增强，需了解其隐含的道德偏好与解释能力，评估是否能与人类实现道德上的共生与可信协作。

Method: 选取六个代表性大语言模型，设计18个道德困境覆盖五种道德框架，要求模型对每个选项进行排序和评分；比较模型架构（是否含推理能力）、文化来源与可解释性表现，统计评分差异并分析说明质量。

Result: 发现关怀与美德价值被普遍高估；具有推理模块的模型对情境更敏感并能提供更丰富解释；非推理模型给出更一致但不透明的判断；不同文化背景模型差异存在但总体偏好趋同。

Conclusion: 这些模型普遍偏向关怀（Care）与美德（Virtue）导向的道德判断，惩罚自由主义（Libertarian）选项，并在不同体系间表现出一致的价值偏好。

Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent
questions about how to align machine decision-making with human moral values.
This working paper investigates how leading AI systems prioritize moral
outcomes and what this reveals about the prospects for human-AI symbiosis. We
address two central questions: (1) What moral values do state-of-the-art large
language models (LLMs) implicitly favour when confronted with dilemmas? (2) How
do differences in model architecture, cultural origin, and explainability
affect these moral preferences? To explore these questions, we conduct a
quantitative experiment with six LLMs, ranking and scoring outcomes across 18
dilemmas representing five moral frameworks. Our findings uncover strikingly
consistent value biases. Across all models, Care and Virtue values outcomes
were rated most moral, while libertarian choices were consistently penalized.
Reasoning-enabled models exhibited greater sensitivity to context and provided
richer explanations, whereas non-reasoning models produced more uniform but
opaque judgments. This research makes three contributions: (i) Empirically, it
delivers a large-scale comparison of moral reasoning across culturally distinct
LLMs; (ii) Theoretically, it links probabilistic model behaviour with
underlying value encodings; (iii) Practically, it highlights the need for
explainability and cultural awareness as critical design principles to guide AI
toward a transparent, aligned, and symbiotic future.

</details>


### [74] [State Algebra for Propositional Logic](https://arxiv.org/abs/2509.10326)
*Dmitry Lesnik,Tobias Schäfer*

Main category: cs.AI

TL;DR: 提出 State Algebra：一个以代数方法表示命题逻辑的三层框架，非规范约简带来灵活性与潜在更紧凑表示，通过固定变量顺序可恢复唯一规范形式，并可扩展到概率与加权计数。


<details>
  <summary>Details</summary>
Motivation: 将命题逻辑的表示与操作代数化，兼顾语义基础与代数计算优势，寻求在表示紧凑性与规范性之间的可控折衷，并拓展到概率与加权计数场景。

Method: 建立三层表示层次（Set、Coordinate、Row Decomposition），并用代数引擎在这些表示上进行状态向量的约简与操作；分析了默认约简非规范性及通过固定变量顺序实现规范化的方法；扩展讨论包括搜索与知识编译算法的表达，以及向概率逻辑和加权模型计数的自然推广。

Result: 提出并形式化了 State Algebra 框架，证明了通过变量顺序可获得规范形式，讨论了其在某些问题上可能更紧凑且能表达搜索与知识编译流程，并示范其可扩展至概率逻辑与加权模型计数。

Conclusion: State Algebra 提供了一种用代数方法表示和操作命题逻辑的新框架，强调表示灵活性与计算可操作性之间的权衡；通过固定变量顺序可获得唯一的规范形式，从而恢复规范性。

Abstract: This paper presents State Algebra, a novel framework designed to represent
and manipulate propositional logic using algebraic methods. The framework is
structured as a hierarchy of three representations: Set, Coordinate, and Row
Decomposition. These representations anchor the system in well-known semantics
while facilitating the computation using a powerful algebraic engine. A key
aspect of State Algebra is its flexibility in representation. We show that
although the default reduction of a state vector is not canonical, a unique
canonical form can be obtained by applying a fixed variable order during the
reduction process. This highlights a trade-off: by foregoing guaranteed
canonicity, the framework gains increased flexibility, potentially leading to
more compact representations of certain classes of problems. We explore how
this framework provides tools to articulate both search-based and knowledge
compilation algorithms and discuss its natural extension to probabilistic logic
and Weighted Model Counting.

</details>


### [75] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P通过三步因果化推理（归纳-干预-预测）弥补了现有方法的反事实推理缺陷，显著提高失败归因的步级准确率，增强了可验证性。


<details>
  <summary>Details</summary>
Motivation: 现有方法作为模式识别处理长会话日志，缺乏反事实推理能力，步级精度低，难以用于复杂多智能体系统的调试。

Method: 提出Abduct-Act-Predict (A2P) Scaffolding：在单次推理中引导大模型按三步走（1）Abduction：推断行为背后的隐含根因；（2）Action：指定最小修正干预；（3）Prediction：模拟干预后的轨迹以验证是否消除了失败。

Result: 在Who&When基准上取得显著提升：Algorithm-Generated数据集步级准确率从16.67%提升到47.46%；Hand-Crafted数据集从12.07%提升到29.31%。

Conclusion: A2P框架通过将失败归因重构为因果推理任务，显著提升了步级别失败定位准确率，提供了更可验证的调试工具。

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [76] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: 提出信息论诊断框架：学习过程产生可识别的信息签名，且不同故障类型在互信息通道上有不同表现，可用于无架构改动的在线故障检测与定位。


<details>
  <summary>Details</summary>
Motivation: 现实环境中RL代理会遭遇传感器故障、执行器磨损和环境变化，但缺乏内生的异常检测与诊断机制。基于信息论的方法可以提供与策略行为和环境交互相关的普适度量，便于实时监测系统健康。

Method: 通过在机器人控制任务中计算状态、动作和下一状态之间的互信息及其随训练时间的变化，分析信息签名；并通过受控扰动实验在观测空间和动作空间分别引入噪声，比较各类互信息通道（如MI(S;A)、MI(A;S')、MI(S,A;S')）的响应差异，实现诊断。

Result: 实验中MI(S;A)从0.84比特增长到2.83比特（增长238%），MI(S,A;S')在早期学习达到峰值后下降，表明从广泛探索向高效利用的转变。观测噪声引起多个信息通道崩塌并明显降低状态-动作耦合；动作噪声主要破坏动作-结果可预测性而保留状态-动作关系。

Conclusion: 本文提出基于信息论的框架，用于揭示强化学习（RL）过程的内在动力学并诊断部署时的异常。研究发现：学习成功时状态-动作互信息（MI(S;A)）显著增加，而联合互信息MI(S,A;S')呈现倒U型曲线；信息度量能区分观测空间噪声（传感器故障）与动作空间噪声（执行器故障），从而实现故障定位，无需改动模型结构。

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [77] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: 对抗性测试显示Flan-T5与RoBERTa-Base鲁棒，BERT-Base脆弱；提出需在防御有效性与计算成本间权衡并优化防御策略。


<details>
  <summary>Details</summary>
Motivation: 评估当前大语言模型在现实对抗攻击场景下的安全性与鲁棒性，识别脆弱模型并为更高效的防御策略提供建议。

Method: 使用TextFooler和BERTAttack对Flan-T5、BERT-Base和RoBERTa-Base进行系统性对抗测试，通过测量攻击成功率与模型准确率下降来评估鲁棒性。

Result: RoBERTa-Base与Flan-T5在实验中攻击成功率为0%，保持高准确率；BERT-Base在TextFooler攻击下成功率为93.75%，准确率从48%降至3%。研究指出防御有效但计算开销大。

Conclusion: 该研究表明不同LLM在面对对抗性攻击时表现差异显著，RoBERTa-Base和Flan-T5展现出强鲁棒性，而BERT-Base易受攻击。研究强调有效防御机制通常伴随高计算成本。

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


### [78] [ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)](https://arxiv.org/abs/2509.09787)
*Nojan Sheybani,Alessandro Pegoraro,Jonathan Knauer,Phillip Rieger,Elissa Mollakuqe,Farinaz Koushanfar,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: 利用交互式零知识证明和频率表示，在客户端实现可验证、私密且高效的Split Learning防御，有效阻止后门注入，同时保持低运行开销。


<details>
  <summary>Details</summary>
Motivation: Split Learning环境下，客户端受限且分布式，易被恶意客户端通过中间梯度注入后门攻击。现有防御多集中在服务器端并增加服务器负担；而在客户端实施防御面临无法强制恶意客户端遵从的问题。需要一种既保护隐私又能验证客户端行为的方法。

Method: 设计并应用交互式ZKP，使客户端能证明其正确执行防御算法。使用模型的频谱（频率）表示对本地训练模型分区进行深度检查，确保客户端传递给下游的模型检查点是良性的。实现轻量化证明流程，保证在百万参数级别客户端模型上开销小（<10秒）。

Result: 在多种模型架构、攻击策略和数据场景下评估，ZORRO将攻击成功率降至<6%，且在客户端保存百万级参数模型时证明生成开销低于10秒，证明了方案的有效性和实用性。

Conclusion: ZORRO通过交互式零知识证明（ZKP）在客户端实施可验证的防御，保证本地训练模型分区的计算完整性，从而显著降低了Split Learning中的后门攻击成功率。

Abstract: Split Learning (SL) is a distributed learning approach that enables
resource-constrained clients to collaboratively train deep neural networks
(DNNs) by offloading most layers to a central server while keeping in- and
output layers on the client-side. This setup enables SL to leverage server
computation capacities without sharing data, making it highly effective in
resource-constrained environments dealing with sensitive data. However, the
distributed nature enables malicious clients to manipulate the training
process. By sending poisoned intermediate gradients, they can inject backdoors
into the shared DNN. Existing defenses are limited by often focusing on
server-side protection and introducing additional overhead for the server. A
significant challenge for client-side defenses is enforcing malicious clients
to correctly execute the defense algorithm.
  We present ZORRO, a private, verifiable, and robust SL defense scheme.
Through our novel design and application of interactive zero-knowledge proofs
(ZKPs), clients prove their correct execution of a client-located defense
algorithm, resulting in proofs of computational integrity attesting to the
benign nature of locally trained DNN portions. Leveraging the frequency
representation of model partitions enables ZORRO to conduct an in-depth
inspection of the locally trained models in an untrusted environment, ensuring
that each client forwards a benign checkpoint to its succeeding client. In our
extensive evaluation, covering different model architectures as well as various
attack strategies and data scenarios, we show ZORRO's effectiveness, as it
reduces the attack success rate to less than 6\% while causing even for models
storing \numprint{1000000} parameters on the client-side an overhead of less
than 10 seconds.

</details>


### [79] [SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2509.09942)
*Lei Yu,Jingyuan Zhang,Xin Wang,Jiajia Ma,Li Yang,Fengjun Zhang*

Main category: cs.CR

TL;DR: 提出SmartCoder-R1，通过CPT、L-CoT SFT和S-GRPO三阶段训练，显著降低智能合约漏洞并提高生成推理的可解释性与质量，在多个自动与人工指标上达新高。


<details>
  <summary>Details</summary>
Motivation: LLM在生成智能合约时既缺乏可审计的推理路径（黑箱问题），又容易产生严重安全漏洞，需同时提升可解释性与安全性。

Method: 采用三阶段训练流程：Continual Pre-training（CPT）针对领域适应；Long Chain-of-Thought Supervised Fine-Tuning（L-CoT SFT）用7,998条专家标注的推理+代码样本教授安全分析；Security-Aware Group Relative Policy Optimization（S-GRPO）通过强化学习优化一个加权奖励（编译成功、安全合规、格式正确）以直接降低漏洞。

Result: 在756个真实函数的基准测试上，SmartCoder-R1在ComPass(87.70%)、VulRate(8.60%)、SafeAval(80.16%)、FuncRate(53.84%)、FullRate(50.53%)等五项指标上领先，FullRate比最强基线DeepSeek-R1相对提升45.79%；人工评估中，功能82.7%、安全85.3%、清晰度90.7%。

Conclusion: SmartCoder-R1 能显著提高智能合约生成的安全性与可解释性，在多项指标上超越现有基线，且生成的推理在人工评估中表现优异。

Abstract: Smart contracts automate the management of high-value assets, where
vulnerabilities can lead to catastrophic financial losses. This challenge is
amplified in Large Language Models (LLMs) by two interconnected failures: they
operate as unauditable "black boxes" lacking a transparent reasoning process,
and consequently, generate code riddled with critical security vulnerabilities.
To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a
novel framework for secure and explainable smart contract generation. It begins
with Continual Pre-training (CPT) to specialize the model. We then apply Long
Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated
reasoning-and-code samples to train the model to emulate human security
analysis. Finally, to directly mitigate vulnerabilities, we employ
Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement
learning phase that refines the generation policy by optimizing a weighted
reward signal for compilation success, security compliance, and format
correctness. Evaluated against 17 baselines on a benchmark of 756 real-world
functions, SmartCoder-R1 establishes a new state of the art, achieving top
performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a
SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This
FullRate marks a 45.79% relative improvement over the strongest baseline,
DeepSeek-R1. Crucially, its generated reasoning also excels in human
evaluations, achieving high-quality ratings for Functionality (82.7%), Security
(85.3%), and Clarity (90.7%).

</details>


### [80] [Byte by Byte: Unmasking Browser Fingerprinting at the Function Level Using V8 Bytecode Transformers](https://arxiv.org/abs/2509.09950)
*Pouneh Nikkhah Bahrami,Dylan Cutler,Igor Bilogrevic*

Main category: cs.CR

TL;DR: 基于V8字节码和Transformer的函数级指纹检测与编译期签名匹配，可实现低开销、抗混淆且精确的实时指纹防护。


<details>
  <summary>Details</summary>
Motivation: 现有基于脚本或AST的检测易受代码混淆与URL绕过影响，或采用脚本级阻断导致网站功能损坏；需提供函数级精确检测并对混淆具有鲁棒性的解决方案。

Method: 使用Transformer对V8字节码序列进行离线训练以识别指纹函数，并提取轻量签名以在编译期对字节码进行实时匹配与阻断。

Result: 在Top 100k网站上测试，ByteDefender在函数与脚本级别均表现出高检测准确率，较AST方法在抗混淆能力上有显著提升，且页面加载平均仅增加4%延迟。

Conclusion: ByteDefender通过在V8字节码层检测JavaScript函数级指纹跟踪行为，实现了高精度且对抗混淆的指纹识别与阻断。

Abstract: Browser fingerprinting enables persistent cross-site user tracking via subtle
techniques that often evade conventional defenses or cause website breakage
when script-level blocking countermeasures are applied. Addressing these
challenges requires detection methods offering both function-level precision to
minimize breakage and inherent robustness against code obfuscation and URL
manipulation.
  We introduce ByteDefender, the first system leveraging V8 engine bytecode to
detect fingerprinting operations specifically at the JavaScript function level.
A Transformer-based classifier, trained offline on bytecode sequences,
accurately identifies functions exhibiting fingerprinting behavior. We develop
and evaluate light-weight signatures derived from this model to enable
low-overhead, on-device matching against function bytecode during compilation
but prior to execution, which only adds a 4% (average) latency to the page load
time. This mechanism facilitates targeted, real-time prevention of
fingerprinting function execution, thereby preserving legitimate script
functionality. Operating directly on bytecode ensures inherent resilience
against common code obfuscation and URL-based evasion. Our evaluation on the
top 100k websites demonstrates high detection accuracy at both function- and
script-level, with substantial improvements over state-of-the-art AST-based
methods, particularly in robustness against obfuscation. ByteDefender offers a
practical framework for effective, precise, and robust fingerprinting
mitigation.

</details>


### [81] [Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching](https://arxiv.org/abs/2509.09970)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.CR

TL;DR: 将LLM固件生成与自动化安全检测+AI驱动的迭代修复结合，可在虚拟化环境中显著提高嵌入式固件的安全性与实时性能，实验展示高修复率与合规性并提供开源数据。


<details>
  <summary>Details</summary>
Motivation: LLM生成固件虽提高开发效率，但易引入安全缺陷并可能不满足嵌入式系统的实时性要求，需构建自动化验证与修复流程以降低风险并确保性能。

Method: 提出三阶段方法：1) 使用结构化提示词让GPT-4等模型生成针对网络和控制任务的固件；2) 在QEMU上运行FreeRTOS进行模糊测试、静态分析和运行时监控以检测CWE-120、CWE-362、CWE-400等漏洞；3) 通过威胁检测、性能优化和合规模块的AI代理对检测到的问题进行分类（按CWE）并生成针对性补丁，循环迭代直至满足安全和实时性目标。

Result: 实验结果：漏洞修复率92.4%（提升37.3%），威胁模型合规率95.8%，安全覆盖指数0.87；实时性能：最坏执行时间8.6ms，抖动195μs；并发布开源数据集。

Conclusion: 该方法通过将LLM生成固件与自动化安全验证和迭代修正结合，在虚拟化环境中显著提升了嵌入式固件的安全性与实时性能，实验证明漏洞修复率和合规性均有较高提升。

Abstract: Large Language Models (LLMs) show promise in generating firmware for embedded
systems, but often introduce security flaws and fail to meet real-time
performance constraints. This paper proposes a three-phase methodology that
combines LLM-based firmware generation with automated security validation and
iterative refinement in a virtualized environment. Using structured prompts,
models like GPT-4 generate firmware for networking and control tasks, deployed
on FreeRTOS via QEMU. These implementations are tested using fuzzing, static
analysis, and runtime monitoring to detect vulnerabilities such as buffer
overflows (CWE-120), race conditions (CWE-362), and denial-of-service threats
(CWE-400). Specialized AI agents for Threat Detection, Performance
Optimization, and Compliance Verification collaborate to improve detection and
remediation. Identified issues are categorized using CWE, then used to prompt
targeted LLM-generated patches in an iterative loop. Experiments show a 92.4\%
Vulnerability Remediation Rate (37.3\% improvement), 95.8\% Threat Model
Compliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms
worst-case execution time and 195{\mu}s jitter. This process enhances firmware
security and performance while contributing an open-source dataset for future
research.

</details>


### [82] [rCamInspector: Building Reliability and Trust on IoT (Spy) Camera Detection using XAI](https://arxiv.org/abs/2509.09989)
*Priyanka Rushikesh Chaudhary,Manan Gupta,Jabez Christopher,Putrevu Venkata Sai Charan,Rajib Ranjan Maiti*

Main category: cs.CR

TL;DR: 提出 rCamInspector，結合 XGBoost 與 SHAP/LIME 對 IoT 攝像頭流量分類提供高準確且有信度的解釋。


<details>
  <summary>Details</summary>
Motivation: 解決 ML 模型作為黑盒導致在 IoT 網路安全決策中缺乏可解釋性與可信度問題，特別針對 IoT 攝像頭流量分類。

Method: 構建兩層分類器：Flow Classifier（四類：IoTCam, Conf, Share, Others）和 SmartCam Classifier（六類具體攝像頭品牌），使用八種監督式 ML 模型進行比較，最終選擇 XGBoost（XGB）為主模型；並用 SHAP 和 LIME 作為解釋器評估模型可解釋性，同時與互信息（MI）特徵重要性對比。

Result: 在 38GB 網路流量上實驗：XGB 在 Flow 與 SmartCam 分類器上分別達到 92% 與 99% 的準確率；SHAP 與 LIME 能提供一致性>0.7 及充分性=1.0，並指出例如 Init Bwd Win Byts 為重要解釋特徵；與現有工作比較，在準確率、精確率與假陰性率上均有改進。

Conclusion: rCamInspector 成功利用 XAI 提供對 IoT 攝像頭流量分類模型輸出的可解釋性，並在實驗中展示了高準確度與可靠性。

Abstract: The classification of network traffic using machine learning (ML) models is
one of the primary mechanisms to address the security issues in IoT networks
and/or IoT devices. However, the ML models often act as black-boxes that create
a roadblock to take critical decision based on the model output. To address
this problem, we design and develop a system, called rCamInspector, that
employs Explainable AI (XAI) to provide reliable and trustworthy explanations
to model output. rCamInspector adopts two classifiers, Flow Classifier -
categorizes a flow into one of four classes, IoTCam, Conf, Share and Others,
and SmartCam Classifier - classifies an IoTCam flow into one of six classes,
Netatmo, Spy Clock, Canary, D3D, Ezviz, V380 Spy Bulb; both are IP address and
transport port agnostic. rCamInspector is evaluated using 38GB of network
traffic and our results show that XGB achieves the highest accuracy of 92% and
99% in the Flow and SmartCam classifiers respectively among eight supervised ML
models. We analytically show that the traditional mutual information (MI) based
feature importance cannot provide enough reliability on the model output of XGB
in either classifiers. Using SHAP and LIME, we show that a separate set of
features can be picked up to explain a correct prediction of XGB. For example,
the feature Init Bwd Win Byts turns out to have the highest SHAP values to
support the correct prediction of both IoTCam in Flow Classifier and Netatmo
class in SmartCam Classifier. To evaluate the faithfulness of the explainers on
our dataset, we show that both SHAP and LIME have a consistency of more than
0.7 and a sufficiency of 1.0. Comparing with existing works, we show that
rCamInspector achieves a better accuracy (99%), precision (99%), and false
negative rate (0.7%).

</details>


### [83] [Why Data Anonymization Has Not Taken Off](https://arxiv.org/abs/2509.10165)
*Matthew J. Schneider,James Bailie,Dawn Iacobucci*

Main category: cs.CR

TL;DR: 数据匿名化并非万能，实施复杂且需按案例定制，企业应结合多种隐私安全策略而非仅依赖差分隐私或合成数据。


<details>
  <summary>Details</summary>
Motivation: 企业希望通过数据匿名化（差分隐私和合成数据）实现合规与商业可行性，但实践中遇到复杂选择与效果差异，导致采用困难。

Method: 作者通过概念分析对比不同匿名化选择（如保护单元、域及保护范围和保护标准）如何影响差分隐私等方法的实际含义与效果，结合业务需求讨论可行性与限制。

Result: 作者指出：1) 匿名化实施涉及多项互相关的关键选择，会改变隐私保证的含义；2) 在保护单元接近所需洞察尺度时，方法效果不足；3) 因需保留与盈利相关的模式，解决方案常须定制化，降低可扩展性；4) 最佳策略是混合多种隐私与安全手段。

Conclusion: 本文结论为：数据匿名化并非通用简单方案，而是需要针对具体场景定制，且常与其他隐私/安全措施结合使用。

Abstract: Companies are looking to data anonymization research $\unicode{x2013}$
including differential private and synthetic data methods $\unicode{x2013}$ for
simple and straightforward compliance solutions. But data anonymization has not
taken off in practice because it is anything but simple to implement. For one,
it requires making complex choices which are case dependent, such as the domain
of the dataset to anonymize; the units to protect; the scope where the data
protection should extend to; and the standard of protection. Each variation of
these choices changes the very meaning, as well as the practical implications,
of differential privacy (or of any other measure of data anonymization). Yet
differential privacy is frequently being branded as the same privacy guarantee
regardless of variations in these choices. Some data anonymization methods can
be effective, but only when the insights required are much larger than the unit
of protection. Given that businesses care about profitability, any solution
must preserve the patterns between a firm's data and that profitability. As a
result, data anonymization solutions usually need to be bespoke and
case-specific, which reduces their scalability. Companies should not expect
easy wins, but rather recognize that anonymization is just one approach to data
privacy with its own particular advantages and drawbacks, while the best
strategies jointly leverage the full range of approaches to data privacy and
security in combination.

</details>


### [84] [Investigating Feature Attribution for 5G Network Intrusion Detection](https://arxiv.org/abs/2509.10206)
*Federica Uccello,Simin Nadjm-Tehrani*

Main category: cs.CR

TL;DR: 在5G安全告警解释中，基于逻辑的VoTE-XAI比统计的SHAP更简洁、稳定且高效，适合实时可操作响应。


<details>
  <summary>Details</summary>
Motivation: 随着5G在关键场景普及，需要从检测恶意活动进化到能提供可用于缓解的可靠判决，因而需要理解和解释ML模型的安全告警以支持可操作的事件响应。作者质疑现有统计归因是否适用于下一代通信系统，并探索逻辑解释的价值。

Method: 作者用XGBoost生成告警，对比两种XAI方法：统计关联的SHAP与基于逻辑的VoTE-XAI。在三个用例和多种5G攻击上评估解释的稀疏性、稳定性和效率，并量化了特征数量和响应时间等指标。

Result: 在92维特征的5G网络中，针对ICMPFlood DoS变体，VoTE-XAI只标识了6个重要特征，而SHAP识别了超过20个。两者在选取特征上显著分歧，但VoTE-XAI未遗漏SHAP的顶级特征。VoTE-XAI在高维（478特征）场景下单次解释时间低于0.002秒，显著更快。

Conclusion: 论文结论是逻辑解释方法（VoTE-XAI）在稀疏性、稳定性和响应效率方面优于统计归因方法（SHAP），尤其在高维5G安全告警场景中更具实用价值。

Abstract: With the rise of fifth-generation (5G) networks in critical applications, it
is urgent to move from detection of malicious activity to systems capable of
providing a reliable verdict suitable for mitigation. In this regard,
understanding and interpreting machine learning (ML) models' security alerts is
crucial for enabling actionable incident response orchestration. Explainable
Artificial Intelligence (XAI) techniques are expected to enhance trust by
providing insights into why alerts are raised. A dominant approach
statistically associates feature sets that can be correlated to a given alert.
This paper starts by questioning whether such attribution is relevant for
future generation communication systems, and investigates its merits in
comparison with an approach based on logical explanations. We extensively study
two methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts
generated by an XGBoost model in three different use cases with several 5G
communication attacks. We identify three metrics for assessing explanations:
sparsity, how concise they are; stability, how consistent they are across
samples from the same attack type; and efficiency, how fast an explanation is
generated. As an example, in a 5G network with 92 features, 6 were deemed
important by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while
SHAP identified over 20. More importantly, we found a significant divergence
between features selected by SHAP and VoTE-XAI. However, none of the top-ranked
features selected by SHAP were missed by VoTE-XAI. When it comes to efficiency
of providing interpretations, we found that VoTE-XAI is significantly more
responsive, e.g. it provides a single explanation in under 0.002 seconds, in a
high-dimensional setting (478 features).

</details>


### [85] [Dynamic Vulnerability Patching for Heterogeneous Embedded Systems Using Stack Frame Reconstruction](https://arxiv.org/abs/2509.10213)
*Ming Zhou,Xupu Hu,Zhihao Wang,Haining Wang,Hui Wen,Limin Sun,Peng Zhang*

Main category: cs.CR

TL;DR: StackPatch通过栈帧重构和异常机制触发，实现了对ARM、RISC-V、Xtensa嵌入式设备的快速、轻量级热修补，能在不中断服务的前提下修复RTOS中大量漏洞，单次修补耗时极短。


<details>
  <summary>Details</summary>
Motivation: 嵌入式、尤其是关键任务设备（如医疗设备）对可用内存和计算能力有限，无法采用传统动态修补方法；设备需连续运行，且架构多样化使得通用触发器难以实现，因而需要一种轻量、跨架构且不中断服务的热修补方案。

Method: 通过重建栈帧并利用嵌入式处理器的异常处理机制作为触发手段，StackPatch实现了对内存中程序的更新。提出了多种触发策略以适配不同硬件和内核环境，并在ARM、RISC-V、Xtensa三大架构上实现。

Result: 在实验中，StackPatch成功修复了实时操作系统中102个公开漏洞，适用于医疗设备、可编程逻辑控制器和网络服务等场景；每次修补在所有测试MCU上均在260个时钟周期内完成。

Conclusion: StackPatch是一个基于栈帧重构的热修补框架，适用于资源受限且需持续运行的嵌入式设备，能在多种MCU架构上进行控制流重定向并快速修复漏洞。

Abstract: Existing dynamic vulnerability patching techniques are not well-suited for
embedded devices, especially mission-critical ones such as medical equipment,
as they have limited computational power and memory but uninterrupted service
requirements. Those devices often lack sufficient idle memory for dynamic
patching, and the diverse architectures of embedded systems further complicate
the creation of patch triggers that are compatible across various system
kernels and hardware platforms. To address these challenges, we propose a hot
patching framework called StackPatch that facilitates patch development based
on stack frame reconstruction. StackPatch introduces different triggering
strategies to update programs stored in memory units. We leverage the
exception-handling mechanisms commonly available in embedded processors to
enhance StackPatch's adaptability across different processor architectures for
control flow redirection. We evaluated StackPatch on embedded devices featuring
three major microcontroller (MCU) architectures: ARM, RISC-V, and Xtensa. In
the experiments, we used StackPatch to successfully fix 102 publicly disclosed
vulnerabilities in real-time operating systems (RTOS). We applied patching to
medical devices, soft programmable logic controllers (PLCs), and network
services, with StackPatch consistently completing each vulnerability
remediation in less than 260 MCU clock cycles.

</details>


### [86] [Empirical Evaluation of Memory-Erasure Protocols](https://arxiv.org/abs/2509.10224)
*Reynaldo Gil-Pons,Sjouke Mauw,Rolando Trujillo-Rasua*

Main category: cs.CR

TL;DR: 首次在真实IoT设备上系统评测软件型内存擦除协议，发现其可行但性能受限于设备算力和网络，提供了选择协议的实用评估框架。


<details>
  <summary>Details</summary>
Motivation: 为推动软件型内存擦除协议的工业采纳，弥补仅有理论分析而缺乏在真实设备上安全、性能与可行性实测的空白。

Method: 在3种不同算力的现代IoT设备上，评测7个协议与6种哈希实现，考察多种性能与安全指标，并基于不同网络速度、内存大小与所需安全级别进行比较分析，构建评估框架以根据安全需求选取最佳折衷方案。

Result: 实验显示协议总体可行但在慢设备上耗时较长（可达数秒）；网络速度与协议常数对性能影响较大；无协议在所有设置均优；并提供了一个可根据安全要求自动选择最优协议的评估框架。

Conclusion: 本文首次对软件型内存擦除协议进行实证评估，证明这些协议在实际IoT设备上可行，但性能受设备算力与网络影响显著；没有单一协议在所有场景中占优。

Abstract: Software-based memory-erasure protocols are two-party communication protocols
where a verifier instructs a computational device to erase its memory and send
a proof of erasure. They aim at guaranteeing that low-cost IoT devices are free
of malware by putting them back into a safe state without requiring secure
hardware or physical manipulation of the device. Several software-based
memory-erasure protocols have been introduced and theoretically analysed. Yet,
many of them have not been tested for their feasibility, performance and
security on real devices, which hinders their industry adoption. This article
reports on the first empirical analysis of software-based memory-erasure
protocols with respect to their security, erasure guarantees, and performance.
The experimental setup consists of 3 modern IoT devices with different
computational capabilities, 7 protocols, 6 hash-function implementations, and
various performance and security criteria. Our results indicate that existing
software-based memory-erasure protocols are feasible, although slow devices may
take several seconds to erase their memory and generate a proof of erasure. We
found that no protocol dominates across all empirical settings, defined by the
computational power and memory size of the device, the network speed, and the
required level of security. Interestingly, network speed and hidden constants
within the protocol specification played a more prominent role in the
performance of these protocols than anticipated based on the related
literature. We provide an evaluation framework that, given a desired level of
security, determines which protocols offer the best trade-off between
performance and erasure guarantees.

</details>


### [87] [ExDoS: Expert-Guided Dual-Focus Cross-Modal Distillation for Smart Contract Vulnerability Detection](https://arxiv.org/abs/2509.10252)
*Yifan Jia,Ye Tian,Yanbin Wang,Jianguo Sun,Haitao Xu*

Main category: cs.CR

TL;DR: ExDoS通过源字节码跨模态蒸馏，结合Dual-Attention图网络和细粒度漏洞模式注释，实现对字节码更精准的漏洞检测，显著提升F1性能。


<details>
  <summary>Details</summary>
Motivation: 智能合约通常只提供字节码，源代码与字节码之间存在模态差异，且现有对齐方法多为图级对齐，忽略细粒度结构/语义关联，且字节码缺少精确漏洞模式注释，导致判别性特征学习不足。

Method: 构建源代码语义图和字节码控制流图，设计Dual-Attention Graph Network和节点注意力聚合模块以增强局部模式捕获；提出双重蒸馏目标（全局语义蒸馏损失与局部语义蒸馏损失）；并制作了第一个字节码级漏洞模式注释数据集与源代码定义对齐。

Result: 在真实合约数据上，ExDoS相比强基线在F1上稳定提升3%--6%。

Conclusion: 该论文提出了一种跨模态知识蒸馏方法ExDoS，将源代码的丰富语义转移到字节码以提升智能合约漏洞检测性能。

Abstract: The success of smart contracts has made them a target for attacks, but their
closed-source nature often forces vulnerability detection to work on bytecode,
which is inherently more challenging than source-code-based analysis. While
recent studies try to align source and bytecode embeddings during training to
transfer knowledge, current methods rely on graph-level alignment that obscures
fine-grained structural and semantic correlations between the two modalities.
Moreover, the absence of precise vulnerability patterns and granular
annotations in bytecode leads to depriving the model of crucial supervisory
signals for learning discriminant features. We propose ExDoS to transfer rich
semantic knowledge from source code to bytecode, effectively supplementing the
source code prior in practical settings. Specifically, we construct semantic
graphs from source code and control-flow graphs from bytecode. To address
obscured local signals in graph-level contract embeddings, we propose a
Dual-Attention Graph Network introducing a novel node attention aggregation
module to enhance local pattern capture in graph embeddings. Furthermore, by
summarizing existing source code vulnerability patterns and designing a
corresponding set of bytecode-level patterns for each, we construct the first
dataset of vulnerability pattern annotations aligned with source code
definitions to facilitate fine-grained cross-modal alignment and the capture of
function-level vulnerability signals. Finally, we propose a dual-focus
objective for our cross-modal distillation framework, comprising: a Global
Semantic Distillation Loss for transferring graph-level knowledge and a Local
Semantic Distillation Loss enabling expert-guided, fine-grained
vulnerability-specific distillation. Experiments on real-world contracts
demonstrate that our method achieves consistent F1-score improvements
(3\%--6\%) over strong baselines.

</details>


### [88] [URL2Graph++: Unified Semantic-Structural-Character Learning for Malicious URL Detection](https://arxiv.org/abs/2509.10287)
*Ye Tian,Yifan Jia,Yanbin Wang,Jianguo Sun,Zhiquan Liu,Xiaowen Ling*

Main category: cs.CR

TL;DR: 提出将子词/字符双图与BERT语义嵌入通过门控融合结合的恶意URL检测方法，能同时捕获语义、字符级和结构特征，实验优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 面对URL多样性和攻击者混淆技术，需获得语义理解以提升泛化，并建模URL内部结构关系以捕获上下文依赖，从而提高检测鲁棒性。

Method: 构建子词级与字符级双粒度URL图，节点表示为token/字符并以共现构建边；用字符级卷积网络初始化节点表示；通过联合训练的图卷积网络学习一致的图级表示；采用BERT获取URL的语义表征；最后用门控动态融合网络将BERT表示与图表示融合进行分类。

Result: 在多维度、多个基准的广泛评测中，该方法优于现有SOTA，包括在对抗复杂混淆和与大语言模型比较中表现更好。

Conclusion: 该方法通过多粒度图学习与语义嵌入的融合，有效提升了恶意URL检测的鲁棒性与泛化能力，实验表明优于现有最先进方法。

Abstract: Malicious URL detection remains a major challenge in cybersecurity, primarily
due to two factors: (1) the exponential growth of the Internet has led to an
immense diversity of URLs, making generalized detection increasingly difficult;
and (2) attackers are increasingly employing sophisticated obfuscation
techniques to evade detection. We advocate that addressing these challenges
fundamentally requires: (1) obtaining semantic understanding to improve
generalization across vast and diverse URL sets, and (2) accurately modeling
contextual relationships within the structural composition of URLs. In this
paper, we propose a novel malicious URL detection method combining
multi-granularity graph learning with semantic embedding to jointly capture
semantic, character-level, and structural features for robust URL analysis. To
model internal dependencies within URLs, we first construct dual-granularity
URL graphs at both subword and character levels, where nodes represent URL
tokens/characters and edges encode co-occurrence relationships. To obtain
fine-grained embeddings, we initialize node representations using a
character-level convolutional network. The two graphs are then processed
through jointly trained Graph Convolutional Networks to learn consistent
graph-level representations, enabling the model to capture complementary
structural features that reflect co-occurrence patterns and character-level
dependencies. Furthermore, we employ BERT to derive semantic representations of
URLs for semantically aware understanding. Finally, we introduce a gated
dynamic fusion network to combine the semantically enriched BERT
representations with the jointly optimized graph vectors, further enhancing
detection performance. We extensively evaluate our method across multiple
challenging dimensions. Results show our method exceeds SOTA performance,
including against large language models.

</details>


### [89] [Innovating Augmented Reality Security: Recent E2E Encryption Approaches](https://arxiv.org/abs/2509.10313)
*Hamish Alsop,Leandros Maglaras,Helge Janicke,Iqbal H. Sarker,Mohamed Amine Ferrag*

Main category: cs.CR

TL;DR: E2EE在保护隐私和阻碍执法间存在矛盾，放弃强加密风险高，建议通过技术与政策组合寻找平衡。


<details>
  <summary>Details</summary>
Motivation: 随着通信工具广泛采用E2EE，社会面临如何在保障个人隐私与维护公共安全之间取得平衡的挑战，论文旨在系统分析这一矛盾并提出务实的解决思路。

Method: 通过文献综述和案例分析，评估E2EE对隐私保护和执法调查的影响，并讨论现有的技术替代方案（如安全后门、元数据分析、客户侧扫描）及其局限性与风险。

Result: 论文得出结论：未经慎重设计的弱化E2EE会降低整体网络安全并损害用户信任；更现实的路径是增强透明度、改进可审计的政策框架、投资元数据与端点安全的合法使用，以及推动加密友好且可行的执法协作机制。

Conclusion: 该论文讨论了端到端加密(E2EE)的双重角色，强调在保护隐私与可能阻碍执法之间的权衡，提出需要技术、政策与透明度的结合来平衡安全与监管需求。

Abstract: End-to-end encryption (E2EE) has emerged as a fundamental element of modern
digital communication, protecting data from unauthorized access during
transmission. By design, E2EE ensures that only the intended recipient can
decrypt the information, making it inaccessible even to service providers. Yet,
this powerful safeguard of individual privacy and digital trust also introduces
a paradox: it can simultaneously prevent law enforcement efforts by hiding
potential malicious activities. This paper examines the dual role of E2EE, its
critical importance to privacy, the challenges it

</details>


### [90] [Automated Testing of Broken Authentication Vulnerabilities in Web APIs with AuthREST](https://arxiv.org/abs/2509.10320)
*Davide Corradini,Mariano Ceccato,Mohammad Ghafari*

Main category: cs.CR

TL;DR: AuthREST是一款开源工具，自动检测凭证填充、密码爆破和令牌未验证问题，实测能发现真实API中的认证漏洞。


<details>
  <summary>Details</summary>
Motivation: Broken authentication是API安全中最常见的风险，需自动化工具检测此类问题。

Method: 自动化对Web API执行凭证填充、密码暴力破解和未验证令牌检测等测试。

Result: 在实证测试中，AuthREST发现了四个公开API中的未知认证漏洞，显示出较高的实用性。

Conclusion: AuthREST能有效发现实际API的认证缺陷，并提升Web API安全。

Abstract: We present AuthREST, an open-source security testing tool targeting broken
authentication, one of the most prevalent API security risks in the wild.
AuthREST automatically tests web APIs for credential stuffing, password brute
forcing, and unchecked token authenticity. Empirical results show that AuthREST
is effective in improving web API security. Notably, it uncovered previously
unknown authentication vulnerabilitiesin in four public APIs.

</details>


### [91] [Bitcoin Cross-Chain Bridge: A Taxonomy and Its Promise in Artificial Intelligence of Things](https://arxiv.org/abs/2509.10413)
*Guojun Tang,Carylyne Chan,Ning Nan,Spencer Yang,Jiayu Zhou,Henry Leung,Mohammad Mamun,Steve Drew*

Main category: cs.CR

TL;DR: 对比特币跨链桥进行三类分类并在多维指标上比较，强调BitVM与递归侧链的前景，探讨在AIoT中的应用场景与设计建议。


<details>
  <summary>Details</summary>
Motivation: 比特币脚本能力受限且缺乏原生互操作机制，限制其在DeFi与多链应用中的作用；因此需要梳理跨链桥设计以指导AIoT系统中安全高效的跨链集成。

Method: 通过将桥协议分为三类（简单代币交换、抵押资产桥、任意消息桥），并对每类在信任模型、延迟、资本效率、DeFi可组合性等指标上进行系统比较与讨论，同时结合AIoT场景进行案例分析。

Result: 提出的分类与比较揭示了各类桥在安全性、可扩展性与可组合性之间的权衡，指明BitVM与递归侧链在实现可编程、安全互操作方面的潜力；并给出AIoT应用的具体场景与需求匹配建议。

Conclusion: 本文构建了比特币跨链桥协议的分类体系，并指出不同设计在信任假设、性能与AIoT适用性方面的权衡；强调BitVM和递归侧链作为具潜力的创新方向，但需进一步实证与安全分析。

Abstract: Bitcoin's limited scripting capabilities and lack of native interoperability
mechanisms have constrained its integration into the broader blockchain
ecosystem, especially decentralized finance (DeFi) and multi-chain
applications. This paper presents a comprehensive taxonomy of Bitcoin
cross-chain bridge protocols, systematically analyzing their trust assumptions,
performance characteristics, and applicability to the Artificial Intelligence
of Things (AIoT) scenarios. We categorize bridge designs into three main types:
naive token swapping, pegged-asset bridges, and arbitrary-message bridges. Each
category is evaluated across key metrics such as trust model, latency, capital
efficiency, and DeFi composability. Emerging innovations like BitVM and
recursive sidechains are highlighted for their potential to enable secure,
scalable, and programmable Bitcoin interoperability. Furthermore, we explore
practical use cases of cross-chain bridges in AIoT applications, including
decentralized energy trading, healthcare data integration, and supply chain
automation. This taxonomy provides a foundational framework for researchers and
practitioners seeking to design secure and efficient cross-chain
infrastructures in AIoT systems.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [92] [DBOS Network Sensing: A Web Services Approach to Collaborative Awareness](https://arxiv.org/abs/2509.09898)
*Sophia Lockton,Jeremy Kepner,Michael Stonebraker,Hayden Jananthan,LaToya Anderson,William Arcand,David Bestor,William Bergeron,Alex Bonn,Daniel Burrill,Chansup Byun,Timothy Davis,Vijay Gadepally,Michael Houle,Matthew Hubbell,Michael Jones,Piotr Luszczek,Peter Michaleas,Lauren Milechin,Chasen Milner,Guillermo Morales,Julie Mullen,Michel Pelletier,Alex Poliakov,Andrew Prout,Albert Reuther,Antonio Rosa,Charles Yee,Alex Pentland*

Main category: cs.NI

TL;DR: 在DBOS中通过GraphBLAS矩阵（Python-GraphBLAS与OneSparse PostgreSQL）添加网络感知，验证了高吞吐、低开销和良好扩展性，支持协作网络意识的可行性。


<details>
  <summary>Details</summary>
Motivation: 降低Web部署工作量并提升分布式Web服务的弹性与安全性，借助协作的网络感知共享环境信息来改进集体防御与鲁棒性，同时保持高性能Web请求处理能力。

Method: 将网络流量表示为GraphBLAS超稀疏流量矩阵，通过两种实现路径接入DBOS：1) Python-GraphBLAS直接在内存中构建与计算；2) OneSparse PostgreSQL利用数据库一稀疏表结构存储与查询。使用IEEE/MIT/Amazon匿名网络感知图挑战的数据和分析工作流进行功能验证，并在MIT SuperCloud上用pPython并行化，跨最多64个计算节点做基准测试。

Result: 单个DBOS实例可维持>10^5次/秒的Web请求率，表明加入网络感知开销极小。Python-GraphBLAS实现在线性比例下扩展到64节点，OneSparse PostgreSQL实现线性扩展到32节点；聚合器架构验证了多实例协作感知可行性且总体资源增加可忽略。

Conclusion: DBOS成功将网络感知能力无缝集成到数据库操作系统中，且对单实例吞吐影响可忽略不计；在协作感知场景下，基于Python-GraphBLAS和OneSparse PostgreSQL的实现分别在64和32节点前线性扩展，表明协作网络感知在资源开销上代价很小。

Abstract: DBOS (DataBase Operating System) is a novel capability that integrates web
services, operating system functions, and database features to significantly
reduce web-deployment effort while increasing resilience. Integration of high
performance network sensing enables DBOS web services to collaboratively create
a shared awareness of their network environments to enhance their collective
resilience and security. Network sensing is added to DBOS using GraphBLAS
hypersparse traffic matrices via two approaches: (1) Python-GraphBLAS and (2)
OneSparse PostgreSQL. These capabilities are demonstrated using the workflow
and analytics from the IEEE/MIT/Amazon Anonymized Network Sensing Graph
Challenge. The system was parallelized using pPython and benchmarked using 64
compute nodes on the MIT SuperCloud. The web request rate sustained by a single
DBOS instance was ${>}10^5$, well above the required maximum, indicating that
network sensing can be added to DBOS with negligible overhead. For
collaborative awareness, many DBOS instances were connected to a single DBOS
aggregator. The Python-GraphBLAS and OneSparse PostgreSQL implementations
scaled linearly up to 64 and 32 nodes respectively. These results suggest that
DBOS collaborative network awareness can be achieved with a negligible increase
in computing resources.

</details>


### [93] [Taming Volatility: Stable and Private QUIC Classification with Federated Learning](https://arxiv.org/abs/2509.09997)
*Richard Jozsa,Karel Hynek,Adrian Pekar*

Main category: cs.NI

TL;DR: 为应对网络流量的时间波动对联邦学习的破坏，作者提出客户端数据缓冲，实验证明能稳定训练并在QUIC分类达到接近集中式的性能（95.2% F1）。


<details>
  <summary>Details</summary>
Motivation: 现实网络流量呈日间波动，导致客户端数据可用性不稳定，进而破坏联邦学习的训练稳定性与收敛性；现有针对统计异质性的研究未充分考虑时间波动影响。

Method: 构建并评估客户端侧数据缓冲机制，在CESNET-QUIC22数据集上以14个自治客户端划分，比较标准FL与缓冲后FL的收敛性与性能，使用F1等指标量化。

Result: 在引入客户端缓冲后，联邦系统在QUIC流量分类任务上稳定收敛，取得95.2% F1，比集中式非隐私模型仅低2.3个百分点，证明缓冲机制能有效缓解时变流量带来的不稳定性。

Conclusion: 本文表明在有时变流量的真实联邦学习部署中，标准FL不稳定，使用客户端数据缓冲可以稳定训练并实现接近集中式模型的性能。

Abstract: Federated Learning (FL) is a promising approach for privacy-preserving
network traffic analysis, but its practical deployment is challenged by the
non-IID nature of real-world data. While prior work has addressed statistical
heterogeneity, the impact of temporal traffic volatility-the natural daily ebb
and flow of network activity-on model stability remains largely unexplored.
This volatility can lead to inconsistent data availability at clients,
destabilizing the entire training process. In this paper, we systematically
address the problem of temporal volatility in federated QUIC classification. We
first demonstrate the instability of standard FL in this dynamic setting. We
then propose and evaluate a client-side data buffer as a practical mechanism to
ensure stable and consistent local training, decoupling it from real-time
traffic fluctuations. Using the real-world CESNET-QUIC22 dataset partitioned
into 14 autonomous clients, we then demonstrate that this approach enables
robust convergence. Our results show that a stable federated system achieves a
95.2% F1 score, a mere 2.3 percentage points below a non-private centralized
model. This work establishes a blueprint for building operationally stable FL
systems for network management, proving that the challenges of dynamic network
environments can be overcome with targeted architectural choices.

</details>


### [94] [Service Function Chaining Architecture for Multi-hop Split Inference and Learning](https://arxiv.org/abs/2509.10001)
*Takanori Hara,Masahiro Sasabe*

Main category: cs.NI

TL;DR: 论文用SFC+NSF将拆分神经网络编排为服务链并在SRv6/eBPF上实现，验证了可行性、实时推理优势与动态路由适应性。


<details>
  <summary>Details</summary>
Motivation: 传统端到端神经网络在资源受限或网络分布环境下难以高效部署，拆分推理/学习需要灵活可靠的跨节点通信与路径管理，SFC可为子模型串联提供动态路由与服务组合能力，从而提升效率与适应性。

Method: 将拆分子模型视为服务函数，利用SFC动态建立通信路径；设计Neural Service Functions作为透明TCP代理执行子模型；在网络层使用SRv6与eBPF SFC代理以实现动态路由与最低侵入性。对推理与训练场景分别实现MSI与MSL，并在多跳网络环境中评估延迟、吞吐与路径重配置影响。

Result: 实验表明：架构在MSI/MSL下可行；对实时推理（小批量）效果尤佳，延迟降低并保持吞吐；支持动态路径重配置，能在网络变化时快速调整且控制开销对推理/训练影响较小。

Conclusion: 该论文提出将服务功能链(SFC)理念引入分布式神经网络推理与训练，构建MSI/MSL架构，并实现了NSF与SRv6/eBPF集成原型，验证了可行性与动态路径重配置能力。

Abstract: Service Function Chaining (SFC) is a networking technique that ensures
traffic traverses a predefined sequence of service functions, realizing
arbitrary network services through dynamic and efficient communication paths.
Inspired by this concept, we propose an SFC-based architecture for Multi-hop
Split Inference (MSI), where split sub-models are interpreted as service
functions and their composition forms a service chain representing the global
model. By leveraging SFC, the proposed architecture dynamically establishes
communication paths for split sub-models, ensuring efficient and adaptive
execution. Furthermore, we extend this architecture to Multi-hop Split Learning
(MSL) by applying SFC to the bidirectional communication required for training
tasks. To realize the proposed architecture, we design Neural Service Functions
(NSFs) to execute split sub-models as transparent TCP proxies and integrate
them with Segment Routing over IPv6 (SRv6) and the extended Berkeley Packet
Filter (eBPF)-based SFC proxy. This integration ensures efficient ML processing
over dynamic routing while maintaining compatibility with existing
applications. Evaluation results demonstrate that (1) the proposed architecture
is feasible for both MSI and MSL; (2) it is particularly suitable for real-time
inference in MSI scenarios with small mini-batch sizes; (3) it supports dynamic
path reconfiguration, enabling adaptive responses to changing network
conditions while minimizing the impact of control mechanisms on inference and
learning processes.

</details>


### [95] [Maximising Energy Efficiency in Large-Scale Open RAN: Hybrid xApps and Digital Twin Integration](https://arxiv.org/abs/2509.10097)
*Ahmed Al-Tahmeesschi,Yi Chu,Gurdeep Singh,Charles Turyagyenda,Dritan Kaleshi,David Grace,Hamed Ahmadi*

Main category: cs.NI

TL;DR: 提出基于启发式+无监督学习的混合xApp并结合TeraVM数字孪生进行验证，实现约13%能耗节省且不损害QoS，适用于大规模Open RAN部署。


<details>
  <summary>Details</summary>
Motivation: 5G及以后网络对高带宽、超可靠低时延服务的需求导致RAN能耗大幅上升，给运营商带来运营成本和可持续性压力；O-RAN的解耦与可编程性为节能提供机会，但也增加了管理复杂性，因而需要智能化、可验证的节能策略。

Method: 设计并实现了一个混合xApp：首先使用无监督学习对基站/小区流量和负载模式进行聚类，从而识别可进入睡眠模式的候选RU；然后结合启发式规则（例如负载阈值、QoS约束和切换成本）决定具体的睡眠/唤醒策略；通过数字孪生（AI-RSG）生成逼真的RAN场景进行大规模仿真与在线评估。

Result: 在一个现实的大规模仿真Open RAN场景中，混合xApp相比基线策略实现了约13%的能耗降低，同时保持用户QoS（例如吞吐量、时延和连接稳定性）未见明显恶化。

Conclusion: 该论文提出了一种结合启发式方法与无监督机器学习的混合xApp，并通过数字孪生平台（TeraVM AI-RSG）在大规模Open RAN仿真场景中验证，实现了在不损害用户QoS的前提下约13%的能耗节省。

Abstract: The growing demand for high-speed, ultra-reliable, and low-latency
communications in 5G and beyond networks has significantly driven up power
consumption, particularly within the Radio Access Network (RAN). This surge in
energy demand poses critical operational and sustainability challenges for
mobile network operators, necessitating innovative solutions that enhance
energy efficiency without compromising Quality of Service (QoS). Open Radio
Access Network (O-RAN), spearheaded by the O-RAN Alliance, offers
disaggregated, programmable, and intelligent architectures, promoting
flexibility, interoperability, and cost-effectiveness. However, this
disaggregated approach adds complexity, particularly in managing power
consumption across diverse network components such as Open Radio Units (RUs).
In this paper, we propose a hybrid xApp leveraging heuristic methods and
unsupervised machine learning, integrated with digital twin technology through
the TeraVM AI RAN Scenario Generator (AI-RSG). This approach dynamically
manages RU sleep modes to effectively reduce energy consumption. Our
experimental evaluation in a realistic, large-scale emulated Open RAN scenario
demonstrates that the hybrid xApp achieves approximately 13% energy savings,
highlighting its practicality and significant potential for real-world
deployments without compromising user QoS.

</details>


### [96] [Secure and Scalable Rerouting in LEO Satellite Networks](https://arxiv.org/abs/2509.10173)
*Lyubomir Yanev,Pietro Ronchetti,Joshua Smailes,Martin Strohmeier*

Main category: cs.NI

TL;DR: Compared local, segment, global, and naive routing under random/targeted failures using DSNS; segment-based routing provides near-optimal resilience with low overhead, making it a practical choice for LEO networks.


<details>
  <summary>Details</summary>
Motivation: LEO satellite networks face frequent and unpredictable link/node failures (including due to cyberattacks); understanding how the scope of failure awareness at routing nodes affects resilience and performance is crucial for designing fault-tolerant routing protocols.

Method: Extended the Deep Space Network Simulator (DSNS) to implement and compare four routing paradigms (local neighbor-based, segment-based, global-knowledge-based, and naive source routing) under controlled random and targeted failure scenarios, measuring delivery ratio, latency, rerouting overhead, and loop occurrence.

Result: Segment-based rerouting achieved favorable tradeoffs: high delivery ratios close to global-knowledge routing, lower latency than naive source routing, and minimal rerouting overhead and loops compared to local-only approaches.

Conclusion: Segment-based rerouting offers a strong middle ground, combining local responsiveness with enough global coordination to maintain high delivery ratios and low overhead under various failure modes, making it suitable for resilient LEO satellite networks.

Abstract: Resilient routing in large-scale Low Earth Orbit (LEO) satellite networks
remains a key challenge due to frequent and unpredictable link and node
failures, potentially in response to cybersecurity breaches. While prior work
has explored rerouting strategies with various levels of network awareness,
their relative tradeoffs under dynamic failure conditions remain underexplored.
In this work, we extend the Deep Space Network Simulator (DSNS) to
systematically compare three rerouting paradigms, each differing in the scope
of failure knowledge available to each node. We compare local neighbor-based,
segment-based and global-knowledge-based rerouting as well as a naive source
routing solution that is unaware of failures. Our main goal is to evaluate how
the breadth of failure awareness impacts routing performance and resilience
under failures, both random and targeted. We measure delivery ratio, latency,
rerouting overhead, and loop occurrence. Our findings show the potential of
segment-based rerouting to achieve a favorable tradeoff between local
responsiveness and global coordination, offering resilience benefits with
minimal overhead--insights that can inform future fault-tolerant satellite
network design.

</details>


### [97] [Friend or Foe? Identifying Anomalous Peers in Moneros P2P Network](https://arxiv.org/abs/2509.10214)
*Yannik Kopyciok,Stefan Schmid,Friedhelm Victor*

Main category: cs.NI

TL;DR: 基于五个全球视点240+小时流量的首次系统研究：约14.74%的Monero节点行为异常，可能为并发监控/攻击，论文给出形式化分类、离线检测方法并开源防护管道。


<details>
  <summary>Details</summary>
Motivation: 存在证据表明P2P网络中存在伪装成诚信节点的非标准节点，用于监控和窃听，但对异常对等体的检测与分析理解有限，因此需要系统化研究以量化影响并提高防护能力。

Method: 收集来自五个全球视点的超过240小时网络流量，提出形式化框架定义并分类P2P网络异常模式，基于离线分析实现检测方法（可扩展为实时监控），并实现和公开了用于识别和屏蔽可疑对等体的管道。

Result: 通过分析，作者确认了大比例非标准行为节点，并归纳出多类可疑行为模式；结果表明这些行为削弱了Monero的隐私保障与网络去中心化；同时作者释放了检测工具以支持运营者防护。

Conclusion: 该文首次对Monero P2P网络中的异常行为进行了系统性研究，发现约14.74%（或13.19%）可达节点表现出非标准行为，暗示可能存在多种并发攻击，影响Monero的隐私和去中心化特性。作者提供了分析框架与离线检测方法，并发布了可复现的检查管道以阻断可疑节点。

Abstract: Monero, the leading privacy-focused cryptocurrency, relies on a peer-to-peer
(P2P) network to propagate transactions and blocks. Growing evidence suggests
that non-standard nodes exist in the network, posing as honest nodes but are
perhaps intended for monitoring the network and spying on other nodes. However,
our understanding of the detection and analysis of anomalous peer behavior
remains limited. This paper presents a first comprehensive study of anomalous
behavior in Monero's P2P network. To this end, we collected and analyzed over
240 hours of network traffic captured from five distinct vantage points
worldwide. We further present a formal framework which allows us to
analytically define and classify anomalous patterns in P2P cryptocurrency
networks. Our detection methodology, implemented as an offline analysis,
provides a foundation for real-time monitoring systems. Our analysis reveals
the presence of non-standard peers in the network where approximately 14.74%
(13.19%) of (reachable) peers in the network exhibit non-standard behavior.
These peers exhibit distinct behavioral patterns that might suggest multiple
concurrent attacks, pointing to substantial shortcomings in Monero's privacy
guarantees and network decentralization. To support reproducibility and enable
network operators to protect themselves, we release our examination pipeline to
identify and block suspicious peers based on newly captured network traffic.

</details>


### [98] [RFSeek and Ye Shall Find](https://arxiv.org/abs/2509.10216)
*Noga H. Rotman,Tiago Ferreira,Hila Peleg,Mark Silberstein,Alexandra Silva*

Main category: cs.NI

TL;DR: RFSeek用LLM从RFC生成可追溯、可交互的协议图示，弥补或超越原始图表，便于理解与实现。


<details>
  <summary>Details</summary>
Motivation: RFC文本冗长且主要为自然语言，原有图示常缺失或不完整，导致实现者难以准确理解协议逻辑，需自动化、可审计的可视化摘要以提高可操作性。

Method: 构建交互式工具RFSeek，利用大语言模型解析RFC文本，生成带来源追溯的可探索图表（状态机与连接逻辑），并提供语义对比与导出功能；在多协议（TCP、QUIC、PPTP、DCCP）上评估工具能重建或生成新图示。

Result: RFSeek成功重建了部分RFC自带图示，并发现文本中描述但图示未呈现的重要节点/边；对复杂协议（如QUIC）生成新的可视化摘要；相比现有工具，呈现更透明、易核对的图表。

Conclusion: RFSeek展示了将LLM与可视化结合以提高RFC协议理解的可行性，能从文本中提取并可追溯地呈现状态机与隐含逻辑，补全或超越原始RFC图示，便于审计与实现。

Abstract: Requests for Comments (RFCs) are extensive specification documents for
network protocols, but their prose-based format and their considerable length
often impede precise operational understanding. We present RFSeek, an
interactive tool that automatically extracts visual summaries of protocol logic
from RFCs. RFSeek leverages large language models (LLMs) to generate
provenance-linked, explorable diagrams, surfacing both official state machines
and additional logic found only in the RFC text. Compared to existing RFC
visualizations, RFSeek's visual summaries are more transparent and easier to
audit against their textual source. We showcase the tool's potential through a
series of use cases, including guided knowledge extraction and semantic
diffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.
  In practice, RFSeek not only reconstructs the RFC diagrams included in some
specifications, but, more interestingly, also uncovers important logic such as
nodes or edges described in the text but missing from those diagrams. RFSeek
further derives new visualization diagrams for complex RFCs, with QUIC as a
representative case. Our approach, which we term \emph{Summary Visualization},
highlights a promising direction: combining LLMs with formal, user-customized
visualizations to enhance protocol comprehension and support robust
implementations.

</details>


### [99] [Trusted Repeater Placement in QKD-enabled Optical Networks](https://arxiv.org/abs/2509.10338)
*Arup Kumar Marik,Basabdatta Palit,Sadananda Behera*

Main category: cs.NI

TL;DR: 将节点信任度引入加权Dijkstra并结合介数与特征向量中心性构建复合评分，本文方法在相同TRN数量下可增加约10.8%的最短路径覆盖，提升QKD中继部署的可靠性与安全性。


<details>
  <summary>Details</summary>
Motivation: 传统QKD中假定中继与KMS完全可信，忽视了软件漏洞与内部威胁风险；因此需要在布署中考虑节点的可靠性以增强网络安全性和可扩展性。

Method: 为每个节点分配信任分数，将信任分数映射为链路权重并整合到Dijkstra算法中计算可靠最短路径；同时计算节点的介数中心性和特征向量中心性，按加权组合生成复合评分，用于排序与选择TRN。

Result: 在参考城域光网络拓扑上的仿真表明，本文方法在选取约8个TRN时，相较于基于度中心性的传统方法，覆盖的最短路径数提高了10.77%，从而提升了安全连接覆盖率。

Conclusion: 本文提出了一种将节点信任度融入路径选取的TRN部署框架，通过加权最短路径和复合中心性评分选择中继节点，实现对软件漏洞和内部威胁的鲁棒性提升。

Abstract: Quantum Key Distribution (QKD) provides information-theoretic security, but
is limited by distance in optical networks, thereby requiring repeater nodes to
extend coverage. Existing works usually assume all repeater nodes and
associated Key Management Servers (KMSs) to be Trusted Repeater Nodes (TRNs),
while ignoring risks from software exploits and insider threats. In this paper,
we propose a reliability-aware TRN placement framework for metro optical
networks, which assigns each node a trust score and integrates it into the
Dijkstra algorithm via weighted links. We then rank the nodes using a composite
score, which is a weighted combination of betweenness centrality and
eigenvector centrality to enable a secure and scalable TRN deployment.
Simulation results on a reference topology show that our method covers 10.77%
more shortest paths compared to traditional metrics like degree centrality,
using the same number (around eight) of TRNs, making it suitable for TRN
selection to maximize secure connectivity.

</details>
